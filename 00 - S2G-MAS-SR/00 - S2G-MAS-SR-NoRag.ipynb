{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ffa47eea-919c-42dc-b8e8-02e0a9533eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[generate_search_string_node] Current iteration: 1\n",
      "Final String:content='ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR peer reviewed literature OR bibliometric analysis ) AND ( modeling artifacts OR software artifacts OR input artifacts OR output artifacts ) AND ( research type OR study application OR domain application ) )' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 576, 'total_tokens': 631, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'finish_reason': 'stop', 'logprobs': None} id='run-b5997950-36ae-4327-92fe-217ec1cbebd5-0' usage_metadata={'input_tokens': 576, 'output_tokens': 55, 'total_tokens': 631, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Metadata saved to: 00-Query/00-Query-Agent-MDTE-open-mixtral-8x7b-1.0/JSON\\results.json\n",
      "[generate_search_string_node] Search string (final): ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR peer reviewed literature OR bibliometric analysis ) AND ( modeling artifacts OR software artifacts OR input artifacts OR output artifacts ) AND ( research type OR study application OR domain application ) )\n",
      "[Profiling] wrapper took 8.5123 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vitto\\anaconda3\\lib\\site-packages\\codecarbon\\output_methods\\file.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame.from_records([dict(total.values)])])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[extract_databases_node] Input text: \n",
      "Research Questions:\n",
      "\n",
      "RQ0: What are the bibliometric key facts of peer-reviewed literature documenting applications of MDE to DTs?\n",
      "RQ0.1: In which years are they published?\n",
      "RQ0.2: In which types of venues are they published?\n",
      "\n",
      "RQ1: How and how often are automation techniques applied to DTs in peer-reviewed literature?\n",
      "RQ1.1: How often are the different automation techniques applied in the context of DTs?\n",
      "RQ1.2: Which modeling artifacts and software artifacts are used by these automation techniques?\n",
      "RQ1.3: Which combinations of input and output artifacts are used by these automation techniques?\n",
      "RQ1.4: What is the research type of the studies that apply these automation techniques?\n",
      "\n",
      "RQ2: To which types of DTs are automation techniques applied in peer-reviewed literature?\n",
      "RQ2.1: Which TT does the DT represent, in which SLCP of the TT are automation techniques applied, and what is the TLCP of DTs to which automation techniques are applied?\n",
      "RQ2.2: How does the application of automation techniques (identified with RQ1) vary for different DT types?\n",
      "\n",
      "RQ3: In which domains are automation techniques applied to DTs in peer-reviewed literature?\n",
      "RQ3.1: For which domains are automation techniques applied to DTs?\n",
      "RQ3.2: How does the application of automation techniques (identified with RQ1) vary for the identified domains?\n",
      "RQ3.3: How does the DT type (identified with RQ2) vary for the identified domains?\n",
      "\n",
      "Database: Scopus\n",
      "            \n",
      "[extract_databases_node] Databases extracted: ['scopus']\n",
      "[Profiling] wrapper took 4.9094 seconds\n",
      "[format_queries_node] Search string: ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR peer reviewed literature OR bibliometric analysis ) AND ( modeling artifacts OR software artifacts OR input artifacts OR output artifacts ) AND ( research type OR study application OR domain application ) )\n",
      "[format_queries_node] Databases: ['scopus']\n",
      "[format_queries_node] Formatted queries: {'scopus': 'ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR peer reviewed literature OR bibliometric analysis ) AND ( modeling artifacts OR software artifacts OR input artifacts OR output artifacts ) AND ( research type OR study application OR domain application ) )'}\n",
      "[Profiling] wrapper took 4.8911 seconds\n",
      "[run_db_search_node] Formatted queries: {'scopus': 'ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR peer reviewed literature OR bibliometric analysis ) AND ( modeling artifacts OR software artifacts OR input artifacts OR output artifacts ) AND ( research type OR study application OR domain application ) )'}\n",
      "[multi_db_search_wrapper] Input JSON: {'query': 'ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR peer reviewed literature OR bibliometric analysis ) AND ( modeling artifacts OR software artifacts OR input artifacts OR output artifacts ) AND ( research type OR study application OR domain application ) )', 'dbs': 'scopus'}\n",
      "[extract_db_names] Input: scopus\n",
      "[extract_db_names] Extracted: ['scopus']\n",
      "[multi_db_search] Databases: ['scopus']\n",
      "Formatted Scopus Query: ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR peer reviewed literature OR bibliometric analysis ) AND ( modeling artifacts OR software artifacts OR input artifacts OR output artifacts ) AND ( research type OR study application OR domain application ) )\n",
      "Performing Scopus search for query: ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR peer reviewed literature OR bibliometric analysis ) AND ( modeling artifacts OR software artifacts OR input artifacts OR output artifacts ) AND ( research type OR study application OR domain application ) )\n",
      "Error during Scopus search: HTTPSConnectionPool(host='api.elsevier.com', port=443): Max retries exceeded with url: /content/search/scopus?count=25&view=COMPLETE&query=ALL+%28+%28+model+driven+engineering+OR+MDE+OR+digital+twins+OR+DTs+%29+AND+%28+automation+techniques+OR+peer+reviewed+literature+OR+bibliometric+analysis+%29+AND+%28+modeling+artifacts+OR+software+artifacts+OR+input+artifacts+OR+output+artifacts+%29+AND+%28+research+type+OR+study+application+OR+domain+application+%29+%29&cursor=%2A (Caused by ResponseError('too many 500 error responses'))\n",
      "[Profiling] wrapper took 13.3935 seconds\n",
      "Error parsing results for scopus: Expecting value: line 1 column 1 (char 0)\n",
      "Total results found: 0\n",
      "Insufficient results; triggering relaxed (broaden) search string generation.\n",
      "[Profiling] wrapper took 4.8733 seconds\n",
      "[generate_search_string_node] Current iteration: 2\n",
      "Final String:content='ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR peer reviewed literature OR bibliometric analysis ) AND ( modeling artifacts OR software artifacts OR input artifacts OR output artifacts ) AND ( research type OR study application OR literature review ) AND ( application domains OR domain variation OR digital twin types ) )' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 67, 'prompt_tokens': 576, 'total_tokens': 643, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'finish_reason': 'stop', 'logprobs': None} id='run-d0a32eb5-7116-4cca-bc5b-7b3971fad919-0' usage_metadata={'input_tokens': 576, 'output_tokens': 67, 'total_tokens': 643, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Metadata saved to: 00-Query/00-Query-Agent-MDTE-open-mixtral-8x7b-1.0/JSON\\results.json\n",
      "[generate_search_string_node] Search string (final): ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR peer reviewed literature OR bibliometric analysis ) AND ( modeling artifacts OR software artifacts OR input artifacts OR output artifacts ) AND ( research type OR study application OR literature review ) AND ( application domains OR domain variation OR digital twin types ) )\n",
      "[Profiling] wrapper took 7.8740 seconds\n",
      "[extract_databases_node] Input text: \n",
      "Research Questions:\n",
      "\n",
      "RQ0: What are the bibliometric key facts of peer-reviewed literature documenting applications of MDE to DTs?\n",
      "RQ0.1: In which years are they published?\n",
      "RQ0.2: In which types of venues are they published?\n",
      "\n",
      "RQ1: How and how often are automation techniques applied to DTs in peer-reviewed literature?\n",
      "RQ1.1: How often are the different automation techniques applied in the context of DTs?\n",
      "RQ1.2: Which modeling artifacts and software artifacts are used by these automation techniques?\n",
      "RQ1.3: Which combinations of input and output artifacts are used by these automation techniques?\n",
      "RQ1.4: What is the research type of the studies that apply these automation techniques?\n",
      "\n",
      "RQ2: To which types of DTs are automation techniques applied in peer-reviewed literature?\n",
      "RQ2.1: Which TT does the DT represent, in which SLCP of the TT are automation techniques applied, and what is the TLCP of DTs to which automation techniques are applied?\n",
      "RQ2.2: How does the application of automation techniques (identified with RQ1) vary for different DT types?\n",
      "\n",
      "RQ3: In which domains are automation techniques applied to DTs in peer-reviewed literature?\n",
      "RQ3.1: For which domains are automation techniques applied to DTs?\n",
      "RQ3.2: How does the application of automation techniques (identified with RQ1) vary for the identified domains?\n",
      "RQ3.3: How does the DT type (identified with RQ2) vary for the identified domains?\n",
      "\n",
      "Database: Scopus\n",
      "            \n",
      "[extract_databases_node] Databases extracted: ['scopus']\n",
      "[Profiling] wrapper took 4.9146 seconds\n",
      "[format_queries_node] Search string: ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR peer reviewed literature OR bibliometric analysis ) AND ( modeling artifacts OR software artifacts OR input artifacts OR output artifacts ) AND ( research type OR study application OR literature review ) AND ( application domains OR domain variation OR digital twin types ) )\n",
      "[format_queries_node] Databases: ['scopus']\n",
      "[format_queries_node] Formatted queries: {'scopus': 'ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR peer reviewed literature OR bibliometric analysis ) AND ( modeling artifacts OR software artifacts OR input artifacts OR output artifacts ) AND ( research type OR study application OR literature review ) AND ( application domains OR domain variation OR digital twin types ) )'}\n",
      "[Profiling] wrapper took 4.9470 seconds\n",
      "[run_db_search_node] Formatted queries: {'scopus': 'ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR peer reviewed literature OR bibliometric analysis ) AND ( modeling artifacts OR software artifacts OR input artifacts OR output artifacts ) AND ( research type OR study application OR literature review ) AND ( application domains OR domain variation OR digital twin types ) )'}\n",
      "[multi_db_search_wrapper] Input JSON: {'query': 'ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR peer reviewed literature OR bibliometric analysis ) AND ( modeling artifacts OR software artifacts OR input artifacts OR output artifacts ) AND ( research type OR study application OR literature review ) AND ( application domains OR domain variation OR digital twin types ) )', 'dbs': 'scopus'}\n",
      "[extract_db_names] Input: scopus\n",
      "[extract_db_names] Extracted: ['scopus']\n",
      "[multi_db_search] Databases: ['scopus']\n",
      "Formatted Scopus Query: ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR peer reviewed literature OR bibliometric analysis ) AND ( modeling artifacts OR software artifacts OR input artifacts OR output artifacts ) AND ( research type OR study application OR literature review ) AND ( application domains OR domain variation OR digital twin types ) )\n",
      "Performing Scopus search for query: ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR peer reviewed literature OR bibliometric analysis ) AND ( modeling artifacts OR software artifacts OR input artifacts OR output artifacts ) AND ( research type OR study application OR literature review ) AND ( application domains OR domain variation OR digital twin types ) )\n",
      "Error during Scopus search: HTTPSConnectionPool(host='api.elsevier.com', port=443): Max retries exceeded with url: /content/search/scopus?count=25&view=COMPLETE&query=ALL+%28+%28+model+driven+engineering+OR+MDE+OR+digital+twins+OR+DTs+%29+AND+%28+automation+techniques+OR+peer+reviewed+literature+OR+bibliometric+analysis+%29+AND+%28+modeling+artifacts+OR+software+artifacts+OR+input+artifacts+OR+output+artifacts+%29+AND+%28+research+type+OR+study+application+OR+literature+review+%29+AND+%28+application+domains+OR+domain+variation+OR+digital+twin+types+%29+%29&cursor=%2A (Caused by ResponseError('too many 500 error responses'))\n",
      "[Profiling] wrapper took 10.4070 seconds\n",
      "Error parsing results for scopus: Expecting value: line 1 column 1 (char 0)\n",
      "Total results found: 0\n",
      "Insufficient results; triggering relaxed (broaden) search string generation.\n",
      "[Profiling] wrapper took 4.9370 seconds\n",
      "[generate_search_string_node] Current iteration: 3\n",
      "Final String:content='ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR modeling artifacts OR software artifacts ) AND ( peer reviewed literature OR bibliometric analysis OR research studies ) AND ( application domains OR different domains OR domain variation ) )' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 576, 'total_tokens': 628, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'finish_reason': 'stop', 'logprobs': None} id='run-1db96315-fb73-4949-baf0-56def288c3a2-0' usage_metadata={'input_tokens': 576, 'output_tokens': 52, 'total_tokens': 628, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Metadata saved to: 00-Query/00-Query-Agent-MDTE-open-mixtral-8x7b-1.0/JSON\\results.json\n",
      "[generate_search_string_node] Search string (final): ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR modeling artifacts OR software artifacts ) AND ( peer reviewed literature OR bibliometric analysis OR research studies ) AND ( application domains OR different domains OR domain variation ) )\n",
      "[Profiling] wrapper took 6.7788 seconds\n",
      "[extract_databases_node] Input text: \n",
      "Research Questions:\n",
      "\n",
      "RQ0: What are the bibliometric key facts of peer-reviewed literature documenting applications of MDE to DTs?\n",
      "RQ0.1: In which years are they published?\n",
      "RQ0.2: In which types of venues are they published?\n",
      "\n",
      "RQ1: How and how often are automation techniques applied to DTs in peer-reviewed literature?\n",
      "RQ1.1: How often are the different automation techniques applied in the context of DTs?\n",
      "RQ1.2: Which modeling artifacts and software artifacts are used by these automation techniques?\n",
      "RQ1.3: Which combinations of input and output artifacts are used by these automation techniques?\n",
      "RQ1.4: What is the research type of the studies that apply these automation techniques?\n",
      "\n",
      "RQ2: To which types of DTs are automation techniques applied in peer-reviewed literature?\n",
      "RQ2.1: Which TT does the DT represent, in which SLCP of the TT are automation techniques applied, and what is the TLCP of DTs to which automation techniques are applied?\n",
      "RQ2.2: How does the application of automation techniques (identified with RQ1) vary for different DT types?\n",
      "\n",
      "RQ3: In which domains are automation techniques applied to DTs in peer-reviewed literature?\n",
      "RQ3.1: For which domains are automation techniques applied to DTs?\n",
      "RQ3.2: How does the application of automation techniques (identified with RQ1) vary for the identified domains?\n",
      "RQ3.3: How does the DT type (identified with RQ2) vary for the identified domains?\n",
      "\n",
      "Database: Scopus\n",
      "            \n",
      "[extract_databases_node] Databases extracted: ['scopus']\n",
      "[Profiling] wrapper took 4.9092 seconds\n",
      "[format_queries_node] Search string: ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR modeling artifacts OR software artifacts ) AND ( peer reviewed literature OR bibliometric analysis OR research studies ) AND ( application domains OR different domains OR domain variation ) )\n",
      "[format_queries_node] Databases: ['scopus']\n",
      "[format_queries_node] Formatted queries: {'scopus': 'ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR modeling artifacts OR software artifacts ) AND ( peer reviewed literature OR bibliometric analysis OR research studies ) AND ( application domains OR different domains OR domain variation ) )'}\n",
      "[Profiling] wrapper took 5.3775 seconds\n",
      "[run_db_search_node] Formatted queries: {'scopus': 'ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR modeling artifacts OR software artifacts ) AND ( peer reviewed literature OR bibliometric analysis OR research studies ) AND ( application domains OR different domains OR domain variation ) )'}\n",
      "[multi_db_search_wrapper] Input JSON: {'query': 'ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR modeling artifacts OR software artifacts ) AND ( peer reviewed literature OR bibliometric analysis OR research studies ) AND ( application domains OR different domains OR domain variation ) )', 'dbs': 'scopus'}\n",
      "[extract_db_names] Input: scopus\n",
      "[extract_db_names] Extracted: ['scopus']\n",
      "[multi_db_search] Databases: ['scopus']\n",
      "Formatted Scopus Query: ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR modeling artifacts OR software artifacts ) AND ( peer reviewed literature OR bibliometric analysis OR research studies ) AND ( application domains OR different domains OR domain variation ) )\n",
      "Performing Scopus search for query: ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR modeling artifacts OR software artifacts ) AND ( peer reviewed literature OR bibliometric analysis OR research studies ) AND ( application domains OR different domains OR domain variation ) )\n",
      "Error during Scopus search: HTTPSConnectionPool(host='api.elsevier.com', port=443): Max retries exceeded with url: /content/search/scopus?count=25&view=COMPLETE&query=ALL+%28+%28+model+driven+engineering+OR+MDE+OR+digital+twins+OR+DTs+%29+AND+%28+automation+techniques+OR+modeling+artifacts+OR+software+artifacts+%29+AND+%28+peer+reviewed+literature+OR+bibliometric+analysis+OR+research+studies+%29+AND+%28+application+domains+OR+different+domains+OR+domain+variation+%29+%29&cursor=%2A (Caused by ResponseError('too many 500 error responses'))\n",
      "[Profiling] wrapper took 10.0221 seconds\n",
      "Error parsing results for scopus: Expecting value: line 1 column 1 (char 0)\n",
      "Total results found: 0\n",
      "Insufficient results; triggering relaxed (broaden) search string generation.\n",
      "[Profiling] wrapper took 4.8748 seconds\n",
      "[generate_search_string_node] Current iteration: 4\n",
      "Final String:content='ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR modeling artifacts OR software artifacts ) AND ( peer reviewed literature OR bibliometric analysis OR research studies ) AND ( application domains OR different domains OR domain variation ) )' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 576, 'total_tokens': 628, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'finish_reason': 'stop', 'logprobs': None} id='run-5eefa151-91a6-4907-b707-bf670d6a4437-0' usage_metadata={'input_tokens': 576, 'output_tokens': 52, 'total_tokens': 628, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Metadata saved to: 00-Query/00-Query-Agent-MDTE-open-mixtral-8x7b-1.0/JSON\\results.json\n",
      "[generate_search_string_node] Search string (final): ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR modeling artifacts OR software artifacts ) AND ( peer reviewed literature OR bibliometric analysis OR research studies ) AND ( application domains OR different domains OR domain variation ) )\n",
      "[Profiling] wrapper took 7.7727 seconds\n",
      "[extract_databases_node] Input text: \n",
      "Research Questions:\n",
      "\n",
      "RQ0: What are the bibliometric key facts of peer-reviewed literature documenting applications of MDE to DTs?\n",
      "RQ0.1: In which years are they published?\n",
      "RQ0.2: In which types of venues are they published?\n",
      "\n",
      "RQ1: How and how often are automation techniques applied to DTs in peer-reviewed literature?\n",
      "RQ1.1: How often are the different automation techniques applied in the context of DTs?\n",
      "RQ1.2: Which modeling artifacts and software artifacts are used by these automation techniques?\n",
      "RQ1.3: Which combinations of input and output artifacts are used by these automation techniques?\n",
      "RQ1.4: What is the research type of the studies that apply these automation techniques?\n",
      "\n",
      "RQ2: To which types of DTs are automation techniques applied in peer-reviewed literature?\n",
      "RQ2.1: Which TT does the DT represent, in which SLCP of the TT are automation techniques applied, and what is the TLCP of DTs to which automation techniques are applied?\n",
      "RQ2.2: How does the application of automation techniques (identified with RQ1) vary for different DT types?\n",
      "\n",
      "RQ3: In which domains are automation techniques applied to DTs in peer-reviewed literature?\n",
      "RQ3.1: For which domains are automation techniques applied to DTs?\n",
      "RQ3.2: How does the application of automation techniques (identified with RQ1) vary for the identified domains?\n",
      "RQ3.3: How does the DT type (identified with RQ2) vary for the identified domains?\n",
      "\n",
      "Database: Scopus\n",
      "            \n",
      "[extract_databases_node] Databases extracted: ['scopus']\n",
      "[Profiling] wrapper took 5.2047 seconds\n",
      "[format_queries_node] Search string: ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR modeling artifacts OR software artifacts ) AND ( peer reviewed literature OR bibliometric analysis OR research studies ) AND ( application domains OR different domains OR domain variation ) )\n",
      "[format_queries_node] Databases: ['scopus']\n",
      "[format_queries_node] Formatted queries: {'scopus': 'ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR modeling artifacts OR software artifacts ) AND ( peer reviewed literature OR bibliometric analysis OR research studies ) AND ( application domains OR different domains OR domain variation ) )'}\n",
      "[Profiling] wrapper took 5.4197 seconds\n",
      "[run_db_search_node] Formatted queries: {'scopus': 'ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR modeling artifacts OR software artifacts ) AND ( peer reviewed literature OR bibliometric analysis OR research studies ) AND ( application domains OR different domains OR domain variation ) )'}\n",
      "[multi_db_search_wrapper] Input JSON: {'query': 'ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR modeling artifacts OR software artifacts ) AND ( peer reviewed literature OR bibliometric analysis OR research studies ) AND ( application domains OR different domains OR domain variation ) )', 'dbs': 'scopus'}\n",
      "[extract_db_names] Input: scopus\n",
      "[extract_db_names] Extracted: ['scopus']\n",
      "[multi_db_search] Databases: ['scopus']\n",
      "Formatted Scopus Query: ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR modeling artifacts OR software artifacts ) AND ( peer reviewed literature OR bibliometric analysis OR research studies ) AND ( application domains OR different domains OR domain variation ) )\n",
      "Performing Scopus search for query: ALL ( ( model driven engineering OR MDE OR digital twins OR DTs ) AND ( automation techniques OR modeling artifacts OR software artifacts ) AND ( peer reviewed literature OR bibliometric analysis OR research studies ) AND ( application domains OR different domains OR domain variation ) )\n",
      "Error during Scopus search: HTTPSConnectionPool(host='api.elsevier.com', port=443): Max retries exceeded with url: /content/search/scopus?count=25&view=COMPLETE&query=ALL+%28+%28+model+driven+engineering+OR+MDE+OR+digital+twins+OR+DTs+%29+AND+%28+automation+techniques+OR+modeling+artifacts+OR+software+artifacts+%29+AND+%28+peer+reviewed+literature+OR+bibliometric+analysis+OR+research+studies+%29+AND+%28+application+domains+OR+different+domains+OR+domain+variation+%29+%29&cursor=%2A (Caused by ResponseError('too many 500 error responses'))\n",
      "[Profiling] wrapper took 11.0219 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "###################################\n",
    "#           LIBRARIES             #\n",
    "###################################\n",
    "\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import uuid\n",
    "import faiss\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pybliometrics.scopus import ScopusSearch, AbstractRetrieval\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET  # For parsing the Ecore file\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# LangChain and related modules\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.llms import Ollama\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.tools.base import Tool\n",
    "from typing import Callable, List, Dict, Any, TypedDict\n",
    "import json\n",
    "import re\n",
    "\n",
    "###################################\n",
    "#         GENERATION CHAIN        #\n",
    "###################################\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "###################################\n",
    "#         CONFIGURATION           #\n",
    "###################################\n",
    "\n",
    "# Define configuration paths and constants\n",
    "CONFIG_FILE = \"config/llm_config_mistral.json\"\n",
    "MODELS_FILE = \"config/llm_models.json\"\n",
    "CONFIG_RAG_FILE = \"config/llm_config_openai_rag.json\"\n",
    "LLM_TYPE = 'Ollama'  # Options: 'Others', 'Ollama'\n",
    "RAG_CHAT = 'OpenAI'  # Options: 'OpenAI', 'LangChain'\n",
    "SLR = 'MDTE'\n",
    "\n",
    "# Define minimum and maximum thresholds for retrieved papers\n",
    "min_threshold = 2     # Minimum desired number of papers\n",
    "max_threshold = 2000    # Maximum desired number of papers\n",
    "\n",
    "###################################\n",
    "#         UTILITY FUNCTIONS       #\n",
    "###################################\n",
    "\n",
    "# Function to load configuration from a JSON file\n",
    "def load_config(config_file):\n",
    "    try:\n",
    "        with open(config_file, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Configuration file {config_file} not found.\")\n",
    "        return {}\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Function to load file content\n",
    "def load_file_content(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {file_path} not found.\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to save content to a file\n",
    "def save_to_file(file_path, content):\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(content)\n",
    "\n",
    "# Function to save metadata to a file (in JSON format)\n",
    "def save_metadata(file_path, metadata):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(metadata, file, indent=4)\n",
    "\n",
    "###################################\n",
    "#         LLM CONFIGURATION       #\n",
    "###################################\n",
    "\n",
    "# Load LLM configuration\n",
    "config = load_config(CONFIG_FILE)\n",
    "models_config = load_config(MODELS_FILE)\n",
    "\n",
    "# Extract LLM parameters from configuration\n",
    "LLM = config.get(\"llm\")\n",
    "if not LLM:\n",
    "    raise ValueError(\"LLM name must be specified in the configuration file.\")\n",
    "\n",
    "PRICE_PER_INPUT_TOKEN = config.get(\"price_per_input_token\")\n",
    "PRICE_PER_OUTPUT_TOKEN = config.get(\"price_per_output_token\")\n",
    "temperature = config.get(\"temperature\")\n",
    "max_retries = config.get(\"max_retries\")\n",
    "api_key = config.get(\"api_keys\", {}).get(LLM.lower(), None)\n",
    "base_url = config.get(\"base_url\")\n",
    "\n",
    "# Determine LLM type and initialize LLM instance\n",
    "llm_config = models_config.get(LLM, None)\n",
    "if llm_config and LLM_TYPE != 'Ollama':\n",
    "    llm_params = llm_config.get(\"params\", {})\n",
    "    llm_params[\"temperature\"] = temperature\n",
    "    llm_params[\"max_retries\"] = max_retries\n",
    "    llm_params[\"api_key\"] = api_key\n",
    "    llm_params[\"base_url\"] = base_url\n",
    "\n",
    "    # Dynamically initialize the LLM class\n",
    "    llm_class = eval(llm_config[\"class\"])\n",
    "    llm_LangChain = llm_class(**llm_params)\n",
    "    model_name = LLM  # Use LLM name as the model name\n",
    "elif LLM_TYPE == 'Ollama':\n",
    "    llm_params = llm_config.get(\"params\", {})\n",
    "    llm_params[\"temperature\"] = temperature\n",
    "    llm_params[\"base_url\"] = base_url\n",
    "\n",
    "    llm_class = eval(llm_config[\"class\"])\n",
    "    llm_LangChain = llm_class(**llm_params)\n",
    "    model_name = LLM\n",
    "else:\n",
    "    raise ValueError(f\"Model configuration for '{LLM}' not found in {MODELS_FILE}.\")\n",
    "\n",
    "###################################\n",
    "#       CONFIGURATION FOLDERS     #\n",
    "###################################\n",
    "\n",
    "base_output_dir = f\"00-Query/00-Query-Agent-{SLR}-{model_name.lower()}-{temperature}\"\n",
    "base_output_json_dir = f\"00-Query/00-Query-Agent-{SLR}-{model_name.lower()}-{temperature}/JSON\"\n",
    "\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "os.makedirs(base_output_json_dir, exist_ok=True)\n",
    "\n",
    "filename = 'results.json'\n",
    "csv_filename = os.path.join(base_output_dir, \"Scopus_Search_results.csv\")\n",
    "csv_abstracts_filename = os.path.join(base_output_dir, \"Scopus_AbstractRetrieval_results.csv\")\n",
    "\n",
    "# output_trace_path = os.path.join(base_output_dir, file_name.replace(\".hepsy\", \".xes\"))\n",
    "# metadata_path = os.path.join(base_output_json_dir, file_name.replace(\".hepsy\", \".json\"))\n",
    "\n",
    "###################################\n",
    "#       PROFILING & CODECARBON    #\n",
    "###################################\n",
    "\n",
    "###################################\n",
    "#         GLOBAL PROFILING        #\n",
    "###################################\n",
    "\n",
    "# Global list to collect CodeCarbon metrics for each node call (per file)\n",
    "cc_metrics_for_file = []  # This will be reset for each file\n",
    "\n",
    "# Global list for overall CodeCarbon summary per file\n",
    "cc_summary_records = []\n",
    "\n",
    "# Global list to save profiling data\n",
    "profiling_records = []\n",
    "\n",
    "# Profiling Folder\n",
    "PROFILING_FOLDER = f\"00-Query/00-Query-Agent-{SLR}-{model_name.lower()}-{temperature}/JSON\"\n",
    "if not os.path.exists(PROFILING_FOLDER):\n",
    "    os.makedirs(PROFILING_FOLDER)\n",
    "PROFILING_CSV_FILE = os.path.join(PROFILING_FOLDER, \"profiling.csv\")\n",
    "\n",
    "# CodeCarbon Folder\n",
    "CODECARBON_FOLDER  = f\"00-Query/00-Query-Agent-{SLR}-{model_name.lower()}-{temperature}/JSON\"\n",
    "if not os.path.exists(CODECARBON_FOLDER ):\n",
    "    os.makedirs(CODECARBON_FOLDER )\n",
    "PROFILING_CSV_FILE = os.path.join(PROFILING_FOLDER, \"codecarbon_summary.csv\")\n",
    "\n",
    "# Folder to save evaluation results per file\n",
    "EVALUATION_FOLDER = f\"00-Query/00-Query-Agent-{SLR}-{model_name.lower()}-{temperature}/JSON\"\n",
    "if not os.path.exists(EVALUATION_FOLDER):\n",
    "    os.makedirs(EVALUATION_FOLDER)\n",
    "\n",
    "###################################\n",
    "#      TIMING NODE PROFILING      #\n",
    "###################################\n",
    "\n",
    "def timing_profile_node(func):\n",
    "    \"\"\"\n",
    "    Decorator to profile a node function.\n",
    "    Appends a record with the node name and its execution time (in seconds) to profiling_records.\n",
    "    \"\"\"\n",
    "    def wrapper(state, *args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(state, *args, **kwargs)\n",
    "        end = time.time()\n",
    "        elapsed = end - start\n",
    "        profiling_records.append({\"node\": func.__name__, \"execution_time\": elapsed})\n",
    "        print(f\"[Profiling] {func.__name__} took {elapsed:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "###################################\n",
    "#    CODECARBON NODE DECORATOR    #\n",
    "###################################\n",
    "\n",
    "# os.environ[\"CODECARBON_API_KEY\"] = \"CODECARBON_API_KEY\"\n",
    "# os.environ[\"CODECARBON_API_URL\"] = \"https://api.codecarbon.io\"\n",
    "# os.environ[\"CODECARBON_EXPERIMENT_ID\"] = \"UUID\"\n",
    "\n",
    "def cc_profile_node(func):\n",
    "    \"\"\"\n",
    "    Decorator that wraps a node function with CodeCarbon tracking.\n",
    "    It starts a tracker before calling the node and stops it right after.\n",
    "    The resulting metrics are appended to the global cc_metrics_for_file list.\n",
    "    \"\"\"\n",
    "    def wrapper(state, *args, **kwargs):\n",
    "        # Create a CodeCarbon tracker for this node\n",
    "        tracker = EmissionsTracker(\n",
    "            project_name=f\"cc_{func.__name__}\",\n",
    "            measure_power_secs=1,\n",
    "            output_dir=CODECARBON_FOLDER,  # You can adjust output_dir as needed (\".\")\n",
    "            allow_multiple_runs=True\n",
    "            # api_call_interval=4,\n",
    "            # experiment_id=experiment_id,\n",
    "            # save_to_api=True\n",
    "        )\n",
    "        tracker.start()\n",
    "        result = func(state, *args, **kwargs)\n",
    "        emissions = tracker.stop()\n",
    "        # Try to extract detailed metrics if available (from the internal attribute)\n",
    "        if hasattr(tracker, \"_final_emissions_data\"):\n",
    "            metrics = tracker._final_emissions_data\n",
    "        else:\n",
    "            metrics = {\"total_emissions\": emissions}\n",
    "        # Append the node's CodeCarbon metrics to the global list\n",
    "        cc_metrics_for_file.append({\n",
    "            \"node\": func.__name__,\n",
    "            **metrics  # Flatten the metrics dictionary\n",
    "        })\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "###################################\n",
    "#       PROFILE & CC DECORATORS   #\n",
    "###################################\n",
    "\n",
    "# (Assuming you already have a @profile_node decorator for timing, as in your code.)\n",
    "# Here we combine both decorators so that each node is profiled for time and CodeCarbon metrics.\n",
    "# The order of decorators means that cc_profile_node will wrap the function first.\n",
    "def profile_node(func):\n",
    "    return timing_profile_node(cc_profile_node(func))\n",
    "\n",
    "###################################\n",
    "#       LOAD URLS FROM CSV        #\n",
    "###################################\n",
    "\n",
    "def load_urls_from_csv(csv_file_path):\n",
    "    urls = []\n",
    "    try:\n",
    "        with open(csv_file_path, 'r', newline='', encoding='utf-8') as csv_file:\n",
    "            reader = csv.reader(csv_file)\n",
    "            for row in reader:\n",
    "                if row:  # Ensure the row is not empty\n",
    "                    urls.append(row[0].strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: CSV file '{csv_file_path}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "    return urls\n",
    "\n",
    "###################################\n",
    "#       GRAPH WORKFLOW SETUP      #\n",
    "###################################\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Extend GraphState to include keys from both retrieval and database parts\n",
    "class GraphState(TypedDict, total=False):\n",
    "    # Retrieval branch keys\n",
    "    question: str                       # The user's search question or refined query\n",
    "    generation: str                     # The generated answer or refined query output\n",
    "    documents: List[Any]                # List of retrieved documents\n",
    "    file_name: str                      # Name of the file being processed (if applicable)\n",
    "    context_llm: str                    # The refined context generated by the LLM\n",
    "    trace_status: str                   # Status of the processing trace\n",
    "    metadata: Dict[str, Any]            # Additional metadata related to the process\n",
    "    branch: str                         # Indicates which branch is being used ('retrieve' or 'web_search')\n",
    "    evaluation_metrics: Dict[str, float]  # Metrics evaluating the generated results\n",
    "    bert_score: Dict[str, float]        # BERTScore metrics for evaluating document support\n",
    "    web_bert_score: Dict[str, float]    # BERTScore metrics for evaluating web search branch results\n",
    "    skip_router: bool                   # Flag to bypass the routing node if not necessary\n",
    "    \n",
    "    # Research database branch keys\n",
    "    input_text: str                     # The initial input text provided by the user\n",
    "    raw_context: str                    # Raw context before any refinement\n",
    "    search_string: str                  # The final search string generated for database queries\n",
    "    databases: List[str]                # List of database names extracted from the input\n",
    "    formatted_queries: Dict[str, str]   # Queries formatted specifically for each database\n",
    "    db_results: Dict[str, str]          # Search results from each database (as JSON strings)\n",
    "    final_output: Dict[str, Any]        # Aggregated output containing search string and database results\n",
    "    \n",
    "    # Additional evaluation keys for query relaxation\n",
    "    min_results: int                    # Minimum number of articles expected to be returned\n",
    "    max_results: int                    # Maximum number of articles desired\n",
    "    relax_query: bool                   # Flag indicating whether the query should be relaxed to retrieve more results\n",
    "    adjusted_query: str                 # The adjusted query string after applying relaxation or modification\n",
    "    iteration: int                      # New field for iteration tracking\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "###################################\n",
    "# RAG AGENT SETUP (Chroma/FAISS)  #\n",
    "###################################\n",
    "\n",
    "# Load RAG configuration\n",
    "config_rag = load_config(CONFIG_RAG_FILE)\n",
    "api_key_rag = config_rag.get(\"api_keys\", {}).get(LLM.lower(), None)\n",
    "\n",
    "# Initialize RAG LLM and router\n",
    "LLM_RAG = config_rag.get(\"llm\")\n",
    "LLM_RAG_TEMP = config_rag.get(\"temperature\")\n",
    "\n",
    "if RAG_CHAT == 'OpenAI':\n",
    "    llm_rag = ChatOpenAI(model=LLM_RAG, temperature=LLM_RAG_TEMP)\n",
    "elif RAG_CHAT == 'LangChain':\n",
    "    llm_rag = OllamaFunctions(model=LLM) \n",
    "\n",
    "if RAG_CHAT == 'OpenAI':\n",
    "    llm_for_context = ChatOpenAI(model=LLM_RAG, temperature=LLM_RAG_TEMP)\n",
    "elif RAG_CHAT == 'LangChain':\n",
    "    llm_for_context = llm_LangChain\n",
    "\n",
    "###################################\n",
    "#       DATABASE BRANCH NODES     #\n",
    "# (from generate_search_string onward)\n",
    "###################################\n",
    "\n",
    "# --- Define simulated search functions for each database ---\n",
    "def scopus_search(query: str) -> str:\n",
    "\n",
    "    # query = \"TITLE-ABS-KEY(Digital AND twin AND federation)\"\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Perform the Scopus search using Pybliometrics\n",
    "    # -----------------------------\n",
    "    try:\n",
    "        print(\"Performing Scopus search for query:\", query)\n",
    "        # Using the STANDARD view for the initial search\n",
    "        scopus_search_res = ScopusSearch(query, view=\"COMPLETE\") # STANDARD\n",
    "        # Convert the results to a DataFrame\n",
    "        df_scopus = pd.DataFrame(scopus_search_res.results)\n",
    "    except Exception as e:\n",
    "        print(\"Error during Scopus search:\", e)\n",
    "        # Return an error string instead of calling exit(1)\n",
    "        return f\"Error during Scopus search: {e}\"\n",
    "    \n",
    "    results = scopus_search_res.results\n",
    "    print(f\"Number of results found: {len(results)}\")\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Filter the DataFrame to include only the desired columns\n",
    "    # -----------------------------\n",
    "    desired_columns = [\n",
    "        'eid', 'doi', 'title', 'subtype', 'subtypeDescription', 'creator', \n",
    "        'affilname', 'affiliation_city', 'affiliation_country', 'coverDate', \n",
    "        'coverDisplayDate', 'publicationName', 'issn', 'source_id', \n",
    "        'aggregationType', 'authkeywords', 'citedby_count', 'openaccess'\n",
    "    ]\n",
    "    df_filtered = df_scopus.reindex(columns=desired_columns)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Display the retrieved results on screen\n",
    "    # -----------------------------\n",
    "    # print(\"Displaying retrieved results:\")\n",
    "    # print(df_filtered)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Save the filtered DataFrame to a CSV file\n",
    "    # -----------------------------\n",
    "    # csv_filename = \"Scopus_Search_results.csv\"\n",
    "    \n",
    "    try:\n",
    "        df_filtered.to_csv(csv_filename, index=False, encoding=\"utf-8\")\n",
    "        print(f\"\\nResults have been saved to '{csv_filename}'\")\n",
    "    except Exception as e:\n",
    "        print(\"Error saving CSV:\", e)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Retrieve abstracts using AbstractRetrieval for each article and save to another CSV\n",
    "    # -----------------------------\n",
    "    \"\"\"\n",
    "    abstracts_list = []\n",
    "    abstracts_list_reduced = []\n",
    "    print(\"\\nRetrieving abstracts for each article...\")\n",
    "    \n",
    "    for index, row in df_filtered.iterrows():\n",
    "        try:\n",
    "            # Attempt retrieval using DOI if available; otherwise, use EID.\n",
    "            if pd.notnull(row['doi']) and row['doi'] != \"\":\n",
    "                abs_obj = AbstractRetrieval(row['doi'], view=\"FULL\")\n",
    "            elif pd.notnull(row['eid']) and row['eid'] != \"\":\n",
    "                abs_obj = AbstractRetrieval(row['eid'], view=\"FULL\")\n",
    "            else:\n",
    "                print(f\"No DOI or EID available for row index {index}\")\n",
    "                abs_obj = None\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving abstract for index {index}: {e}\")\n",
    "            abs_obj = None\n",
    "    \n",
    "        abstracts_list_reduced.append({\n",
    "            'eid': row['eid'],\n",
    "            'doi': row['doi'],\n",
    "            'title': row['title'],\n",
    "            'abstract': abs_obj.abstract if abs_obj is not None else None, \n",
    "            'authkeywords': abs_obj.authkeywords if abs_obj is not None else None,\n",
    "            'doi-link': \"http://doi.org/\" + row['doi'] if row.get('doi') else None,\n",
    "            'publicationName': row['publicationName'],\n",
    "            'aggregationType': row['aggregationType'],\n",
    "            'citedby_count': row['citedby_count'],\n",
    "            'openaccess': row['openaccess']\n",
    "        })\n",
    "        \n",
    "        abstracts_list.append({\n",
    "            'eid': row['eid'],\n",
    "            'doi': row['doi'],\n",
    "            'title': row['title'],\n",
    "            'abstract': abs_obj.description if abs_obj is not None else None, \n",
    "            'authkeywords': str(abs_obj.authkeywords) if abs_obj is not None else None, \n",
    "            'doi-link': \"http://doi.org/\" + row['doi'] if row.get('doi') else None,\n",
    "            'subtype': row['subtype'],\n",
    "            'subtypeDescription': row['subtypeDescription'],\n",
    "            'publicationName': row['publicationName'],\n",
    "            'publisher': str(abs_obj.publisher) if abs_obj is not None else None, \n",
    "            'authors': str(abs_obj.authors) if abs_obj is not None else None, \n",
    "            'creator':  row['creator'],\n",
    "            'affilname': row['affilname'],\n",
    "            'affiliation_city': row['affiliation_city'],\n",
    "            'affiliation_country': row['affiliation_country'],\n",
    "            'language': str(abs_obj.language) if abs_obj is not None else None, \n",
    "            'date_created': str(abs_obj.date_created) if abs_obj is not None else None, \n",
    "            'coverDate': row['coverDate'],\n",
    "            'coverDisplayDate': row['coverDisplayDate'],\n",
    "            'issn': row['issn'],\n",
    "            'isbn': str(abs_obj.isbn) if abs_obj is not None else None, \n",
    "            'source_id': row['source_id'],\n",
    "            'aggregationType': row['aggregationType'],\n",
    "            'citedby_count': row['citedby_count'],\n",
    "            'openaccess': row['openaccess'],\n",
    "            'openaccessFlag': str(abs_obj.openaccessFlag) if abs_obj is not None else None, \n",
    "            # 'abstract': str(abs_obj.abstract) if abs_obj is not None else None,\n",
    "            'refcount': str(abs_obj.refcount) if abs_obj is not None else None,\n",
    "            # 'references': str(abs_obj.references) if abs_obj is not None else None\n",
    "            'subject_areas': str(abs_obj.subject_areas) if abs_obj is not None else None,\n",
    "            'url': str(abs_obj.url) if abs_obj is not None else None,\n",
    "            'website': str(abs_obj.website) if abs_obj is not None else None,\n",
    "            'freetoread': row['freetoread'],\n",
    "            'freetoreadLabel': row['freetoreadLabel'],\n",
    "            'volume': row['volume'],\t\n",
    "            'issueIdentifier': row['issueIdentifier'],\t\n",
    "            'article_number': row['article_number'],\n",
    "            'pageRange': row['pageRange']\n",
    "        })\n",
    "    \n",
    "    # Create a DataFrame for abstracts and display it\n",
    "    df_results = pd.DataFrame(abstracts_list_reduced)\n",
    "    df_abstracts = pd.DataFrame(abstracts_list)\n",
    "    #print(\"\\nDisplaying retrieved abstracts:\")\n",
    "    #print(df_abstracts)\n",
    "    \n",
    "    # Save the abstracts DataFrame to a CSV file\n",
    "    # csv_abstracts_filename = \"Scopus_AbstractRetrieval_results.csv\"\n",
    "    try:\n",
    "        df_abstracts.to_csv(csv_abstracts_filename, index=False, encoding=\"utf-8\")\n",
    "        print(f\"\\nAbstracts have been saved to '{csv_abstracts_filename}'\")\n",
    "    except Exception as e:\n",
    "        print(\"Error saving abstracts CSV:\", e) \n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the DataFrame to a JSON string (list of records)\n",
    "    try:\n",
    "        df_json = df_filtered.to_json(orient=\"records\")\n",
    "        return df_json\n",
    "    except Exception as e:\n",
    "        print(\"Error converting DataFrame to JSON:\", e)\n",
    "        return json.dumps({\"error\": f\"Error converting DataFrame to JSON: {e}\"})\n",
    "\n",
    "def ieee_search(query: str) -> str:\n",
    "    return f\"IEEE results for query: '{query}'\"\n",
    "\n",
    "def sciencedirect_search(query: str) -> str:\n",
    "    return f\"ScienceDirect results for query: '{query}'\"\n",
    "\n",
    "# --- Map database names (lowercased) to tools ---\n",
    "db_tools = {\n",
    "    \"scopus\": Tool(\n",
    "        name=\"ScopusSearch\",\n",
    "        func=scopus_search,\n",
    "        description=\"Executes a search query on Scopus.\"\n",
    "    ),\n",
    "    \"ieee\": Tool(\n",
    "        name=\"IEEESearch\",\n",
    "        func=ieee_search,\n",
    "        description=\"Executes a search query on IEEE.\"\n",
    "    ),\n",
    "    \"sciencedirect\": Tool(\n",
    "        name=\"ScienceDirectSearch\",\n",
    "        func=sciencedirect_search,\n",
    "        description=\"Executes a search query on ScienceDirect.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "def extract_db_names(db_string: str) -> List[str]:\n",
    "    names = [db.strip().lower() for db in db_string.split(\",\") if db.strip()]\n",
    "    print(\"[extract_db_names] Input:\", db_string)\n",
    "    print(\"[extract_db_names] Extracted:\", names)\n",
    "    return names\n",
    "\n",
    "def format_db_query(db: str, query: str) -> str:\n",
    "    if db == \"scopus\":\n",
    "        # If the query already starts with TITLE-ABS-KEY/ALL, return it as-is.\n",
    "        if query.strip().upper().startswith(\"ALL\"):  # TITLE-ABS-KEY\n",
    "            return query\n",
    "        else:\n",
    "            return f\"ALL({query})\"  # TITLE-ABS-KEY\n",
    "            # return f\"{query}\"\n",
    "    elif db == \"ieee\":\n",
    "        formatted = f\"INDEXTERMS({query})\"\n",
    "    elif db == \"sciencedirect\":\n",
    "        formatted = f\"KEY({query})\"\n",
    "    else:\n",
    "        formatted = query\n",
    "    print(f\"[format_db_query] Database: {db} | Query: {query} | Formatted: {formatted}\")\n",
    "    return formatted\n",
    "\n",
    "def multi_db_search(query: str, dbs: str) -> str:\n",
    "    db_list = extract_db_names(dbs)\n",
    "    aggregated_results = {}\n",
    "    print(\"[multi_db_search] Databases:\", db_list)\n",
    "    for db in db_list:\n",
    "        if db in db_tools:\n",
    "            formatted_query = format_db_query(db, query)\n",
    "            print(\"Formatted Scopus Query:\", formatted_query)\n",
    "            tool = db_tools[db]\n",
    "            result = tool.func(formatted_query)\n",
    "            aggregated_results[db] = result\n",
    "        else:\n",
    "            aggregated_results[db] = f\"No tool available for database '{db}'.\"\n",
    "            # Simulate executing the search\n",
    "            #aggregated_results[db] = f\"Simulated result for {formatted_query}\" ############### ADD FUNCTION FOR SCOPYUS\n",
    "    aggregated_json = json.dumps(aggregated_results)\n",
    "    # print(\"[multi_db_search] Aggregated Results:\", aggregated_json)\n",
    "    return aggregated_json\n",
    "\n",
    "def multi_db_search_wrapper(state: Dict[str, Any], tool_input: str) -> str:\n",
    "    try:\n",
    "        data = json.loads(tool_input)\n",
    "        query = data.get(\"query\", \"\")\n",
    "        dbs = data.get(\"dbs\", \"\")\n",
    "        print(\"[multi_db_search_wrapper] Input JSON:\", data)\n",
    "        return multi_db_search(query, dbs)\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error parsing tool input: {str(e)}\"\n",
    "        \n",
    "        # Initialize iteration counter if not present, then increment it\n",
    "        if \"iteration\" not in state:\n",
    "            state[\"iteration\"] = 0\n",
    "        state[\"iteration\"] += 1\n",
    "        print(f\"[generate_search_string_node] Current iteration: {state['iteration']}\")\n",
    "    \n",
    "        # Define maximum iterations allowed before forcing a relaxed query\n",
    "        max_iterations = 10\n",
    "        if state[\"iteration\"] >= max_iterations:\n",
    "            print(\"[generate_search_string_node] Maximum iterations reached; switching to 'broaden' mode.\")\n",
    "            state[\"adjust_query\"] = \"broaden\"    \n",
    "        \n",
    "        print(\"[multi_db_search_wrapper] Error:\", error_message)\n",
    "        return error_message\n",
    "\n",
    "@profile_node\n",
    "def evaluate_results_node(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Checks the total number of results obtained from the database search.\n",
    "    If the number of results is below the minimum threshold, sets a flag to broaden the query.\n",
    "    If the number is above the maximum threshold, sets a flag to tighten the query.\n",
    "    Otherwise, no adjustment is needed.\n",
    "    \"\"\"\n",
    "\n",
    "    total_results = 0\n",
    "    for db, json_str in state.get(\"db_results\", {}).items():\n",
    "        try:\n",
    "            db_obj = json.loads(json_str)\n",
    "            # If the result object contains a key (like \"scopus\") with a JSON string\n",
    "            if isinstance(db_obj, dict) and \"scopus\" in db_obj:\n",
    "                articles = json.loads(db_obj[\"scopus\"])\n",
    "            elif isinstance(db_obj, list):\n",
    "                articles = db_obj\n",
    "            else:\n",
    "                articles = []\n",
    "            total_results += len(articles)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing results for {db}: {e}\")\n",
    "    \n",
    "    print(f\"Total results found: {total_results}\")\n",
    "    \n",
    "    # Determine if the query needs adjustment\n",
    "    if total_results < min_threshold:\n",
    "        # Not enough articles: broaden the query (remove some constraints)\n",
    "        state[\"relax_query\"] = True\n",
    "        state[\"adjust_query\"] = \"broaden\"\n",
    "        print(\"Insufficient results; triggering relaxed (broaden) search string generation.\")\n",
    "    elif total_results > max_threshold:\n",
    "        # Too many articles: tighten the query (add more constraints)\n",
    "        state[\"relax_query\"] = True\n",
    "        state[\"adjust_query\"] = \"tighten\"\n",
    "        print(\"Too many results; triggering tightened search string generation.\")\n",
    "    else:\n",
    "        state[\"relax_query\"] = False\n",
    "        state[\"adjust_query\"] = \"none\"\n",
    "        print(\"The number of results is within the desired range; no adjustment needed.\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# After obtaining search_text from the LLM\n",
    "# Remove any numbers inside quotes, but leave the PUBYEAR constraint intact.\n",
    "# This regex removes numbers inside quotes (if any), but you may need to adjust it to your specific output format.\n",
    "def remove_numeric_keywords(search_str: str) -> str:\n",
    "    # This pattern removes digits that are inside double quotes, e.g., \"2023\"\n",
    "    return re.sub(r'\"(\\d+)\"', '\"\"', search_str)\n",
    "\n",
    "\n",
    "@profile_node\n",
    "def generate_search_string_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generates an optimized search string using the input text.\n",
    "    If 'relax_query' is True, the query is adjusted according to 'adjust_query':\n",
    "      - 'broaden': generates a broader query by either adding alternative keywords (joined by OR) within groups or by removing one or more entire groups (joined by AND),\n",
    "         based on the current iteration.\n",
    "      - 'tighten': generates a more specific query by adding more constraints.\n",
    "    The prompt uses the iteration number to vary the search string.\n",
    "    Also appends a publication year constraint if the input text contains a time limit.\n",
    "    \"\"\"\n",
    "    query_user_input = state[\"input_text\"]\n",
    "    metadata_path = os.path.join(base_output_json_dir, filename)\n",
    "\n",
    "    # Initialize iteration counter if not present, then increment it.\n",
    "    if \"iteration\" not in state:\n",
    "        state[\"iteration\"] = 0\n",
    "    state[\"iteration\"] += 1\n",
    "    current_iter = state[\"iteration\"]\n",
    "    print(f\"[generate_search_string_node] Current iteration: {current_iter}\")\n",
    "\n",
    "    # Define maximum iterations allowed before forcing a relaxed query.\n",
    "    max_iterations = 10\n",
    "    if current_iter >= max_iterations:\n",
    "        print(\"[generate_search_string_node] Maximum iterations reached; switching to 'broaden' mode.\")\n",
    "        state[\"adjust_query\"] = \"broaden\"    \n",
    "\n",
    "    adjust_query = state.get(\"adjust_query\", \"none\")\n",
    "\n",
    "    # Define the system prompt according to adjustment mode.\n",
    "    # TITLE-ABS-KEY\n",
    "    if adjust_query == \"broaden\":\n",
    "        system_prompt = (\n",
    "            \"You are an expert at generating search strings for research queries. \"\n",
    "            \"The current iteration number is {iteration}. \"\n",
    "            \"Generate a broader search string by relaxing the constraints in two ways: \"\n",
    "            \"1) Randomly insert a number of new keywords (between 1 and 5) as additional OR alternatives within existing groups; \"\n",
    "            \"2) Randomly remove a number of keywords (between 1 and 5) from groups, while keeping the most representative keywords. \"\n",
    "            \"For each group, add synonyms or domain-related keywords using OR so that the group covers alternative terms. \"\n",
    "            \"Each group represents a parallel domain defined by the research questions, and groups are combined with AND. \"\n",
    "            \"Each keyword must be a natural phrase or term, and must use spaces between words. Do not concatenate multiple words into a single token.\"\n",
    "            \"Return the search string in the following exact format (do not include any extra text):\\n\\n\"\n",
    "            \"ALL ( ( Keyword1 OR Keyword2 OR ... OR KeywordN ) AND \"\n",
    "            \"( KeywordA OR KeywordB OR ... OR KeywordM ) AND ... )\\n\\n\"\n",
    "            \"Do not include the publication year as a keyword.\"\n",
    "        )\n",
    "    elif adjust_query == \"tighten\":\n",
    "        system_prompt = (\n",
    "            \"You are an expert at generating search strings for research queries. \"\n",
    "            \"The current iteration number is {iteration}. \"\n",
    "            \"Generate a more specific search string by adding additional specific keywords to each group while still respecting the research questions and goals. \"\n",
    "            \"Ensure that within each group keywords are combined with OR and groups are combined with AND. \"\n",
    "            \"All keywords must be unique. \"\n",
    "            \"Each keyword must be a natural phrase or term, and must use spaces between words. Do not concatenate multiple words into a single token.\"\n",
    "            \"Return the search string in the following exact format (do not include any extra text):\\n\\n\"\n",
    "            \"ALL ( ( Keyword1 OR Keyword2 OR ... OR KeywordN ) AND \"\n",
    "            \"( KeywordA OR KeywordB OR ... OR KeywordM ) AND ... )\\n\\n\"\n",
    "            \"Do not include the publication year as a keyword.\"\n",
    "        )\n",
    "    else:\n",
    "        system_prompt = (\n",
    "            \"You are an expert at generating search strings for research queries. \"\n",
    "            \"The current iteration number is {iteration}. \"\n",
    "            \"Generate a search string using only the logical operators OR and AND, where within each group keywords are combined with OR and groups are combined with AND. \"\n",
    "            \"Ensure that all keywords in the search string are unique and that the search string varies with each iteration. \"\n",
    "            \"Each keyword must be a natural phrase or term, and must use spaces between words. Do not concatenate multiple words into a single token.\"\n",
    "            \"Return the search string in the following exact format (do not include any extra text):\\n\\n\"\n",
    "            \"ALL ( ( Keyword1 OR Keyword2 OR ... OR KeywordN ) AND \"\n",
    "            \"( KeywordA OR KeywordB OR ... OR KeywordM ) AND ... )\\n\\n\"\n",
    "            \"Do not include the publication year as a keyword.\"\n",
    "        )\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", \n",
    "         f\"User question:\\n{query_user_input}\\n\\n\"\n",
    "         \"Based on the user question and the current iteration, generate an optimal search string composed solely of essential, unique keywords. \"\n",
    "         \"Use the following structure: within each group, keywords are combined with OR; between groups, use AND. \"\n",
    "         \"If necessary to increase the number of results, modify the query by either adding new keywords (1 to 5) in OR or by removing one or more groups. \"\n",
    "         \"Return only the search string in the exact format specified above.\\n\\n\"\n",
    "         \"Search String:\"\n",
    "         )\n",
    "    ])\n",
    "\n",
    "    # search_string = (prompt_template | llm_for_context).invoke({\"context_llm\": context_llm})\n",
    "    # Invoke the LLM chain for string generation\n",
    "    start_time_llm = time.time()\n",
    "    search_string = (prompt_template | llm_for_context).invoke({\"iteration\": current_iter}) \n",
    "\n",
    "    print(f\"Final String:{search_string}\")\n",
    "    end_time_llm = time.time()\n",
    "    execution_time = end_time_llm - start_time_llm\n",
    "    \n",
    "    # Extract the string from the result\n",
    "    if LLM_TYPE != 'Ollama':\n",
    "        string_output = search_string.content.strip()\n",
    "    else:\n",
    "        # string_output = search_string.strip()\n",
    "        # Extract the string from the result\n",
    "        if hasattr(search_string, \"content\"):\n",
    "            string_output = search_string.content.strip()\n",
    "        else:\n",
    "            string_output = str(search_string).strip()\n",
    "    \n",
    "    # Build metadata for the response\n",
    "    if LLM_TYPE != 'Ollama':\n",
    "        metadata = {\n",
    "            \"response_length\": len(string_output),\n",
    "            \"execution_time\": execution_time,\n",
    "            \"temperature\": temperature,\n",
    "            \"usage\": search_string.usage_metadata,\n",
    "            \"price_usd\": search_string.usage_metadata.get(\"input_tokens\", 0) * PRICE_PER_INPUT_TOKEN +\n",
    "                         search_string.usage_metadata.get(\"output_tokens\", 0) * PRICE_PER_OUTPUT_TOKEN,\n",
    "            \"model_name\": model_name\n",
    "        }\n",
    "    else:\n",
    "        metadata = {\n",
    "            \"response_length\": len(string_output),\n",
    "            \"execution_time\": execution_time,\n",
    "            \"temperature\": temperature,\n",
    "            \"model_name\": model_name\n",
    "        }\n",
    "\n",
    "    # Save the string and metadata to output files\n",
    "    save_metadata(metadata_path, metadata)\n",
    "\n",
    "    print(f\"Metadata saved to: {metadata_path}\")\n",
    "    \n",
    "    search_text = search_string.content if hasattr(search_string, \"content\") else str(search_string)\n",
    "    search_text = search_text.strip()\n",
    "\n",
    "    cleaned_search_text_year = remove_numeric_keywords(search_text)\n",
    "    # Further clean-up if needed (e.g., remove extra spaces)\n",
    "    cleaned_search_text_year = re.sub(r'\\s+', ' ', cleaned_search_text_year).strip()\n",
    "\n",
    "    # If the user has entered a time constraint in the input, such as \"after 2020\", add the PUBYEAR constraint.\n",
    "    year_match = re.search(r'after\\s+(\\d{4})', state.get(\"input_text\", \"\"), re.IGNORECASE)\n",
    "    if year_match:\n",
    "        year = year_match.group(1)\n",
    "        cleaned_search_text_year += f\" AND PUBYEAR > {year}\"\n",
    "    \n",
    "    cleaned_search_text = re.sub(r'\\s*site:\\S+(?:\\s*OR\\s*site:\\S+)+', '', cleaned_search_text_year)\n",
    "    state[\"search_string\"] = cleaned_search_text\n",
    "    print(\"[generate_search_string_node] Search string (final):\", state[\"search_string\"])\n",
    "    return state\n",
    "\n",
    "@profile_node\n",
    "def extract_databases_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extracts a list of database names from the input_text.\n",
    "    It looks for an explicit \"databases:\" segment and also searches for known\n",
    "    database names (e.g., scopus, ieee, sciencedirect) anywhere in the input.\n",
    "    If no known databases are found, the system notifies the user and requests\n",
    "    a valid database to be entered.\n",
    "    \"\"\"\n",
    "    input_text = state.get(\"input_text\", \"\")\n",
    "    print(\"[extract_databases_node] Input text:\", input_text)\n",
    "    lower_text = input_text.lower()\n",
    "    extracted_dbs = []\n",
    "    \n",
    "    # First, check for an explicit \"databases:\" segment.\n",
    "    if \"databases:\" in lower_text:\n",
    "        idx = lower_text.find(\"databases:\")\n",
    "        db_string = input_text[idx + len(\"databases:\"):].strip()\n",
    "        extracted_dbs = extract_db_names(db_string)\n",
    "    \n",
    "    # List of known database names to search for.\n",
    "    known_dbs = [\"scopus\", \"ieee\", \"sciencedirect\"]\n",
    "    # Check if any of the known database names appear in the input text.\n",
    "    for db in known_dbs:\n",
    "        if db in lower_text and db not in extracted_dbs:\n",
    "            extracted_dbs.append(db)\n",
    "\n",
    "    # If no known databases were found, notify the user and prompt for input.\n",
    "    if not extracted_dbs:\n",
    "        print(\"[extract_databases_node] No known databases found in the input.\")\n",
    "        valid_db_input = input(\"Please enter a valid database (e.g., scopus, ieee, sciencedirect): \")\n",
    "        extracted_dbs = extract_db_names(valid_db_input)\n",
    "        # Validate that the provided database is one of the known ones.\n",
    "        valid_extracted_dbs = [db for db in extracted_dbs if db in known_dbs]\n",
    "        if not valid_extracted_dbs:\n",
    "            print(\"[extract_databases_node] Invalid database entered. Please try again.\")\n",
    "            valid_db_input = input(\"Please enter a valid database (e.g., scopus, ieee, sciencedirect): \")\n",
    "            extracted_dbs = extract_db_names(valid_db_input)\n",
    "            valid_extracted_dbs = [db for db in extracted_dbs if db in known_dbs]\n",
    "        extracted_dbs = valid_extracted_dbs\n",
    "    \n",
    "    state[\"databases\"] = extracted_dbs\n",
    "    print(\"[extract_databases_node] Databases extracted:\", state[\"databases\"])\n",
    "    return state\n",
    "\n",
    "@profile_node\n",
    "def format_queries_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Formats the search_string into specific queries for each database.\n",
    "    \"\"\"\n",
    "    search_string = state.get(\"search_string\", \"\")\n",
    "    databases = state.get(\"databases\", [])\n",
    "    print(\"[format_queries_node] Search string:\", search_string)\n",
    "    print(\"[format_queries_node] Databases:\", databases)\n",
    "    formatted = {}\n",
    "    for db in databases:\n",
    "        formatted[db] = format_db_query(db, search_string)\n",
    "    state[\"formatted_queries\"] = formatted\n",
    "    print(\"[format_queries_node] Formatted queries:\", state[\"formatted_queries\"])\n",
    "    return state\n",
    "\n",
    "@profile_node\n",
    "def run_db_search_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Executes the search for each database using the formatted queries.\n",
    "    \"\"\"\n",
    "    formatted_queries = state.get(\"formatted_queries\", {})\n",
    "    print(\"[run_db_search_node] Formatted queries:\", formatted_queries)\n",
    "    results = {}\n",
    "    for db, f_query in formatted_queries.items():\n",
    "        tool_input = json.dumps({\"query\": f_query, \"dbs\": db})\n",
    "        result = multi_db_search_wrapper(state, tool_input)\n",
    "        results[db] = result\n",
    "    state[\"db_results\"] = results\n",
    "    # print(\"[run_db_search_node] Database results:\", state[\"db_results\"])\n",
    "    return state\n",
    "\n",
    "@profile_node\n",
    "def aggregate_results_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Aggregates the search_string and database results into a final output.\n",
    "    \"\"\"\n",
    "    state[\"final_output\"] = {\n",
    "        \"search_string\": state.get(\"search_string\", \"\"),\n",
    "        \"db_results\": state.get(\"db_results\", {})\n",
    "    }\n",
    "    # print(\"[aggregate_results_node] Final output:\", state[\"final_output\"])\n",
    "    return state\n",
    "\n",
    "###################################\n",
    "#         ROUTER QUESTION         #\n",
    "###################################\n",
    "\n",
    "# Starting node\n",
    "workflow.add_edge(START, \"generate_search_string\") # generate_query\n",
    "\n",
    "# Database branch nodes (from generate_search_string onward)\n",
    "workflow.add_node(\"generate_search_string\", generate_search_string_node)\n",
    "workflow.add_node(\"extract_databases\", extract_databases_node)\n",
    "workflow.add_node(\"format_queries\", format_queries_node)\n",
    "workflow.add_node(\"run_db_search\", run_db_search_node)\n",
    "workflow.add_node(\"aggregate_results\", aggregate_results_node)\n",
    "\n",
    "# After cache_context, proceed with database branch nodes\n",
    "# workflow.add_edge(\"cache_context\", \"generate_search_string\")\n",
    "workflow.add_edge(\"generate_search_string\", \"extract_databases\")\n",
    "workflow.add_edge(\"extract_databases\", \"format_queries\")\n",
    "workflow.add_edge(\"format_queries\", \"run_db_search\")\n",
    "\n",
    "# Add the node to evaluate the results\n",
    "workflow.add_node(\"evaluate_results\", evaluate_results_node)\n",
    "\n",
    "# Modify the flow: after run_db_search, proceed to evaluate_results\n",
    "workflow.add_edge(\"run_db_search\", \"evaluate_results\")\n",
    "\n",
    "# If evaluate_results sets relax_query = True, redirect to generate_search_string\n",
    "workflow.add_conditional_edges(\n",
    "    \"evaluate_results\",\n",
    "    lambda state: \"relax\" if state.get(\"relax_query\") else \"continue\",\n",
    "    {\n",
    "        \"relax\": \"generate_search_string\",  # Return to the node to regenerate the query (in relax mode)\n",
    "        \"continue\": \"aggregate_results\"     # Continue with aggregation if the results are sufficient\n",
    "    },\n",
    ")\n",
    "\n",
    "# workflow.add_edge(\"run_db_search\", \"aggregate_results\")\n",
    "workflow.add_edge(\"aggregate_results\", END)\n",
    "\n",
    "# (Optional) Graph visualization\n",
    "\"\"\"\n",
    "try:\n",
    "    from IPython.display import display, Markdown, Image\n",
    "    graph = workflow.compile().get_graph()\n",
    "    graph.mermaid_config = {\"graph_direction\": \"TD\"}\n",
    "    png_bytes = graph.draw_mermaid_png()\n",
    "    with open(\"graph.png\", \"wb\") as f:\n",
    "        f.write(png_bytes)\n",
    "    print(\"The graph has been saved as 'graph.png'.\")\n",
    "    display(Markdown(\"### LangGraph Visualization ###\"))\n",
    "    display(Image(png_bytes))\n",
    "except Exception as e:\n",
    "    print(\"Graph rendering failed:\", e)\n",
    "\"\"\"\n",
    "\n",
    "###################################\n",
    "#       EXECUTION OF WORKFLOW     #\n",
    "###################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initial state: note that for the database part, input_text is required\n",
    "    \"\"\"\n",
    "    user_text = (\n",
    "                Purpose To identify and classify existing solutions leveraging the combination of MDE, DevOps, and AI/ML principles and practices supporting the system and software engineering of cyber-physical systems from the point of view of researchers and practitioners.\n",
    "                RQ1 : Is there a systems and software engineering methodology that explicitly incorporates and integrates the principles and practices of MDE, AI/ML, and DevOps research areas? If such a methodology exists, how does it combine these research areas?\n",
    "                RQ2 : Are the principles and practices of MDE, AI/ML, and DevOps research areas integrated throughout the entire process, or are they applied to specific engineering activities?\n",
    "                RQ3 : Which research fields and application domains are the target of these approaches?\n",
    "                RQ4 : What are the future research directions?\n",
    "                Database: scopus\n",
    "                Pubblication year higher then 2006\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    user_text = (\"\"\"\n",
    "Research Questions:\n",
    "\n",
    "RQ0: What are the bibliometric key facts of peer-reviewed literature documenting applications of MDE to DTs?\n",
    "RQ0.1: In which years are they published?\n",
    "RQ0.2: In which types of venues are they published?\n",
    "\n",
    "RQ1: How and how often are automation techniques applied to DTs in peer-reviewed literature?\n",
    "RQ1.1: How often are the different automation techniques applied in the context of DTs?\n",
    "RQ1.2: Which modeling artifacts and software artifacts are used by these automation techniques?\n",
    "RQ1.3: Which combinations of input and output artifacts are used by these automation techniques?\n",
    "RQ1.4: What is the research type of the studies that apply these automation techniques?\n",
    "\n",
    "RQ2: To which types of DTs are automation techniques applied in peer-reviewed literature?\n",
    "RQ2.1: Which TT does the DT represent, in which SLCP of the TT are automation techniques applied, and what is the TLCP of DTs to which automation techniques are applied?\n",
    "RQ2.2: How does the application of automation techniques (identified with RQ1) vary for different DT types?\n",
    "\n",
    "RQ3: In which domains are automation techniques applied to DTs in peer-reviewed literature?\n",
    "RQ3.1: For which domains are automation techniques applied to DTs?\n",
    "RQ3.2: How does the application of automation techniques (identified with RQ1) vary for the identified domains?\n",
    "RQ3.3: How does the DT type (identified with RQ2) vary for the identified domains?\n",
    "\n",
    "Database: Scopus\n",
    "            \"\"\"\n",
    "    )\n",
    "    \n",
    "    # user_text = input(\"Enter the search text: \")\n",
    "    \n",
    "    initial_state: GraphState = {\n",
    "        \"input_text\": user_text\n",
    "    }\n",
    "    final_state = list(workflow.compile().stream(initial_state, config={\"recursion_limit\": 100}))[-1]\n",
    "    \n",
    "    print(\"\\n--- FINAL WORKFLOW STATE ---\\n\")\n",
    "    \n",
    "    # If the useful data is under \"final_output\", use that; otherwise, use final_state directly.\n",
    "    data = final_state.get(\"aggregate_results\", final_state)\n",
    "    \n",
    "    # Print the various elements\n",
    "    print(\"\\n--- FINAL WORKFLOW STATE ---\\n\")\n",
    "    \n",
    "    print(\"Input Text:\")\n",
    "    print(json.dumps(data.get(\"input_text\", {}), indent=4))\n",
    "    \n",
    "    print(\"\\nDatabases:\")\n",
    "    print(json.dumps(data.get(\"databases\", {}), indent=4))\n",
    "    \n",
    "    print(\"\\nFormatted Queries:\")\n",
    "    print(json.dumps(data.get(\"formatted_queries\", {}), indent=4))\n",
    "    \n",
    "    print(\"\\nSearch String:\")\n",
    "    print(json.dumps(data.get(\"search_string\", {}), indent=4))\n",
    "    \n",
    "    print(\"\\nDB Results:\")\n",
    "    #print(json.dumps(data.get(\"db_results\", {}), indent=4))\n",
    "    db_results = data.get(\"db_results\", {})\n",
    "\n",
    "    # For each database (here, \"scopus\")\n",
    "    for db, json_str in db_results.items():\n",
    "        print(f\"\\nResults for {db}:\")\n",
    "    \n",
    "        # Convert the JSON string into a Python object\n",
    "        try:\n",
    "            # In our case the value is something like '{\"scopus\": \"[{...}, {...}]\" }'\n",
    "            db_obj = json.loads(json_str)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing results for {db}: {e}\")\n",
    "            continue\n",
    "    \n",
    "        # If the structure has a key (like \"scopus\") containing a JSON string,\n",
    "        # parse that as well.\n",
    "        if isinstance(db_obj, dict) and \"scopus\" in db_obj:\n",
    "            try:\n",
    "                articles = json.loads(db_obj[\"scopus\"])\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing articles for {db}: {e}\")\n",
    "                continue\n",
    "        else:\n",
    "            articles = db_obj\n",
    "    \n",
    "        # Now, iterate through the list of articles and print each article in a pretty format\n",
    "        for article in articles:\n",
    "            print(json.dumps(article, indent=4))\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    # Print aggregate results (if available)\n",
    "    \"\"\"\n",
    "    if \"aggregate_results\" in final_state:\n",
    "        print(\"Aggregate Results:\")\n",
    "        print(json.dumps(final_state[\"aggregate_results\"], indent=4))\n",
    "        print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "    else:\n",
    "        print(\"No aggregate_results found.\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # Optionally, print the entire final_state\n",
    "    # print(\"Full Final State:\")\n",
    "    # print(json.dumps(final_state, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe5f721-47a4-45d4-8198-35151d4d405a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
