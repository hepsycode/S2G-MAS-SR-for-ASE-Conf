{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7a9cf8ed-31d9-4225-8fb2-c195d997455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "###################################\n",
    "#           LIBRARIES             #\n",
    "###################################\n",
    "\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import uuid\n",
    "import faiss\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pybliometrics.scopus import ScopusSearch, AbstractRetrieval\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET  # For parsing the Ecore file\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# LangChain and related modules\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.llms import Ollama\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.tools.base import Tool\n",
    "from typing import Callable, List, Dict, Any, TypedDict\n",
    "import json\n",
    "import re\n",
    "\n",
    "###################################\n",
    "#         CONFIGURATION           #\n",
    "###################################\n",
    "\n",
    "# Define configuration paths and constants\n",
    "CONFIG_FILE = \"config/llm_config_mistral.json\"\n",
    "MODELS_FILE = \"config/llm_models.json\"\n",
    "CONFIG_RAG_FILE = \"config/llm_config_openai_rag.json\"\n",
    "CONFIG_RAG_TAVILY_FILE = \"config/secrets-master-llm.json\"\n",
    "VECTOR_DB_TYPE = \"FAISS\"  # Options: FAISS, CHROMA\n",
    "CSV_FILE_PATH = \"config/BASE_URL.csv\"\n",
    "LLM_TYPE = 'Others'  # Options: 'Others', 'Ollama'\n",
    "RAG_CHAT = 'OpenAI'  # Options: 'OpenAI', 'LangChain', 'Mistral'\n",
    "FORCE_CONTEXT_GEN = False\n",
    "# REFINED_CONTEXT_PATH = \"config/refined_context.json\"\n",
    "SLR = 'MDTE'\n",
    "\n",
    "# Define minimum and maximum thresholds for retrieved papers\n",
    "min_threshold = 1     # Minimum desired number of papers\n",
    "max_threshold = 2000    # Maximum desired number of papers\n",
    "\n",
    "# Define an array with all the topics/tools for retrieval\n",
    "vectorstore_topics = [\n",
    "    \"CAEX/AutomationML\",\n",
    "    \"BPMN Designer\",\n",
    "    # \"HEPSYCODE\",\n",
    "    # \"Additional Tool 1\",\n",
    "    # \"Additional Tool 2\",\n",
    "    # Add more topics as needed\n",
    "]\n",
    "\n",
    "###################################\n",
    "#         UTILITY FUNCTIONS       #\n",
    "###################################\n",
    "\n",
    "# Function to load configuration from a JSON file\n",
    "def load_config(config_file):\n",
    "    try:\n",
    "        with open(config_file, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Configuration file {config_file} not found.\")\n",
    "        return {}\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Function to load file content\n",
    "def load_file_content(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {file_path} not found.\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to save content to a file\n",
    "def save_to_file(file_path, content):\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(content)\n",
    "\n",
    "# Function to save metadata to a file (in JSON format)\n",
    "def save_metadata(file_path, metadata):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(metadata, file, indent=4)\n",
    "\n",
    "###################################\n",
    "#         LLM CONFIGURATION       #\n",
    "###################################\n",
    "\n",
    "# Load LLM configuration\n",
    "config = load_config(CONFIG_FILE)\n",
    "models_config = load_config(MODELS_FILE)\n",
    "\n",
    "# Extract LLM parameters from configuration\n",
    "LLM = config.get(\"llm\")\n",
    "if not LLM:\n",
    "    raise ValueError(\"LLM name must be specified in the configuration file.\")\n",
    "\n",
    "PRICE_PER_INPUT_TOKEN = config.get(\"price_per_input_token\")\n",
    "PRICE_PER_OUTPUT_TOKEN = config.get(\"price_per_output_token\")\n",
    "temperature = config.get(\"temperature\")\n",
    "max_retries = config.get(\"max_retries\")\n",
    "api_key = config.get(\"api_keys\", {}).get(LLM.lower(), None)\n",
    "base_url = config.get(\"base_url\")\n",
    "\n",
    "# Determine LLM type and initialize LLM instance\n",
    "llm_config = models_config.get(LLM, None)\n",
    "if llm_config and LLM_TYPE != 'Ollama':\n",
    "    llm_params = llm_config.get(\"params\", {})\n",
    "    llm_params[\"temperature\"] = temperature\n",
    "    llm_params[\"max_retries\"] = max_retries\n",
    "    llm_params[\"api_key\"] = api_key\n",
    "    llm_params[\"base_url\"] = base_url\n",
    "\n",
    "    # Dynamically initialize the LLM class\n",
    "    llm_class = eval(llm_config[\"class\"])\n",
    "    llm_LangChain = llm_class(**llm_params)\n",
    "    model_name = LLM  # Use LLM name as the model name\n",
    "elif LLM_TYPE == 'Ollama':\n",
    "    llm_params = llm_config.get(\"params\", {})\n",
    "    llm_params[\"temperature\"] = temperature\n",
    "    llm_params[\"base_url\"] = base_url\n",
    "\n",
    "    llm_class = eval(llm_config[\"class\"])\n",
    "    llm_LangChain = llm_class(**llm_params)\n",
    "    model_name = LLM\n",
    "else:\n",
    "    raise ValueError(f\"Model configuration for '{LLM}' not found in {MODELS_FILE}.\")\n",
    "\n",
    "###################################\n",
    "#       CONFIGURATION FOLDERS     #\n",
    "###################################\n",
    "\n",
    "base_output_dir = f\"00-Query/00-Query-Agent-RAG-WEB-{SLR}-{model_name.lower()}-{temperature}\"\n",
    "base_output_json_dir = f\"00-Query/00-Query-Agent-RAG-WEB-{SLR}-{model_name.lower()}-{temperature}/JSON\"\n",
    "\n",
    "REFINED_CONTEXT_PATH = f\"00-Query/00-Query-Agent-RAG-WEB-{SLR}-{model_name.lower()}-{temperature}/refined_context.json\"\n",
    "\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "os.makedirs(base_output_json_dir, exist_ok=True)\n",
    "\n",
    "filename = 'results.json'\n",
    "csv_filename = os.path.join(base_output_dir, \"Scopus_Search_results.csv\")\n",
    "csv_abstracts_filename = os.path.join(base_output_dir, \"Scopus_AbstractRetrieval_results.csv\")\n",
    "\n",
    "# output_trace_path = os.path.join(base_output_dir, file_name.replace(\".hepsy\", \".xes\"))\n",
    "# metadata_path = os.path.join(base_output_json_dir, file_name.replace(\".hepsy\", \".json\"))\n",
    "\n",
    "###################################\n",
    "#       PROFILING & CODECARBON    #\n",
    "###################################\n",
    "\n",
    "###################################\n",
    "#         GLOBAL PROFILING        #\n",
    "###################################\n",
    "\n",
    "# Global list to collect CodeCarbon metrics for each node call (per file)\n",
    "cc_metrics_for_file = []  # This will be reset for each file\n",
    "\n",
    "# Global list for overall CodeCarbon summary per file\n",
    "cc_summary_records = []\n",
    "\n",
    "# Global list to save profiling data\n",
    "profiling_records = []\n",
    "\n",
    "# Profiling Folder\n",
    "PROFILING_FOLDER = f\"00-Query/00-Query-Agent-RAG-WEB-{SLR}-{model_name.lower()}-{temperature}/JSON\"\n",
    "if not os.path.exists(PROFILING_FOLDER):\n",
    "    os.makedirs(PROFILING_FOLDER)\n",
    "PROFILING_CSV_FILE = os.path.join(PROFILING_FOLDER, \"profiling.csv\")\n",
    "\n",
    "# CodeCarbon Folder\n",
    "CODECARBON_FOLDER  = f\"00-Query/00-Query-Agent-RAG-WEB-{SLR}-{model_name.lower()}-{temperature}/JSON\"\n",
    "if not os.path.exists(CODECARBON_FOLDER ):\n",
    "    os.makedirs(CODECARBON_FOLDER )\n",
    "PROFILING_CSV_FILE = os.path.join(PROFILING_FOLDER, \"codecarbon_summary.csv\")\n",
    "\n",
    "# Folder to save evaluation results per file\n",
    "EVALUATION_FOLDER = f\"00-Query/00-Query-Agent-RAG-WEB-{SLR}-{model_name.lower()}-{temperature}/JSON\"\n",
    "if not os.path.exists(EVALUATION_FOLDER):\n",
    "    os.makedirs(EVALUATION_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3df2be90-8da3-4384-a3d8-efb57b62f6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9 URLs from 'config/BASE_URL.csv'.\n",
      "The folder 'faiss' already exists.\n",
      "Loading existing FAISS index from disk...\n",
      "Refined context file exists. Skipping query generation; proceeding directly to cache_context_node.\n",
      "[Profiling] wrapper took 4.8907 seconds\n",
      "Skipping routing; moving directly to cache_context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vitto\\anaconda3\\lib\\site-packages\\codecarbon\\output_methods\\file.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame.from_records([dict(total.values)])])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded refined context from file cache (LangGraph node).\n",
      "[Profiling] wrapper took 4.8939 seconds\n",
      "[generate_search_string_node] Refined context received: ### Research Context on Model-Based Evaluation of Trustworthiness in Cyber-Physical Production Systems\n",
      "\n",
      "#### Introduction\n",
      "The research aims to explore the landscape of model-based evaluation concerning the trustworthiness of Autonomous Cyber-Physical Production Systems (ACPS). This involves identifying publication trends, supporting tools, and applications from the perspective of researchers and practitioners. The study is structured around the Goal-Question-Metric (GQM) approach, focusing on specific research questions to guide the investigation.\n",
      "\n",
      "#### Research Goals and Questions\n",
      "The primary purpose of this research is to identify, classify, and evaluate the current state of model-based evaluation methods for trustworthiness in ACPS. The research questions (RQs) are designed to uncover various dimensions of this field:\n",
      "\n",
      "- **RQ1:** What are the main research trends in the model-based evaluation of ACPS's trustworthiness?\n",
      "- **RQ2:** Which aspects and attributes of trustworthiness are addressed by current studies?\n",
      "- **RQ3:** What are the main modeling languages used for evaluating trustworthiness in ACPS?\n",
      "- **RQ4:** What model-based software tools have been used to evaluate trustworthiness in ACPS?\n",
      "\n",
      "#### Methodology\n",
      "The study utilizes the Scopus database to gather relevant literature, focusing on publications from the year 2006 onwards. This timeframe ensures the inclusion of contemporary research developments and trends.\n",
      "\n",
      "#### Key Findings from Retrieved Documents\n",
      "\n",
      "1. **Research Trends (RQ1):**\n",
      "   - The literature indicates a growing interest in the integration of model-based approaches to enhance the trustworthiness of ACPS. Recent trends highlight the shift towards more sophisticated models that incorporate real-time data analytics and machine learning to predict and mitigate potential system failures.\n",
      "\n",
      "2. **Trustworthiness Aspects (RQ2):**\n",
      "   - Current studies address various aspects of trustworthiness, including reliability, security, and safety. There is a significant focus on developing metrics and frameworks that can quantitatively assess these attributes, ensuring that ACPS can operate autonomously without compromising on performance or safety.\n",
      "\n",
      "3. **Modeling Languages (RQ3):**\n",
      "   - The research identifies several modeling languages that are prevalent in the evaluation of ACPS trustworthiness. These include UML (Unified Modeling Language), SysML (Systems Modeling Language), and domain-specific languages tailored for cyber-physical systems. These languages facilitate the creation of detailed models that can simulate and analyze system behavior under different conditions.\n",
      "\n",
      "4. **Software Tools (RQ4):**\n",
      "   - Various model-based software tools are employed to evaluate trustworthiness in ACPS. Tools such as MATLAB/Simulink, Modelica, and specialized simulation platforms are frequently used to model complex interactions within ACPS and assess their trustworthiness through rigorous testing and validation processes.\n",
      "\n",
      "#### Conclusion\n",
      "The synthesis of information from the retrieved documents provides a comprehensive overview of the current state of model-based evaluation for trustworthiness in ACPS. The findings underscore the importance of adopting advanced modeling techniques and tools to ensure that these systems can be trusted to perform critical functions autonomously. Future research should continue to explore innovative approaches and technologies that can further enhance the reliability and safety of ACPS.\n",
      "\n",
      "#### Metadata for Traceability\n",
      "- **Database:** Scopus\n",
      "- **Publication Year:** Post-2006\n",
      "- **Viewpoint:** Researchers and Practitioners\n",
      "- **Focus:** Model-based evaluation, Trustworthiness, Cyber-Physical Production Systems\n",
      "\n",
      "This refined context serves as a foundation for further exploration and development in the field, providing valuable insights for both academic researchers and industry practitioners.\n",
      "[generate_search_string_node] Current iteration: 1\n",
      "Final String:content='ALL ( ( systematic secondary studies OR mobile app software engineering OR mobile app development ) AND ( research topics OR secondary studies OR systematic reviews ) AND ( development phases OR mobile app development phases OR secondary studies phases ) AND ( key trends OR mobile app development trends OR secondary studies trends ) AND ( overall quality OR mobile app SE quality OR secondary studies quality ) AND ( key recommendations OR future research recommendations OR secondary studies recommendations ) AND ( useful recommendations OR practitioner recommendations OR secondary studies recommendations ) )' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 95, 'prompt_tokens': 1111, 'total_tokens': 1206, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'finish_reason': 'stop', 'logprobs': None} id='run-780ec502-d716-4454-88dc-3e02c623f52d-0' usage_metadata={'input_tokens': 1111, 'output_tokens': 95, 'total_tokens': 1206, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Metadata saved to: 00-Query/00-Query-Agent-RAG-WEB-MDTE-open-mixtral-8x7b-1.0/JSON\\results.json\n",
      "[generate_search_string_node] Search string (final): ALL ( ( systematic secondary studies OR mobile app software engineering OR mobile app development ) AND ( research topics OR secondary studies OR systematic reviews ) AND ( development phases OR mobile app development phases OR secondary studies phases ) AND ( key trends OR mobile app development trends OR secondary studies trends ) AND ( overall quality OR mobile app SE quality OR secondary studies quality ) AND ( key recommendations OR future research recommendations OR secondary studies recommendations ) AND ( useful recommendations OR practitioner recommendations OR secondary studies recommendations ) )\n",
      "[Profiling] wrapper took 12.4698 seconds\n",
      "[extract_databases_node] Input text: \n",
      "Research Questions:\n",
      "\n",
      "RQ1: How many systematic secondary studies on mobile app software engineering have been published? \n",
      "RQ2: What research topics were addressed in these SRs/SMSs? \n",
      "RQ3: What phases of mobile app development are addressed in these secondary studies? \n",
      "RQ4: What are the key trends in mobile app development as reported in these secondary studies? \n",
      "RQ5: What is the overall quality of these mobile app SE secondary studies? \n",
      "RQ6: What are the key recommendations made for future research studies? \n",
      "RQ7: Are any useful recommendations for practitioners made in the secondary studies?\n",
      "\n",
      "Database: Scopus\n",
      "Publication year higher than 2012\n",
      "        \n",
      "[extract_databases_node] Databases extracted: ['scopus']\n",
      "[Profiling] wrapper took 4.8659 seconds\n",
      "[format_queries_node] Search string: ALL ( ( systematic secondary studies OR mobile app software engineering OR mobile app development ) AND ( research topics OR secondary studies OR systematic reviews ) AND ( development phases OR mobile app development phases OR secondary studies phases ) AND ( key trends OR mobile app development trends OR secondary studies trends ) AND ( overall quality OR mobile app SE quality OR secondary studies quality ) AND ( key recommendations OR future research recommendations OR secondary studies recommendations ) AND ( useful recommendations OR practitioner recommendations OR secondary studies recommendations ) )\n",
      "[format_queries_node] Databases: ['scopus']\n",
      "[format_queries_node] Formatted queries: {'scopus': 'ALL ( ( systematic secondary studies OR mobile app software engineering OR mobile app development ) AND ( research topics OR secondary studies OR systematic reviews ) AND ( development phases OR mobile app development phases OR secondary studies phases ) AND ( key trends OR mobile app development trends OR secondary studies trends ) AND ( overall quality OR mobile app SE quality OR secondary studies quality ) AND ( key recommendations OR future research recommendations OR secondary studies recommendations ) AND ( useful recommendations OR practitioner recommendations OR secondary studies recommendations ) )'}\n",
      "[Profiling] wrapper took 4.9791 seconds\n",
      "[run_db_search_node] Formatted queries: {'scopus': 'ALL ( ( systematic secondary studies OR mobile app software engineering OR mobile app development ) AND ( research topics OR secondary studies OR systematic reviews ) AND ( development phases OR mobile app development phases OR secondary studies phases ) AND ( key trends OR mobile app development trends OR secondary studies trends ) AND ( overall quality OR mobile app SE quality OR secondary studies quality ) AND ( key recommendations OR future research recommendations OR secondary studies recommendations ) AND ( useful recommendations OR practitioner recommendations OR secondary studies recommendations ) )'}\n",
      "[multi_db_search_wrapper] Input JSON: {'query': 'ALL ( ( systematic secondary studies OR mobile app software engineering OR mobile app development ) AND ( research topics OR secondary studies OR systematic reviews ) AND ( development phases OR mobile app development phases OR secondary studies phases ) AND ( key trends OR mobile app development trends OR secondary studies trends ) AND ( overall quality OR mobile app SE quality OR secondary studies quality ) AND ( key recommendations OR future research recommendations OR secondary studies recommendations ) AND ( useful recommendations OR practitioner recommendations OR secondary studies recommendations ) )', 'dbs': 'scopus'}\n",
      "[extract_db_names] Input: scopus\n",
      "[extract_db_names] Extracted: ['scopus']\n",
      "[multi_db_search] Databases: ['scopus']\n",
      "Formatted Scopus Query: ALL ( ( systematic secondary studies OR mobile app software engineering OR mobile app development ) AND ( research topics OR secondary studies OR systematic reviews ) AND ( development phases OR mobile app development phases OR secondary studies phases ) AND ( key trends OR mobile app development trends OR secondary studies trends ) AND ( overall quality OR mobile app SE quality OR secondary studies quality ) AND ( key recommendations OR future research recommendations OR secondary studies recommendations ) AND ( useful recommendations OR practitioner recommendations OR secondary studies recommendations ) )\n",
      "Performing Scopus search for query: ALL ( ( systematic secondary studies OR mobile app software engineering OR mobile app development ) AND ( research topics OR secondary studies OR systematic reviews ) AND ( development phases OR mobile app development phases OR secondary studies phases ) AND ( key trends OR mobile app development trends OR secondary studies trends ) AND ( overall quality OR mobile app SE quality OR secondary studies quality ) AND ( key recommendations OR future research recommendations OR secondary studies recommendations ) AND ( useful recommendations OR practitioner recommendations OR secondary studies recommendations ) )\n",
      "Error during Scopus search: HTTPSConnectionPool(host='api.elsevier.com', port=443): Max retries exceeded with url: /content/search/scopus?count=200&view=STANDARD&query=ALL+%28+%28+systematic+secondary+studies+OR+mobile+app+software+engineering+OR+mobile+app+development+%29+AND+%28+research+topics+OR+secondary+studies+OR+systematic+reviews+%29+AND+%28+development+phases+OR+mobile+app+development+phases+OR+secondary+studies+phases+%29+AND+%28+key+trends+OR+mobile+app+development+trends+OR+secondary+studies+trends+%29+AND+%28+overall+quality+OR+mobile+app+SE+quality+OR+secondary+studies+quality+%29+AND+%28+key+recommendations+OR+future+research+recommendations+OR+secondary+studies+recommendations+%29+AND+%28+useful+recommendations+OR+practitioner+recommendations+OR+secondary+studies+recommendations+%29+%29&cursor=%2A (Caused by ResponseError('too many 500 error responses'))\n",
      "[Profiling] wrapper took 10.0287 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#      TIMING NODE PROFILING      #\n",
    "###################################\n",
    "\n",
    "def timing_profile_node(func):\n",
    "    \"\"\"\n",
    "    Decorator to profile a node function.\n",
    "    Appends a record with the node name and its execution time (in seconds) to profiling_records.\n",
    "    \"\"\"\n",
    "    def wrapper(state, *args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(state, *args, **kwargs)\n",
    "        end = time.time()\n",
    "        elapsed = end - start\n",
    "        profiling_records.append({\"node\": func.__name__, \"execution_time\": elapsed})\n",
    "        print(f\"[Profiling] {func.__name__} took {elapsed:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "###################################\n",
    "#    CODECARBON NODE DECORATOR    #\n",
    "###################################\n",
    "\n",
    "# os.environ[\"CODECARBON_API_KEY\"] = \"CODECARBON_API_KEY\"\n",
    "# os.environ[\"CODECARBON_API_URL\"] = \"https://api.codecarbon.io\"\n",
    "# os.environ[\"CODECARBON_EXPERIMENT_ID\"] = \"UUID\"\n",
    "\n",
    "def cc_profile_node(func):\n",
    "    \"\"\"\n",
    "    Decorator that wraps a node function with CodeCarbon tracking.\n",
    "    It starts a tracker before calling the node and stops it right after.\n",
    "    The resulting metrics are appended to the global cc_metrics_for_file list.\n",
    "    \"\"\"\n",
    "    def wrapper(state, *args, **kwargs):\n",
    "        # Create a CodeCarbon tracker for this node\n",
    "        tracker = EmissionsTracker(\n",
    "            project_name=f\"cc_{func.__name__}\",\n",
    "            measure_power_secs=1,\n",
    "            output_dir=CODECARBON_FOLDER,  # You can adjust output_dir as needed (\".\")\n",
    "            allow_multiple_runs=True\n",
    "            # api_call_interval=4,\n",
    "            # experiment_id=experiment_id,\n",
    "            # save_to_api=True\n",
    "        )\n",
    "        tracker.start()\n",
    "        result = func(state, *args, **kwargs)\n",
    "        emissions = tracker.stop()\n",
    "        # Try to extract detailed metrics if available (from the internal attribute)\n",
    "        if hasattr(tracker, \"_final_emissions_data\"):\n",
    "            metrics = tracker._final_emissions_data\n",
    "        else:\n",
    "            metrics = {\"total_emissions\": emissions}\n",
    "        # Append the node's CodeCarbon metrics to the global list\n",
    "        cc_metrics_for_file.append({\n",
    "            \"node\": func.__name__,\n",
    "            **metrics  # Flatten the metrics dictionary\n",
    "        })\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "###################################\n",
    "#       PROFILE & CC DECORATORS   #\n",
    "###################################\n",
    "\n",
    "# (Assuming you already have a @profile_node decorator for timing, as in your code.)\n",
    "# Here we combine both decorators so that each node is profiled for time and CodeCarbon metrics.\n",
    "# The order of decorators means that cc_profile_node will wrap the function first.\n",
    "def profile_node(func):\n",
    "    return timing_profile_node(cc_profile_node(func))\n",
    "\n",
    "###################################\n",
    "#       LOAD URLS FROM CSV        #\n",
    "###################################\n",
    "\n",
    "def load_urls_from_csv(csv_file_path):\n",
    "    urls = []\n",
    "    try:\n",
    "        with open(csv_file_path, 'r', newline='', encoding='utf-8') as csv_file:\n",
    "            reader = csv.reader(csv_file)\n",
    "            for row in reader:\n",
    "                if row:  # Ensure the row is not empty\n",
    "                    urls.append(row[0].strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: CSV file '{csv_file_path}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "    return urls\n",
    "\n",
    "# Load base URLs for the vector database from CSV\n",
    "BASE_URLS = load_urls_from_csv(CSV_FILE_PATH)\n",
    "if not BASE_URLS:\n",
    "    raise ValueError(\"No URLs were loaded from the CSV file.\")\n",
    "else:\n",
    "    print(f\"Loaded {len(BASE_URLS)} URLs from '{CSV_FILE_PATH}'.\")\n",
    "\n",
    "###################################\n",
    "#           GraphState            #\n",
    "###################################\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Extend GraphState to include keys from both retrieval and database parts\n",
    "class GraphState(TypedDict, total=False):\n",
    "    # Retrieval branch keys\n",
    "    question: str                       # The user's search question or refined query\n",
    "    generation: str                     # The generated answer or refined query output\n",
    "    documents: List[Any]                # List of retrieved documents\n",
    "    file_name: str                      # Name of the file being processed (if applicable)\n",
    "    context_llm: str                    # The refined context generated by the LLM\n",
    "    trace_status: str                   # Status of the processing trace\n",
    "    metadata: Dict[str, Any]            # Additional metadata related to the process\n",
    "    branch: str                         # Indicates which branch is being used ('retrieve' or 'web_search')\n",
    "    evaluation_metrics: Dict[str, float]  # Metrics evaluating the generated results\n",
    "    bert_score: Dict[str, float]        # BERTScore metrics for evaluating document support\n",
    "    web_bert_score: Dict[str, float]    # BERTScore metrics for evaluating web search branch results\n",
    "    skip_router: bool                   # Flag to bypass the routing node if not necessary\n",
    "    \n",
    "    # Research database branch keys\n",
    "    input_text: str                     # The initial input text provided by the user\n",
    "    raw_context: str                    # Raw context before any refinement\n",
    "    search_string: str                  # The final search string generated for database queries\n",
    "    databases: List[str]                # List of database names extracted from the input\n",
    "    formatted_queries: Dict[str, str]   # Queries formatted specifically for each database\n",
    "    db_results: Dict[str, str]          # Search results from each database (as JSON strings)\n",
    "    final_output: Dict[str, Any]        # Aggregated output containing search string and database results\n",
    "    \n",
    "    # Additional evaluation keys for query relaxation\n",
    "    min_results: int                    # Minimum number of articles expected to be returned\n",
    "    max_results: int                    # Maximum number of articles desired\n",
    "    relax_query: bool                   # Flag indicating whether the query should be relaxed to retrieve more results\n",
    "    adjusted_query: str                 # The adjusted query string after applying relaxation or modification\n",
    "    iteration: int                      # New field for iteration tracking\n",
    "\n",
    "###################################\n",
    "# RAG AGENT SETUP (Chroma/FAISS)  #\n",
    "###################################\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "if VECTOR_DB_TYPE == \"CHROMA\":\n",
    "    # Directory for persisting the Chroma vector store.\n",
    "    CHROMA_PERSIST_DIR = \"chroma_db\"\n",
    "    \n",
    "    # Load RAG configuration\n",
    "    config_rag = load_config(CONFIG_RAG_FILE)\n",
    "    api_key_rag = config_rag.get(\"api_keys\", {}).get(LLM.lower(), None)\n",
    "    \n",
    "    # Initialize OpenAIEmbeddings\n",
    "    embd = OpenAIEmbeddings(openai_api_key=api_key_rag)\n",
    "    \n",
    "    # Build or load the Chroma vector store\n",
    "    if os.path.exists(CHROMA_PERSIST_DIR) and os.listdir(CHROMA_PERSIST_DIR):\n",
    "        print(\"Loading existing Chroma vector store from disk...\")\n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=CHROMA_PERSIST_DIR,\n",
    "            embedding_function=embd,\n",
    "            collection_name=\"rag-chroma\"\n",
    "        )\n",
    "        retriever = vectorstore.as_retriever()\n",
    "    else:\n",
    "        print(\"Creating new Chroma vector store...\")\n",
    "        docs = [WebBaseLoader(url).load() for url in BASE_URLS]\n",
    "        docs_list = [item for sublist in docs for item in sublist]\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=500, chunk_overlap=0\n",
    "        )\n",
    "        doc_splits = text_splitter.split_documents(docs_list)\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=doc_splits,\n",
    "            collection_name=\"rag-chroma\",\n",
    "            embedding=embd,\n",
    "            persist_directory=CHROMA_PERSIST_DIR\n",
    "        )\n",
    "        retriever = vectorstore.as_retriever()\n",
    "elif VECTOR_DB_TYPE == \"FAISS\":\n",
    "\n",
    "    # Load RAG configuration\n",
    "    config_rag = load_config(CONFIG_RAG_FILE)\n",
    "    api_key_rag = config_rag.get(\"api_keys\", {}).get(LLM.lower(), None)\n",
    "    \n",
    "    EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"  \n",
    "    HUGGINGFACE_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    \n",
    "    faiss_folder = \"faiss\"\n",
    "    if not os.path.exists(faiss_folder):\n",
    "        os.makedirs(faiss_folder)\n",
    "        print(f\"Folder '{faiss_folder}' created.\")\n",
    "    else:\n",
    "        print(f\"The folder '{faiss_folder}' already exists.\")\n",
    "    \n",
    "    DATABASE_PATH = os.path.join(faiss_folder, \"faiss_index.index\")\n",
    "    METADATA_PATH = os.path.join(faiss_folder, \"metadata.json\")\n",
    "    \n",
    "    embedding = HuggingFaceEmbeddings(model_name=HUGGINGFACE_MODEL_NAME)\n",
    "    \n",
    "    if os.path.exists(DATABASE_PATH):\n",
    "        print(\"Loading existing FAISS index from disk...\")\n",
    "        vectorstore = FAISS.load_local(DATABASE_PATH, embedding, allow_dangerous_deserialization=True)\n",
    "        if os.path.exists(METADATA_PATH):\n",
    "            with open(METADATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "                metadata = json.load(f)\n",
    "    else:\n",
    "        print(\"Creating new FAISS vector store...\")\n",
    "        from langchain_community.document_loaders import WebBaseLoader\n",
    "        docs = [WebBaseLoader(url).load() for url in BASE_URLS]\n",
    "        docs_list = [item for sublist in docs for item in sublist]\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap=0)\n",
    "        doc_splits = text_splitter.split_documents(docs_list)\n",
    "        vectorstore = FAISS.from_documents(doc_splits, embedding)\n",
    "        vectorstore.save_local(DATABASE_PATH)\n",
    "        metadata = [doc.metadata for doc in doc_splits]\n",
    "        with open(METADATA_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "        \n",
    "    retriever = vectorstore.as_retriever()   \n",
    "\n",
    "###################################\n",
    "#         ROUTER NODE             #\n",
    "###################################\n",
    "\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Data model for routing the user query\n",
    "class RouteQuery(BaseModel):\n",
    "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
    "        ...,\n",
    "        description=\"Route the user query to either a vectorstore or web search.\"\n",
    "    )\n",
    "\n",
    "# Initialize RAG LLM and router\n",
    "LLM_RAG = config_rag.get(\"llm\")\n",
    "LLM_RAG_TEMP = config_rag.get(\"temperature\")\n",
    "\n",
    "if RAG_CHAT == 'OpenAI':\n",
    "    llm_rag = ChatOpenAI(model=LLM_RAG, temperature=LLM_RAG_TEMP)\n",
    "elif RAG_CHAT == 'LangChain':\n",
    "    llm_rag = OllamaFunctions(model=LLM_RAG) \n",
    "elif RAG_CHAT == 'Mistral':\n",
    "    llm_rag = ChatMistralAI(model=LLM_RAG, temperature=LLM_RAG_TEMP) \n",
    "\n",
    "structured_llm_router = llm_rag.with_structured_output(RouteQuery)\n",
    "\n",
    "# Join the topics into a single string, separated by commas\n",
    "topics_str = \", \".join(vectorstore_topics)\n",
    "\n",
    "# Create router prompt\n",
    "router_system_prompt = (\n",
    "    \"You are an expert at routing user queries to either a vectorstore or web search. \"\n",
    "    \"The vectorstore contains documents related to {topics_str}.\"\n",
    "    \"Use the vectorstore for questions on these topics; otherwise, use web search.\"\n",
    "    \"Based on the query, respond with a JSON object that contains a key 'datasource'\"\n",
    "    \"whose value is either 'vectorstore' or 'web_search'. Do not include any additional keys or text.\"\n",
    ")\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", router_system_prompt),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "question_router = route_prompt | structured_llm_router\n",
    "\n",
    "###################################\n",
    "#      RETRIEVAL GRADER NODE      #\n",
    "###################################\n",
    "\n",
    "# Data model for grading document relevance\n",
    "class GradeDocuments(BaseModel):\n",
    "    binary_score: str = Field(\n",
    "        description=\"Indicates whether the document is relevant ('yes' or 'no').\"\n",
    "    )\n",
    "\n",
    "structured_llm_grader = llm_rag.with_structured_output(GradeDocuments)\n",
    "\n",
    "grader_system_prompt = (\n",
    "    \"You are a grader assessing the relevance of a retrieved document to a user query. \"\n",
    "    \"If the document contains keywords or semantic content related to the user query, grade it as relevant. \"\n",
    "    \"Output a binary score 'yes' or 'no'.\"\n",
    ")\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", grader_system_prompt),\n",
    "        (\"human\", \"Retrieved document:\\n\\n{document}\\n\\nUser query:\\n{question}\"),\n",
    "    ]\n",
    ")\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "###################################\n",
    "#         GENERATION CHAIN        #\n",
    "###################################\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "context_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are an expert in information retrieval and content synthesis. Your task is to refine and enhance context \"\n",
    "        \"from multiple sources by generating a cohesive, well-structured, and detailed context that combines information \"\n",
    "        \"from various retrieved documents.\\n\\n\"\n",
    "        \"Responsibilities:\\n\"\n",
    "        \"1. Synthesize information from multiple sources into a unified explanation.\\n\"\n",
    "        \"2. Expand on the query with relevant details from the retrieved content.\\n\"\n",
    "        \"3. Format the refined context with clear structure and professional language.\\n\"\n",
    "        \"4. Incorporate metadata for traceability.\\n\"\n",
    "    )),\n",
    "    (\"user\", (\n",
    "        \"Question: {question}\\n\\n\"\n",
    "        \"The following are the retrieved documents and metadata:\\n\\n{context}\\n\\n\"\n",
    "        \"Using this information, generate a refined and comprehensive context.\"\n",
    "    )),\n",
    "])\n",
    "if RAG_CHAT == 'OpenAI':\n",
    "    llm_for_context = ChatOpenAI(model=LLM_RAG, temperature=LLM_RAG_TEMP)\n",
    "elif RAG_CHAT == 'LangChain':\n",
    "    llm_for_context = llm_LangChain\n",
    "elif RAG_CHAT == 'Mistral':\n",
    "    llm_for_context = ChatMistralAI(model=LLM_RAG, temperature=LLM_RAG_TEMP) \n",
    "rag_chain = context_prompt_template | llm_for_context | StrOutputParser()\n",
    "\n",
    "###################################\n",
    "#     HALLUCINATION GRADER        #\n",
    "###################################\n",
    "\n",
    "# Data model for grading hallucination\n",
    "class GradeHallucinations(BaseModel):\n",
    "    binary_score: str = Field(\n",
    "        description=\"Indicates if the answer is grounded in facts ('yes' or 'no').\"\n",
    "    )\n",
    "\n",
    "structured_llm_hallucination = llm_rag.with_structured_output(GradeHallucinations)\n",
    "\n",
    "hallucination_system_prompt = (\n",
    "    \"You are a grader assessing whether the LLM generation is grounded in the retrieved facts. \"\n",
    "    \"Output a binary score 'yes' if the answer is supported by the facts, otherwise 'no'.\"\n",
    ")\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", hallucination_system_prompt),\n",
    "        (\"human\", \"Facts:\\n\\n{documents}\\n\\nLLM Generation:\\n{generation}\"),\n",
    "    ]\n",
    ")\n",
    "hallucination_grader = hallucination_prompt | structured_llm_hallucination\n",
    "\n",
    "###################################\n",
    "#         ANSWER GRADER           #\n",
    "###################################\n",
    "\n",
    "# Data model for grading answer relevance\n",
    "class GradeAnswer(BaseModel):\n",
    "    binary_score: str = Field(\n",
    "        description=\"Indicates if the answer addresses the question ('yes' or 'no').\"\n",
    "    )\n",
    "\n",
    "structured_llm_answer = llm_rag.with_structured_output(GradeAnswer)\n",
    "\n",
    "answer_system_prompt = (\n",
    "    \"You are a grader assessing whether an LLM-generated answer addresses the user query. \"\n",
    "    \"Output a binary score 'yes' if it does, otherwise 'no'.\"\n",
    ")\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", answer_system_prompt),\n",
    "        (\"human\", \"User Query:\\n{question}\\n\\nLLM Generation:\\n{generation}\"),\n",
    "    ]\n",
    ")\n",
    "answer_grader = answer_prompt | structured_llm_answer\n",
    "\n",
    "###################################\n",
    "#       QUESTION REWRITER         #\n",
    "###################################\n",
    "\n",
    "rewrite_system_prompt = (\n",
    "    \"You are a question rewriter. Given an input question, produce an improved version optimized for vectorstore retrieval. \"\n",
    "    \"Focus on the underlying semantic intent.\"\n",
    ")\n",
    "rewrite_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rewrite_system_prompt),\n",
    "        (\"human\", \"Original question:\\n{question}\\n\\nRewrite the question:\"),\n",
    "    ]\n",
    ")\n",
    "question_rewriter = rewrite_prompt | llm_for_context | StrOutputParser()\n",
    "\n",
    "###################################\n",
    "#           WEB SEARCH            #\n",
    "###################################\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "config_tavily = load_config(CONFIG_RAG_TAVILY_FILE)\n",
    "os.environ[\"TAVILY_API_KEY\"] = config_tavily.get(\"tavily_api_key\")\n",
    "web_search_tool = TavilySearchResults(k=3)\n",
    "\n",
    "###################################\n",
    "#        EVALUATION NODES         #\n",
    "###################################\n",
    "\n",
    "# (1) LLM-based Evaluation for RAG output (vectorstore branch)\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class RAGEvaluationMetrics(BaseModel):\n",
    "    faithfulness: float = Field(..., description=\"Score (0-1) indicating how faithful the answer is to the facts.\")\n",
    "    answer_relevance: float = Field(..., description=\"Score (0-1) indicating how well the answer addresses the question.\")\n",
    "    context_precision: float = Field(..., description=\"Score (0-1) representing the precision of the context used.\")\n",
    "    context_accuracy: float = Field(..., description=\"Score (0-1) representing the accuracy of the retrieved context.\")\n",
    "    context_recall: float = Field(..., description=\"Score (0-1) representing the recall of the context.\")\n",
    "    context_f1: float = Field(..., description=\"Score (0-1) representing the F1 measure of the context.\")\n",
    "\n",
    "@profile_node\n",
    "def evaluate_rag_output(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Node to evaluate the RAG output (vectorstore branch) based on metrics such as:\n",
    "    Faithfulness, Answer Relevance, Context Precision, Context Accuracy,\n",
    "    Context Recall, and Context F1.\n",
    "    \"\"\"\n",
    "    print(\"--- EVALUATE RAG OUTPUT METRICS ---\")\n",
    "    question_val = state.get(\"question\", \"\")\n",
    "    generation = state.get(\"generation\", \"\")\n",
    "    documents = state.get(\"documents\", [])\n",
    "    context_text = \"\\n\".join([doc.page_content for doc in documents]) if documents else \"\"\n",
    "    \n",
    "    eval_input = {\n",
    "         \"question\": question_val,\n",
    "         \"generation\": generation,\n",
    "         \"context\": context_text\n",
    "    }\n",
    "    \n",
    "    eval_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", (\n",
    "            \"You are an expert evaluator of RAG outputs. Evaluate the output based on the following metrics: \"\n",
    "            \"Faithfulness, Answer Relevance, Context Precision, Context Accuracy, Context Recall, and Context F1. \"\n",
    "            \"For each metric, assign a score between 0 and 1. \"\n",
    "            \"Respond in JSON format with keys: faithfulness, answer_relevance, context_precision, \"\n",
    "            \"context_accuracy, context_recall, context_f1.\"\n",
    "        )),\n",
    "        (\"user\", \"Question:\\n{question}\\n\\nGenerated Answer:\\n{generation}\\n\\nContext:\\n{context}\\n\\nProvide the evaluation:\")\n",
    "    ])\n",
    "    \n",
    "    structured_eval = llm_rag.with_structured_output(RAGEvaluationMetrics)\n",
    "    eval_chain = eval_prompt | structured_eval\n",
    "    try:\n",
    "         eval_metrics = eval_chain.invoke(eval_input)\n",
    "         state[\"evaluation_metrics\"] = eval_metrics.dict()\n",
    "         print(\"Evaluation metrics:\", state[\"evaluation_metrics\"])\n",
    "    except Exception as e:\n",
    "         print(\"Error during evaluation of RAG output:\", e)\n",
    "         state[\"evaluation_metrics\"] = {}\n",
    "    return state\n",
    "\n",
    "# (2) BERTScore Evaluation for RAG output (vectorstore branch)\n",
    "@profile_node\n",
    "def evaluate_bert_score(state: GraphState) -> GraphState:\n",
    "    print(\"--- EVALUATE BERT SCORE ---\")\n",
    "    try:\n",
    "        from bert_score import score\n",
    "    except ImportError:\n",
    "        print(\"Please install bert-score using 'pip install bert-score'\")\n",
    "        state[\"bert_score\"] = None\n",
    "        return state\n",
    "\n",
    "    candidate = state.get(\"generation\", \"\")\n",
    "    documents = state.get(\"documents\", [])\n",
    "    reference = \"\\n\".join([doc.page_content for doc in documents]) if documents else \"\"\n",
    "    \n",
    "    if not candidate or not reference:\n",
    "        print(\"Candidate or reference text is empty. Skipping BERTScore evaluation.\")\n",
    "        state[\"bert_score\"] = None\n",
    "        return state\n",
    "    \n",
    "    P, R, F1 = score([candidate], [reference], lang=\"en\", verbose=True)\n",
    "    bert_precision = P[0].item()\n",
    "    bert_recall = R[0].item()\n",
    "    bert_f1 = F1[0].item()\n",
    "    state[\"bert_score\"] = {\"precision\": bert_precision, \"recall\": bert_recall, \"f1\": bert_f1}\n",
    "    print(\"BERTScore metrics:\", state[\"bert_score\"])\n",
    "    return state\n",
    "\n",
    "# (3) LLM-based Evaluation for Web Search output\n",
    "# Here, we introduce an additional metric \"accuracy\" along with the previous ones.\n",
    "class WebEvaluationMetrics(BaseModel):\n",
    "    faithfulness: float = Field(..., description=\"Score (0-1) indicating how faithful the answer is to the web sources.\")\n",
    "    answer_relevance: float = Field(..., description=\"Score (0-1) indicating how well the answer addresses the query.\")\n",
    "    context_precision: float = Field(..., description=\"Score (0-1) representing the precision of the web search results.\")\n",
    "    context_accuracy: float = Field(..., description=\"Score (0-1) representing the accuracy of the retrieved web content.\")\n",
    "    context_recall: float = Field(..., description=\"Score (0-1) representing the recall of relevant web information.\")\n",
    "    context_f1: float = Field(..., description=\"Score (0-1) representing the F1 measure of the web search results.\")\n",
    "    accuracy: float = Field(..., description=\"Score (0-1) indicating the overall accuracy of the generated context based on web sources.\")\n",
    "\n",
    "@profile_node\n",
    "def evaluate_web_search_output(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Node to evaluate the output of the web search branch.\n",
    "    It uses the same metrics as the RAG evaluation plus an extra metric 'accuracy'.\n",
    "    The reference is the concatenated web search source content.\n",
    "    \"\"\"\n",
    "    print(\"--- EVALUATE WEB SEARCH OUTPUT METRICS ---\")\n",
    "    question_val = state.get(\"question\", \"\")\n",
    "    generation = state.get(\"generation\", \"\")\n",
    "    documents = state.get(\"documents\", [])\n",
    "    context_text = \"\\n\".join([doc.page_content for doc in documents]) if documents else \"\"\n",
    "    \n",
    "    eval_input = {\n",
    "         \"question\": question_val,\n",
    "         \"generation\": generation,\n",
    "         \"context\": context_text\n",
    "    }\n",
    "    \n",
    "    eval_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", (\n",
    "            \"You are an expert evaluator of web search outputs. Evaluate the output based on the following metrics: \"\n",
    "            \"Faithfulness, Answer Relevance, Context Precision, Context Accuracy, Context Recall, Context F1, and Accuracy. \"\n",
    "            \"For each metric, assign a score between 0 and 1. \"\n",
    "            \"Respond in JSON format with keys: faithfulness, answer_relevance, context_precision, \"\n",
    "            \"context_accuracy, context_recall, context_f1, accuracy.\"\n",
    "        )),\n",
    "        (\"user\", \"Question:\\n{question}\\n\\nGenerated Answer/Context:\\n{generation}\\n\\nWeb Search Sources:\\n{context}\\n\\nProvide the evaluation:\")\n",
    "    ])\n",
    "    \n",
    "    structured_eval = llm_rag.with_structured_output(WebEvaluationMetrics)\n",
    "    eval_chain = eval_prompt | structured_eval\n",
    "    try:\n",
    "         eval_metrics = eval_chain.invoke(eval_input)\n",
    "         state[\"evaluation_metrics\"] = eval_metrics.dict()\n",
    "         print(\"Web search evaluation metrics:\", state[\"evaluation_metrics\"])\n",
    "    except Exception as e:\n",
    "         print(\"Error during web search evaluation:\", e)\n",
    "         state[\"evaluation_metrics\"] = {}\n",
    "    return state\n",
    "\n",
    "# (4) BERTScore Evaluation for Web Search output\n",
    "@profile_node\n",
    "def evaluate_web_bert_score(state: GraphState) -> GraphState:\n",
    "    print(\"--- EVALUATE WEB BERT SCORE ---\")\n",
    "    try:\n",
    "        from bert_score import score\n",
    "    except ImportError:\n",
    "        print(\"Please install bert-score using 'pip install bert-score'\")\n",
    "        state[\"web_bert_score\"] = None\n",
    "        return state\n",
    "\n",
    "    candidate = state.get(\"generation\", \"\")\n",
    "    documents = state.get(\"documents\", [])\n",
    "    reference = \"\\n\".join([doc.page_content for doc in documents]) if documents else \"\"\n",
    "    \n",
    "    if not candidate or not reference:\n",
    "        print(\"Candidate or reference text is empty for web search. Skipping BERTScore evaluation.\")\n",
    "        state[\"web_bert_score\"] = None\n",
    "        return state\n",
    "    \n",
    "    P, R, F1 = score([candidate], [reference], lang=\"en\", verbose=True)\n",
    "    web_bert_precision = P[0].item()\n",
    "    web_bert_recall = R[0].item()\n",
    "    web_bert_f1 = F1[0].item()\n",
    "    state[\"web_bert_score\"] = {\"precision\": web_bert_precision, \"recall\": web_bert_recall, \"f1\": web_bert_f1}\n",
    "    print(\"Web BERTScore metrics:\", state[\"web_bert_score\"])\n",
    "    return state\n",
    "\n",
    "###################################\n",
    "#        DECIDE TO GENERATE       #\n",
    "###################################\n",
    "\n",
    "def decide_to_generate(state: GraphState) -> str:\n",
    "    print(\"--- DECIDE TO GENERATE ---\")\n",
    "    filtered_documents = state.get(\"documents\", [])\n",
    "    if not filtered_documents:\n",
    "        if branch == \"retrieve\":\n",
    "            print(\"--- No relevant documents found in vectorstore; transforming query to improve retrieval ---\")\n",
    "            return \"transform_query\"\n",
    "        else:  # branch == \"web_search\"\n",
    "            print(\"--- No documents found via web search; proceeding with generation using empty context ---\")\n",
    "            return \"generate\"\n",
    "    else:\n",
    "        print(\"--- Relevant documents found, generating answer ---\")\n",
    "        return \"generate\"\n",
    "\n",
    "###################################\n",
    "#       CACHE NODE (LangGraph)    #\n",
    "###################################\n",
    "\n",
    "@profile_node\n",
    "def cache_context_node(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    LangGraph node that checks if a refined context is already available.\n",
    "    If present in the state or in the file cache, it uses that value.\n",
    "    Otherwise, it generates the refined context using the rag_chain,\n",
    "    caches it (in state and on disk), and returns the state.\n",
    "    \"\"\"\n",
    "    if \"context_llm\" in state and state[\"context_llm\"]:\n",
    "        print(\"Using refined context already present in state.\")\n",
    "        return state\n",
    "\n",
    "    if os.path.isfile(REFINED_CONTEXT_PATH) and not FORCE_CONTEXT_GEN:\n",
    "        try:\n",
    "            with open(REFINED_CONTEXT_PATH, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            state[\"context_llm\"] = data.get(\"context\", \"\")\n",
    "            print(\"Loaded refined context from file cache (LangGraph node).\")\n",
    "            return state\n",
    "        except Exception as e:\n",
    "            print(\"Error loading refined context from file in cache node:\", e)\n",
    "\n",
    "    print(\"Generating refined context in LangGraph cache node...\")\n",
    "    refined_context = rag_chain.invoke({\"question\": state[\"question\"], \"context\": \"\"})\n",
    "    state[\"context_llm\"] = refined_context\n",
    "    try:\n",
    "        with open(REFINED_CONTEXT_PATH, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\"context\": refined_context}, f, indent=4, ensure_ascii=False)\n",
    "        print(\"Refined context cached to file from LangGraph node.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error caching refined context to file in cache node:\", e)\n",
    "    return state\n",
    "\n",
    "###################################\n",
    "#          GRAPH NODES            #\n",
    "###################################\n",
    "\n",
    "@profile_node\n",
    "def generate_query_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if os.path.isfile(REFINED_CONTEXT_PATH) and not FORCE_CONTEXT_GEN:\n",
    "        print(\"Refined context file exists. Skipping query generation; proceeding directly to cache_context_node.\")\n",
    "        state[\"skip_router\"] = True\n",
    "    else:\n",
    "        state[\"skip_router\"] = False\n",
    "        state[\"question\"] = state[\"input_text\"]\n",
    "        print(\"Query for context refinement:\", state[\"question\"])\n",
    "    return state\n",
    "\n",
    "# Node: Retrieve documents using the vectorstore\n",
    "@profile_node\n",
    "def retrieve(state: GraphState) -> GraphState:\n",
    "    print(\"--- RETRIEVE ---\")\n",
    "    question_val = state[\"question\"]\n",
    "    documents = retriever.invoke(question_val)\n",
    "    state[\"documents\"] = documents\n",
    "    return state\n",
    "\n",
    "# Node: Perform web search (remains separate)\n",
    "@profile_node\n",
    "def web_search(state: GraphState) -> GraphState:\n",
    "    print(\"--- WEB SEARCH ---\")\n",
    "    question_val = state[\"question\"]\n",
    "    docs = web_search_tool.invoke({\"query\": question_val})\n",
    "\n",
    "    # Combine web search results into a single Document\n",
    "    \"\"\"\n",
    "    web_results_content = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    from langchain.schema import Document\n",
    "    web_results_doc = Document(page_content=web_results_content)\n",
    "    state[\"documents\"] = [web_results_doc]\n",
    "    return state\n",
    "    \"\"\"\n",
    "    # Check the type of docs and extract content accordingly.\n",
    "    if isinstance(docs, str):\n",
    "        # If docs is a string, use it directly.\n",
    "        web_results_content = docs\n",
    "    elif isinstance(docs, list):\n",
    "        # If docs is a list, check the type of its elements.\n",
    "        if docs and isinstance(docs[0], dict) and \"content\" in docs[0]:\n",
    "            web_results_content = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "        else:\n",
    "            # Assume it's a list of strings.\n",
    "            web_results_content = \"\\n\".join(docs)\n",
    "    else:\n",
    "        # Fallback: convert docs to string.\n",
    "        web_results_content = str(docs)\n",
    "    \n",
    "    from langchain.schema import Document\n",
    "    web_results_doc = Document(page_content=web_results_content)\n",
    "    state[\"documents\"] = [web_results_doc]\n",
    "    return state    \n",
    "\n",
    "# Merged Node: Generate answer using the RAG chain (used for both branches)\n",
    "def generate(state: GraphState) -> GraphState:\n",
    "    print(\"--- GENERATE (RAG) ---\")\n",
    "    question_val = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question_val})\n",
    "    state[\"generation\"] = generation\n",
    "    return state\n",
    "\n",
    "# Node: Generate answer using web search results\n",
    "@profile_node\n",
    "def generate_web(state: GraphState) -> GraphState:\n",
    "    print(\"--- GENERATE (Web) ---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    state[\"generation\"] = generation\n",
    "    return state\n",
    "\n",
    "# Node: Grade documents for relevance\n",
    "@profile_node\n",
    "def grade_documents(state: GraphState) -> GraphState:\n",
    "    print(\"--- GRADE DOCUMENTS ---\")\n",
    "    question_val = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question_val, \"document\": d.page_content})\n",
    "        if score.binary_score.lower() == \"yes\":\n",
    "            print(\"--- Document is relevant ---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"--- Document is not relevant ---\")\n",
    "    state[\"documents\"] = filtered_docs\n",
    "    return state\n",
    "\n",
    "# Merged Node: Transform the query (for both branches)\n",
    "@profile_node\n",
    "def transform_query(state: GraphState) -> GraphState:\n",
    "    print(\"--- TRANSFORM QUERY (RAG) ---\")\n",
    "    question_val = state[\"question\"]\n",
    "    better_question = question_rewriter.invoke({\"question\": question_val})\n",
    "    print(better_question)\n",
    "    state[\"question\"] = better_question\n",
    "    return state\n",
    "\n",
    "# Node: Transform the query for web search\n",
    "@profile_node\n",
    "def transform_query_web(state: GraphState) -> GraphState:\n",
    "    print(\"--- TRANSFORM QUERY (Web) ---\")\n",
    "    question = state[\"question\"]\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    print(better_question)\n",
    "    state[\"question\"] = better_question\n",
    "    return state\n",
    "\n",
    "# Conditional routing after transformation: based on branch in state\n",
    "\"\"\"\n",
    "def route_after_transform(state: GraphState) -> str:\n",
    "    if state.get(\"branch\") == \"retrieve\":\n",
    "        return \"retrieve\"\n",
    "    elif state.get(\"branch\") == \"web_search\":\n",
    "        return \"web_search\"\n",
    "    return \"retrieve\"\n",
    "\"\"\"\n",
    "\n",
    "# Node: Grade the generation against the documents and question\n",
    "@profile_node\n",
    "def grade_generation_v_documents_and_question(state: GraphState) -> str:\n",
    "    print(\"--- GRADE GENERATION ---\")\n",
    "    question_val = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    # Evaluate if the retrieved documents support the generation.\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    if score.binary_score.lower() == \"yes\":\n",
    "        print(\"--- Generation is grounded in documents ---\")\n",
    "        score_answer = answer_grader.invoke({\"question\": question_val, \"generation\": generation})\n",
    "        if score_answer.binary_score.lower() == \"yes\":\n",
    "            print(\"--- Generation addresses the question ---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"--- Generation does not address the question ---\")\n",
    "            # Fallback: if not useful, use a minimal context (e.g., the original query)\n",
    "            state[\"generation\"] = question_val  \n",
    "            print(\"Fallback: Using original query as generation.\")\n",
    "            return \"useful\"  # Accept the fallback as useful\n",
    "    else:\n",
    "        print(\"--- Generation is not supported by documents, retrying ---\")\n",
    "        # Instead of returning \"not useful\", you can decide to use a fallback as well\n",
    "        state[\"generation\"] = question_val  \n",
    "        print(\"Fallback: Using original query as generation.\")\n",
    "        return \"useful\"    \n",
    "\n",
    "###################################\n",
    "#       DATABASE BRANCH NODES     #\n",
    "# (from generate_search_string onward)\n",
    "###################################\n",
    "\n",
    "# --- Define simulated search functions for each database ---\n",
    "def scopus_search(query: str) -> str:\n",
    "\n",
    "    # query = \"TITLE-ABS-KEY(Digital AND twin AND federation)\"\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Perform the Scopus search using Pybliometrics\n",
    "    # -----------------------------\n",
    "    try:\n",
    "        print(\"Performing Scopus search for query:\", query)\n",
    "        # Using the STANDARD view for the initial search\n",
    "        scopus_search_res = ScopusSearch(query, view=\"STANDARD\", integrity_action='raise') # STANDARD  COMPLETE\n",
    "        # Convert the results to a DataFrame\n",
    "        df_scopus = pd.DataFrame(scopus_search_res.results)\n",
    "    except Exception as e:\n",
    "        print(\"Error during Scopus search:\", e)\n",
    "        # Return an error string instead of calling exit(1)\n",
    "        return f\"Error during Scopus search: {e}\"\n",
    "    \n",
    "    results = scopus_search_res.results\n",
    "    print(f\"Number of results found: {len(results)}\")\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Filter the DataFrame to include only the desired columns\n",
    "    # -----------------------------\n",
    "    desired_columns = [\n",
    "        'eid', 'doi', 'title', 'subtype', 'subtypeDescription', 'creator', \n",
    "        'affilname', 'affiliation_city', 'affiliation_country', 'coverDate', \n",
    "        'coverDisplayDate', 'publicationName', 'issn', 'source_id', \n",
    "        'aggregationType', 'authkeywords', 'citedby_count', 'openaccess'\n",
    "    ]\n",
    "    df_filtered = df_scopus.reindex(columns=desired_columns)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Display the retrieved results on screen\n",
    "    # -----------------------------\n",
    "    # print(\"Displaying retrieved results:\")\n",
    "    # print(df_filtered)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Save the filtered DataFrame to a CSV file\n",
    "    # -----------------------------\n",
    "    # csv_filename = \"Scopus_Search_results.csv\"\n",
    "    \n",
    "    try:\n",
    "        df_filtered.to_csv(csv_filename, index=False, encoding=\"utf-8\")\n",
    "        print(f\"\\nResults have been saved to '{csv_filename}'\")\n",
    "    except Exception as e:\n",
    "        print(\"Error saving CSV:\", e)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Retrieve abstracts using AbstractRetrieval for each article and save to another CSV\n",
    "    # -----------------------------\n",
    "    \"\"\"\n",
    "    abstracts_list = []\n",
    "    abstracts_list_reduced = []\n",
    "    print(\"\\nRetrieving abstracts for each article...\")\n",
    "    \n",
    "    for index, row in df_filtered.iterrows():\n",
    "        try:\n",
    "            # Attempt retrieval using DOI if available; otherwise, use EID.\n",
    "            if pd.notnull(row['doi']) and row['doi'] != \"\":\n",
    "                abs_obj = AbstractRetrieval(row['doi'], view=\"FULL\")\n",
    "            elif pd.notnull(row['eid']) and row['eid'] != \"\":\n",
    "                abs_obj = AbstractRetrieval(row['eid'], view=\"FULL\")\n",
    "            else:\n",
    "                print(f\"No DOI or EID available for row index {index}\")\n",
    "                abs_obj = None\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving abstract for index {index}: {e}\")\n",
    "            abs_obj = None\n",
    "    \n",
    "        abstracts_list_reduced.append({\n",
    "            'eid': row['eid'],\n",
    "            'doi': row['doi'],\n",
    "            'title': row['title'],\n",
    "            'abstract': abs_obj.abstract if abs_obj is not None else None, \n",
    "            'authkeywords': abs_obj.authkeywords if abs_obj is not None else None,\n",
    "            'doi-link': \"http://doi.org/\" + row['doi'] if row.get('doi') else None,\n",
    "            'publicationName': row['publicationName'],\n",
    "            'aggregationType': row['aggregationType'],\n",
    "            'citedby_count': row['citedby_count'],\n",
    "            'openaccess': row['openaccess']\n",
    "        })\n",
    "        \n",
    "        abstracts_list.append({\n",
    "            'eid': row['eid'],\n",
    "            'doi': row['doi'],\n",
    "            'title': row['title'],\n",
    "            'abstract': abs_obj.description if abs_obj is not None else None, \n",
    "            'authkeywords': str(abs_obj.authkeywords) if abs_obj is not None else None, \n",
    "            'doi-link': \"http://doi.org/\" + row['doi'] if row.get('doi') else None,\n",
    "            'subtype': row['subtype'],\n",
    "            'subtypeDescription': row['subtypeDescription'],\n",
    "            'publicationName': row['publicationName'],\n",
    "            'publisher': str(abs_obj.publisher) if abs_obj is not None else None, \n",
    "            'authors': str(abs_obj.authors) if abs_obj is not None else None, \n",
    "            'creator':  row['creator'],\n",
    "            'affilname': row['affilname'],\n",
    "            'affiliation_city': row['affiliation_city'],\n",
    "            'affiliation_country': row['affiliation_country'],\n",
    "            'language': str(abs_obj.language) if abs_obj is not None else None, \n",
    "            'date_created': str(abs_obj.date_created) if abs_obj is not None else None, \n",
    "            'coverDate': row['coverDate'],\n",
    "            'coverDisplayDate': row['coverDisplayDate'],\n",
    "            'issn': row['issn'],\n",
    "            'isbn': str(abs_obj.isbn) if abs_obj is not None else None, \n",
    "            'source_id': row['source_id'],\n",
    "            'aggregationType': row['aggregationType'],\n",
    "            'citedby_count': row['citedby_count'],\n",
    "            'openaccess': row['openaccess'],\n",
    "            'openaccessFlag': str(abs_obj.openaccessFlag) if abs_obj is not None else None, \n",
    "            # 'abstract': str(abs_obj.abstract) if abs_obj is not None else None,\n",
    "            'refcount': str(abs_obj.refcount) if abs_obj is not None else None,\n",
    "            # 'references': str(abs_obj.references) if abs_obj is not None else None\n",
    "            'subject_areas': str(abs_obj.subject_areas) if abs_obj is not None else None,\n",
    "            'url': str(abs_obj.url) if abs_obj is not None else None,\n",
    "            'website': str(abs_obj.website) if abs_obj is not None else None,\n",
    "            'freetoread': row['freetoread'],\n",
    "            'freetoreadLabel': row['freetoreadLabel'],\n",
    "            'volume': row['volume'],\t\n",
    "            'issueIdentifier': row['issueIdentifier'],\t\n",
    "            'article_number': row['article_number'],\n",
    "            'pageRange': row['pageRange']\n",
    "        })\n",
    "    \n",
    "    # Create a DataFrame for abstracts and display it\n",
    "    df_results = pd.DataFrame(abstracts_list_reduced)\n",
    "    df_abstracts = pd.DataFrame(abstracts_list)\n",
    "    #print(\"\\nDisplaying retrieved abstracts:\")\n",
    "    #print(df_abstracts)\n",
    "    \n",
    "    # Save the abstracts DataFrame to a CSV file\n",
    "    # csv_abstracts_filename = \"Scopus_AbstractRetrieval_results.csv\"\n",
    "    try:\n",
    "        df_abstracts.to_csv(csv_abstracts_filename, index=False, encoding=\"utf-8\")\n",
    "        print(f\"\\nAbstracts have been saved to '{csv_abstracts_filename}'\")\n",
    "    except Exception as e:\n",
    "        print(\"Error saving abstracts CSV:\", e) \n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the DataFrame to a JSON string (list of records)\n",
    "    try:\n",
    "        df_json = df_filtered.to_json(orient=\"records\")\n",
    "        return df_json\n",
    "    except Exception as e:\n",
    "        print(\"Error converting DataFrame to JSON:\", e)\n",
    "        return json.dumps({\"error\": f\"Error converting DataFrame to JSON: {e}\"})\n",
    "\n",
    "def ieee_search(query: str) -> str:\n",
    "    return f\"IEEE results for query: '{query}'\"\n",
    "\n",
    "def sciencedirect_search(query: str) -> str:\n",
    "    return f\"ScienceDirect results for query: '{query}'\"\n",
    "\n",
    "# --- Map database names (lowercased) to tools ---\n",
    "db_tools = {\n",
    "    \"scopus\": Tool(\n",
    "        name=\"ScopusSearch\",\n",
    "        func=scopus_search,\n",
    "        description=\"Executes a search query on Scopus.\"\n",
    "    ),\n",
    "    \"ieee\": Tool(\n",
    "        name=\"IEEESearch\",\n",
    "        func=ieee_search,\n",
    "        description=\"Executes a search query on IEEE.\"\n",
    "    ),\n",
    "    \"sciencedirect\": Tool(\n",
    "        name=\"ScienceDirectSearch\",\n",
    "        func=sciencedirect_search,\n",
    "        description=\"Executes a search query on ScienceDirect.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "def extract_db_names(db_string: str) -> List[str]:\n",
    "    names = [db.strip().lower() for db in db_string.split(\",\") if db.strip()]\n",
    "    print(\"[extract_db_names] Input:\", db_string)\n",
    "    print(\"[extract_db_names] Extracted:\", names)\n",
    "    return names\n",
    "\n",
    "def format_db_query(db: str, query: str) -> str:\n",
    "    if db == \"scopus\":\n",
    "        # If the query already starts with TITLE-ABS-KEY/ALL, return it as-is.\n",
    "        if query.strip().upper().startswith(\"ALL\"):\n",
    "            return query\n",
    "        else:\n",
    "            return f\"ALL({query})\"\n",
    "            # return f\"{query}\"\n",
    "    elif db == \"ieee\":\n",
    "        formatted = f\"INDEXTERMS({query})\"\n",
    "    elif db == \"sciencedirect\":\n",
    "        formatted = f\"KEY({query})\"\n",
    "    else:\n",
    "        formatted = query\n",
    "    print(f\"[format_db_query] Database: {db} | Query: {query} | Formatted: {formatted}\")\n",
    "    return formatted\n",
    "\n",
    "def multi_db_search(query: str, dbs: str) -> str:\n",
    "    db_list = extract_db_names(dbs)\n",
    "    aggregated_results = {}\n",
    "    print(\"[multi_db_search] Databases:\", db_list)\n",
    "    for db in db_list:\n",
    "        if db in db_tools:\n",
    "            formatted_query = format_db_query(db, query)\n",
    "            print(\"Formatted Scopus Query:\", formatted_query)\n",
    "            tool = db_tools[db]\n",
    "            result = tool.func(formatted_query)\n",
    "            aggregated_results[db] = result\n",
    "        else:\n",
    "            aggregated_results[db] = f\"No tool available for database '{db}'.\"\n",
    "            # Simulate executing the search\n",
    "            #aggregated_results[db] = f\"Simulated result for {formatted_query}\" ############### ADD FUNCTION FOR SCOPYUS\n",
    "    aggregated_json = json.dumps(aggregated_results)\n",
    "    # print(\"[multi_db_search] Aggregated Results:\", aggregated_json)\n",
    "    return aggregated_json\n",
    "\n",
    "def multi_db_search_wrapper(state: Dict[str, Any], tool_input: str) -> str:\n",
    "    try:\n",
    "        data = json.loads(tool_input)\n",
    "        query = data.get(\"query\", \"\")\n",
    "        dbs = data.get(\"dbs\", \"\")\n",
    "        print(\"[multi_db_search_wrapper] Input JSON:\", data)\n",
    "        return multi_db_search(query, dbs)\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error parsing tool input: {str(e)}\"\n",
    "        \n",
    "        # Initialize iteration counter if not present, then increment it\n",
    "        if \"iteration\" not in state:\n",
    "            state[\"iteration\"] = 0\n",
    "        state[\"iteration\"] += 1\n",
    "        print(f\"[generate_search_string_node] Current iteration: {state['iteration']}\")\n",
    "    \n",
    "        # Define maximum iterations allowed before forcing a relaxed query\n",
    "        max_iterations = 10\n",
    "        if state[\"iteration\"] >= max_iterations:\n",
    "            print(\"[generate_search_string_node] Maximum iterations reached; switching to 'broaden' mode.\")\n",
    "            state[\"adjust_query\"] = \"broaden\"    \n",
    "        \n",
    "        print(\"[multi_db_search_wrapper] Error:\", error_message)\n",
    "        return error_message\n",
    "\n",
    "\"\"\"\n",
    "multi_db_search_tool = Tool(\n",
    "    name=\"MultiDBSearchTool\",\n",
    "    func=multi_db_search_wrapper,\n",
    "    description=(\n",
    "        \"Executes a search on multiple research databases (e.g., Scopus, IEEE, ScienceDirect). \"\n",
    "        \"Input must be a JSON with keys 'query' (the formatted query) and 'dbs' (a comma-separated list of database names).\"\n",
    "    )\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "@profile_node\n",
    "def evaluate_results_node(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Checks the total number of results obtained from the database search.\n",
    "    If the number of results is below the minimum threshold, sets a flag to broaden the query.\n",
    "    If the number is above the maximum threshold, sets a flag to tighten the query.\n",
    "    Otherwise, no adjustment is needed.\n",
    "    \"\"\"\n",
    "\n",
    "    total_results = 0\n",
    "    for db, json_str in state.get(\"db_results\", {}).items():\n",
    "        try:\n",
    "            db_obj = json.loads(json_str)\n",
    "            # If the result object contains a key (like \"scopus\") with a JSON string\n",
    "            if isinstance(db_obj, dict) and \"scopus\" in db_obj:\n",
    "                articles = json.loads(db_obj[\"scopus\"])\n",
    "            elif isinstance(db_obj, list):\n",
    "                articles = db_obj\n",
    "            else:\n",
    "                articles = []\n",
    "            total_results += len(articles)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing results for {db}: {e}\")\n",
    "    \n",
    "    print(f\"Total results found: {total_results}\")\n",
    "    \n",
    "    # Determine if the query needs adjustment\n",
    "    if total_results < min_threshold:\n",
    "        # Not enough articles: broaden the query (remove some constraints)\n",
    "        state[\"relax_query\"] = True\n",
    "        state[\"adjust_query\"] = \"broaden\"\n",
    "        print(\"Insufficient results; triggering relaxed (broaden) search string generation.\")\n",
    "    elif total_results > max_threshold:\n",
    "        # Too many articles: tighten the query (add more constraints)\n",
    "        state[\"relax_query\"] = True\n",
    "        state[\"adjust_query\"] = \"tighten\"\n",
    "        print(\"Too many results; triggering tightened search string generation.\")\n",
    "    else:\n",
    "        state[\"relax_query\"] = False\n",
    "        state[\"adjust_query\"] = \"none\"\n",
    "        print(\"The number of results is within the desired range; no adjustment needed.\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# After obtaining search_text from the LLM\n",
    "# Remove any numbers inside quotes, but leave the PUBYEAR constraint intact.\n",
    "# This regex removes numbers inside quotes (if any), but you may need to adjust it to your specific output format.\n",
    "def remove_numeric_keywords(search_str: str) -> str:\n",
    "    # This pattern removes digits that are inside double quotes, e.g., \"2023\"\n",
    "    return re.sub(r'\"(\\d+)\"', '\"\"', search_str)\n",
    "\n",
    "\n",
    "@profile_node\n",
    "def generate_search_string_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generates an optimized search string using the input text.\n",
    "    If 'relax_query' is True, the query is adjusted according to 'adjust_query':\n",
    "      - 'broaden': generates a broader query by either adding alternative keywords (joined by OR) within groups or by removing one or more entire groups (joined by AND),\n",
    "         based on the current iteration.\n",
    "      - 'tighten': generates a more specific query by adding more constraints.\n",
    "    The prompt uses the iteration number to vary the search string.\n",
    "    Also appends a publication year constraint if the input text contains a time limit.\n",
    "    \"\"\"\n",
    "    \n",
    "    context_llm = state.get(\"context_llm\", \"\")\n",
    "    print(\"[generate_search_string_node] Refined context received:\", context_llm)\n",
    "    \n",
    "    query_user_input = state[\"input_text\"]\n",
    "    metadata_path = os.path.join(base_output_json_dir, filename)\n",
    "\n",
    "    # Initialize iteration counter if not present, then increment it.\n",
    "    if \"iteration\" not in state:\n",
    "        state[\"iteration\"] = 0\n",
    "    state[\"iteration\"] += 1\n",
    "    current_iter = state[\"iteration\"]\n",
    "    print(f\"[generate_search_string_node] Current iteration: {current_iter}\")\n",
    "\n",
    "    # Define maximum iterations allowed before forcing a relaxed query.\n",
    "    max_iterations = 10\n",
    "    if current_iter >= max_iterations:\n",
    "        print(\"[generate_search_string_node] Maximum iterations reached; switching to 'broaden' mode.\")\n",
    "        state[\"adjust_query\"] = \"broaden\"    \n",
    "\n",
    "    adjust_query = state.get(\"adjust_query\", \"none\")\n",
    "\n",
    "    # Define the system prompt according to adjustment mode.\n",
    "    # TITLE-ABS-KEY\n",
    "    if adjust_query == \"broaden\":\n",
    "        system_prompt = (\n",
    "            \"You are an expert at generating search strings for research queries. \"\n",
    "            \"The current iteration number is {iteration}. \"\n",
    "            \"Generate a broader search string by relaxing the constraints in two ways: \"\n",
    "            \"1) Randomly insert a number of new keywords (between 3 and 5) as additional OR alternatives within existing groups; \"\n",
    "            \"2) Randomly remove a number of keywords (between 1 and 3) from groups, while keeping the most representative keywords. \"\n",
    "            \"For each group, add synonyms or domain-related keywords using OR so that the group covers alternative terms. \"\n",
    "            \"Each group represents a parallel domain defined by the research questions, and groups are combined with AND. \"\n",
    "            \"Each keyword must be a natural phrase or term, and must use spaces between words. Do not concatenate multiple words into a single token.\"\n",
    "            \"Return the search string in the following exact format (do not include any extra text):\\n\\n\"\n",
    "            \"ALL ( ( Keyword1 OR Keyword2 OR ... OR KeywordN ) AND \"\n",
    "            \"( KeywordA OR KeywordB OR ... OR KeywordM ) AND ... )\\n\\n\"\n",
    "            \"Do not include the publication year as a keyword.\"\n",
    "            \"Ensure that no special characters like '/' are used in any keyword.\"\n",
    "        )\n",
    "    elif adjust_query == \"tighten\":\n",
    "        system_prompt = (\n",
    "            \"You are an expert at generating search strings for research queries. \"\n",
    "            \"The current iteration number is {iteration}. \"\n",
    "            \"Generate a more specific search string by adding additional specific keywords to each group while still respecting the research questions and goals. \"\n",
    "            \"Ensure that within each group keywords are combined with OR and groups are combined with AND.\"\n",
    "            \"All keywords must be unique.\"\n",
    "            \"Each keyword must be a natural phrase or term, and must use spaces between words. Do not concatenate multiple words into a single token.\"\n",
    "            \"Return the search string in the following exact format (do not include any extra text):\\n\\n\"\n",
    "            \"ALL ( ( Keyword1 OR Keyword2 OR ... OR KeywordN ) AND \"\n",
    "            \"( KeywordA OR KeywordB OR ... OR KeywordM ) AND ... )\\n\\n\"\n",
    "            \"Do not include the publication year as a keyword.\"\n",
    "            \"Ensure that no special characters like '/' are used in any keyword.\"\n",
    "        )\n",
    "    else:\n",
    "        system_prompt = (\n",
    "            \"You are an expert at generating search strings for research queries. \"\n",
    "            \"The current iteration number is {iteration}. \"\n",
    "            \"Generate a search string using only the logical operators OR and AND, where within each group keywords are combined with OR and groups are combined with AND. \"\n",
    "            \"Ensure that all keywords in the search string are unique and that the search string varies with each iteration. \"\n",
    "            \"Each keyword must be a natural phrase or term, and must use spaces between words. Do not concatenate multiple words into a single token.\"\n",
    "            \"Return the search string in the following exact format (do not include any extra text):\\n\\n\"\n",
    "            \"ALL ( ( Keyword1 OR Keyword2 OR ... OR KeywordN ) AND \"\n",
    "            \"( KeywordA OR KeywordB OR ... OR KeywordM ) AND ... )\\n\\n\"\n",
    "            \"Do not include the publication year as a keyword.\"\n",
    "            \"Ensure that no special characters like '/' are used in any keyword.\"\n",
    "        )\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", \n",
    "         f\"User question:\\n{query_user_input}\\n\\n\"\n",
    "         \"Based on the user question, the current iteration, and the refined context below, generate an optimal search string composed solely of essential, unique keywords. \"\n",
    "         \"Use the following structure: within each group, keywords are combined with OR; between groups, use AND. \"\n",
    "         \"If necessary to increase the number of results, modify the query by either adding new keywords (3 to 5) in OR or by removing one or more groups. \"\n",
    "         \"Return only the search string in the exact format specified above.\\n\\n\"\n",
    "         \"Refined Context: {context_llm}\\n\\n\"\n",
    "         \"Search String:\"\n",
    "         )\n",
    "    ])\n",
    "\n",
    "    # search_string = (prompt_template | llm_for_context).invoke({\"context_llm\": context_llm})\n",
    "    # Invoke the LLM chain for string generation\n",
    "    start_time_llm = time.time()\n",
    "    search_string = (prompt_template | llm_for_context).invoke({\"context_llm\": context_llm, \"iteration\": current_iter}) \n",
    "\n",
    "    print(f\"Final String:{search_string}\")\n",
    "    end_time_llm = time.time()\n",
    "    execution_time = end_time_llm - start_time_llm\n",
    "    \n",
    "    # Extract the string from the result\n",
    "    if LLM_TYPE != 'Ollama':\n",
    "        string_output = search_string.content.strip()\n",
    "    else:\n",
    "        # string_output = search_string.strip()\n",
    "        # Extract the string from the result\n",
    "        if hasattr(search_string, \"content\"):\n",
    "            string_output = search_string.content.strip()\n",
    "        else:\n",
    "            string_output = str(search_string).strip()\n",
    "    \n",
    "    # Build metadata for the response\n",
    "    if LLM_TYPE != 'Ollama':\n",
    "        metadata = {\n",
    "            \"response_length\": len(string_output),\n",
    "            \"execution_time\": execution_time,\n",
    "            \"temperature\": temperature,\n",
    "            \"usage\": search_string.usage_metadata,\n",
    "            \"price_usd\": search_string.usage_metadata.get(\"input_tokens\", 0) * PRICE_PER_INPUT_TOKEN +\n",
    "                         search_string.usage_metadata.get(\"output_tokens\", 0) * PRICE_PER_OUTPUT_TOKEN,\n",
    "            \"model_name\": model_name\n",
    "        }\n",
    "    else:\n",
    "        metadata = {\n",
    "            \"response_length\": len(string_output),\n",
    "            \"execution_time\": execution_time,\n",
    "            \"temperature\": temperature,\n",
    "            \"model_name\": model_name\n",
    "        }\n",
    "\n",
    "    # Save the string and metadata to output files\n",
    "    save_metadata(metadata_path, metadata)\n",
    "\n",
    "    print(f\"Metadata saved to: {metadata_path}\")\n",
    "\n",
    "    search_text = search_string.content if hasattr(search_string, \"content\") else str(search_string)\n",
    "    search_text = search_text.strip()\n",
    "\n",
    "    cleaned_search_text_year = remove_numeric_keywords(search_text)\n",
    "    # Further clean-up if needed (e.g., remove extra spaces)\n",
    "    cleaned_search_text_year = re.sub(r'\\s+', ' ', cleaned_search_text_year).strip()\n",
    "\n",
    "    # If the user has entered a time constraint in the input, such as \"after 2020\", add the PUBYEAR constraint.\n",
    "    year_match = re.search(r'after\\s+(\\d{4})', state.get(\"input_text\", \"\"), re.IGNORECASE)\n",
    "    if year_match:\n",
    "        year = year_match.group(1)\n",
    "        cleaned_search_text_year += f\" AND PUBYEAR > {year}\"\n",
    "    \n",
    "    cleaned_search_text = re.sub(r'\\s*site:\\S+(?:\\s*OR\\s*site:\\S+)+', '', cleaned_search_text_year)\n",
    "    state[\"search_string\"] = cleaned_search_text\n",
    "    print(\"[generate_search_string_node] Search string (final):\", state[\"search_string\"])\n",
    "    return state\n",
    "\n",
    "@profile_node\n",
    "def extract_databases_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extracts a list of database names from the input_text.\n",
    "    It looks for an explicit \"databases:\" segment and also searches for known\n",
    "    database names (e.g., scopus, ieee, sciencedirect) anywhere in the input.\n",
    "    If no known databases are found, the system notifies the user and requests\n",
    "    a valid database to be entered.\n",
    "    \"\"\"\n",
    "    input_text = state.get(\"input_text\", \"\")\n",
    "    print(\"[extract_databases_node] Input text:\", input_text)\n",
    "    lower_text = input_text.lower()\n",
    "    extracted_dbs = []\n",
    "    \n",
    "    # First, check for an explicit \"databases:\" segment.\n",
    "    if \"databases:\" in lower_text:\n",
    "        idx = lower_text.find(\"databases:\")\n",
    "        db_string = input_text[idx + len(\"databases:\"):].strip()\n",
    "        extracted_dbs = extract_db_names(db_string)\n",
    "    \n",
    "    # List of known database names to search for.\n",
    "    known_dbs = [\"scopus\", \"ieee\", \"sciencedirect\"]\n",
    "    # Check if any of the known database names appear in the input text.\n",
    "    for db in known_dbs:\n",
    "        if db in lower_text and db not in extracted_dbs:\n",
    "            extracted_dbs.append(db)\n",
    "\n",
    "    # If no known databases were found, notify the user and prompt for input.\n",
    "    if not extracted_dbs:\n",
    "        print(\"[extract_databases_node] No known databases found in the input.\")\n",
    "        valid_db_input = input(\"Please enter a valid database (e.g., scopus, ieee, sciencedirect): \")\n",
    "        extracted_dbs = extract_db_names(valid_db_input)\n",
    "        # Validate that the provided database is one of the known ones.\n",
    "        valid_extracted_dbs = [db for db in extracted_dbs if db in known_dbs]\n",
    "        if not valid_extracted_dbs:\n",
    "            print(\"[extract_databases_node] Invalid database entered. Please try again.\")\n",
    "            valid_db_input = input(\"Please enter a valid database (e.g., scopus, ieee, sciencedirect): \")\n",
    "            extracted_dbs = extract_db_names(valid_db_input)\n",
    "            valid_extracted_dbs = [db for db in extracted_dbs if db in known_dbs]\n",
    "        extracted_dbs = valid_extracted_dbs\n",
    "    \n",
    "    state[\"databases\"] = extracted_dbs\n",
    "    print(\"[extract_databases_node] Databases extracted:\", state[\"databases\"])\n",
    "    return state\n",
    "\n",
    "@profile_node\n",
    "def format_queries_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Formats the search_string into specific queries for each database.\n",
    "    \"\"\"\n",
    "    search_string = state.get(\"search_string\", \"\")\n",
    "    databases = state.get(\"databases\", [])\n",
    "    print(\"[format_queries_node] Search string:\", search_string)\n",
    "    print(\"[format_queries_node] Databases:\", databases)\n",
    "    formatted = {}\n",
    "    for db in databases:\n",
    "        formatted[db] = format_db_query(db, search_string)\n",
    "    state[\"formatted_queries\"] = formatted\n",
    "    print(\"[format_queries_node] Formatted queries:\", state[\"formatted_queries\"])\n",
    "    return state\n",
    "\n",
    "@profile_node\n",
    "def run_db_search_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Executes the search for each database using the formatted queries.\n",
    "    \"\"\"\n",
    "    formatted_queries = state.get(\"formatted_queries\", {})\n",
    "    print(\"[run_db_search_node] Formatted queries:\", formatted_queries)\n",
    "    results = {}\n",
    "    for db, f_query in formatted_queries.items():\n",
    "        tool_input = json.dumps({\"query\": f_query, \"dbs\": db})\n",
    "        result = multi_db_search_wrapper(state, tool_input)\n",
    "        results[db] = result\n",
    "    state[\"db_results\"] = results\n",
    "    # print(\"[run_db_search_node] Database results:\", state[\"db_results\"])\n",
    "    return state\n",
    "\n",
    "@profile_node\n",
    "def aggregate_results_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Aggregates the search_string and database results into a final output.\n",
    "    \"\"\"\n",
    "    state[\"final_output\"] = {\n",
    "        \"search_string\": state.get(\"search_string\", \"\"),\n",
    "        \"db_results\": state.get(\"db_results\", {})\n",
    "    }\n",
    "    # print(\"[aggregate_results_node] Final output:\", state[\"final_output\"])\n",
    "    return state\n",
    "\n",
    "###################################\n",
    "#         ROUTER QUESTION         #\n",
    "###################################\n",
    "\n",
    "# Starting node: route question decides between web_search and vectorstore (retrieve)\n",
    "def route_question(state: GraphState) -> str:\n",
    "    # If the flag is present, skip the routing and return a special key (\"skip\")\n",
    "    if state.get(\"skip_router\", False):\n",
    "        print(\"Skipping routing; moving directly to cache_context.\")\n",
    "        return \"skip\"\n",
    "        \n",
    "    print(\"--- ROUTE QUESTION ---\")\n",
    "    question_val = state[\"question\"]\n",
    "    source = question_router.invoke({\"question\": question_val, \"topics_str\": topics_str})\n",
    "        \n",
    "    # Normalize the datasource value.\n",
    "    datasource = source.datasource.lower().strip()\n",
    "    if datasource == \"vectorstore\":\n",
    "        print(\"--- Routing to vectorstore ---\")\n",
    "        state[\"branch\"] = \"retrieve\"\n",
    "        return \"vectorstore\"\n",
    "    elif datasource == \"web_search\":\n",
    "        print(\"--- Routing to web search ---\")\n",
    "        state[\"branch\"] = \"web_search\"\n",
    "        return \"web_search\"\n",
    "    state[\"branch\"] = \"retrieve\"\n",
    "    return \"vectorstore\"\n",
    "\n",
    "###################################\n",
    "#       GRAPH WORKFLOW SETUP      #\n",
    "###################################\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_edge(START, \"generate_query\") # generate_query\n",
    "\n",
    "# Add the new node to the workflow\n",
    "workflow.add_node(\"generate_query\", generate_query_node)\n",
    "workflow.add_node(\"web_search\", web_search)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)             # Merged generate node\n",
    "workflow.add_node(\"generate_web\", generate_web)\n",
    "workflow.add_node(\"transform_query\", transform_query) # Merged transform node\n",
    "workflow.add_node(\"transform_query_web\", transform_query_web)\n",
    "workflow.add_node(\"cache_context\", cache_context_node)  # Caching node\n",
    "\n",
    "# Add evaluation nodes for vectorstore branch\n",
    "workflow.add_node(\"evaluate_rag_output\", evaluate_rag_output)\n",
    "workflow.add_node(\"evaluate_bert_score\", evaluate_bert_score)\n",
    "\n",
    "# Add evaluation nodes for web search branch\n",
    "workflow.add_node(\"evaluate_web_search_output\", evaluate_web_search_output)\n",
    "workflow.add_node(\"evaluate_web_bert_score\", evaluate_web_bert_score)\n",
    "\n",
    "# Add an edge from the START node to the new \"generate_query\" node\n",
    "workflow.add_edge(START, \"generate_query\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_query\",\n",
    "    route_question,\n",
    "    {\n",
    "        \"skip\": \"cache_context\", # If the flag is active, go directly to cache_context_node\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"vectorstore\": \"retrieve\",  # Key now matches the returned normalized value\n",
    "    },\n",
    ")\n",
    "\n",
    "# For the web search branch, send directly to generate.\n",
    "workflow.add_edge(\"web_search\", \"generate_web\")\n",
    "\n",
    "# For the retrieve branch, first go to grade_documents.\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "# After grading, decide whether to generate or transform.\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    lambda state: decide_to_generate(state),\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_edge(\"transform_query_web\", \"web_search\")\n",
    "\n",
    "# After generate/generate_web, grade the generation.\n",
    "# If the generation is \"useful\", route to the caching node.\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        # \"not supported\": \"generate\",\n",
    "        \"useful\": \"evaluate_rag_output\",\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"evaluate_rag_output\", \"evaluate_bert_score\")\n",
    "workflow.add_edge(\"evaluate_bert_score\", \"cache_context\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_web\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        # \"not supported\": \"generate_web\",\n",
    "        \"useful\": \"evaluate_web_search_output\",\n",
    "        \"not useful\": \"transform_query_web\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"evaluate_web_search_output\", \"evaluate_web_bert_score\")\n",
    "workflow.add_edge(\"evaluate_web_bert_score\", \"cache_context\")\n",
    "\n",
    "\n",
    "# Database branch nodes (from generate_search_string onward)\n",
    "workflow.add_node(\"generate_search_string\", generate_search_string_node)\n",
    "workflow.add_node(\"extract_databases\", extract_databases_node)\n",
    "workflow.add_node(\"format_queries\", format_queries_node)\n",
    "workflow.add_node(\"run_db_search\", run_db_search_node)\n",
    "workflow.add_node(\"aggregate_results\", aggregate_results_node)\n",
    "\n",
    "# After cache_context, proceed with database branch nodes\n",
    "workflow.add_edge(\"cache_context\", \"generate_search_string\")\n",
    "workflow.add_edge(\"generate_search_string\", \"extract_databases\")\n",
    "workflow.add_edge(\"extract_databases\", \"format_queries\")\n",
    "workflow.add_edge(\"format_queries\", \"run_db_search\")\n",
    "\n",
    "# Add the node to evaluate the results\n",
    "workflow.add_node(\"evaluate_results\", evaluate_results_node)\n",
    "\n",
    "# Modify the flow: after run_db_search, proceed to evaluate_results\n",
    "workflow.add_edge(\"run_db_search\", \"evaluate_results\")\n",
    "\n",
    "# If evaluate_results sets relax_query = True, redirect to generate_search_string\n",
    "workflow.add_conditional_edges(\n",
    "    \"evaluate_results\",\n",
    "    lambda state: \"relax\" if state.get(\"relax_query\") else \"continue\",\n",
    "    {\n",
    "        \"relax\": \"generate_search_string\",  # Return to the node to regenerate the query (in relax mode)\n",
    "        \"continue\": \"aggregate_results\"     # Continue with aggregation if the results are sufficient\n",
    "    },\n",
    ")\n",
    "\n",
    "# workflow.add_edge(\"run_db_search\", \"aggregate_results\")\n",
    "workflow.add_edge(\"aggregate_results\", END)\n",
    "\n",
    "# (Optional) Graph visualization\n",
    "\"\"\"\n",
    "try:\n",
    "    from IPython.display import display, Markdown, Image\n",
    "    graph = workflow.compile().get_graph()\n",
    "    graph.mermaid_config = {\"graph_direction\": \"TD\"}\n",
    "    png_bytes = graph.draw_mermaid_png()\n",
    "    with open(\"graph.png\", \"wb\") as f:\n",
    "        f.write(png_bytes)\n",
    "    print(\"The graph has been saved as 'graph.png'.\")\n",
    "    display(Markdown(\"### LangGraph Visualization ###\"))\n",
    "    display(Image(png_bytes))\n",
    "except Exception as e:\n",
    "    print(\"Graph rendering failed:\", e)\n",
    "\"\"\"\n",
    "\n",
    "###################################\n",
    "#       EXECUTION OF WORKFLOW     #\n",
    "###################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initial state: note that for the database part, input_text is required\n",
    "    \"\"\"\n",
    "    user_text = (\n",
    "                Purpose To identify and classify existing solutions leveraging the combination of MDE, DevOps, and AI/ML principles and practices supporting the system and software engineering of cyber-physical systems from the point of view of researchers and practitioners.\n",
    "                RQ1 : Is there a systems and software engineering methodology that explicitly incorporates and integrates the principles and practices of MDE, AI/ML, and DevOps research areas? If such a methodology exists, how does it combine these research areas?\n",
    "                RQ2 : Are the principles and practices of MDE, AI/ML, and DevOps research areas integrated throughout the entire process, or are they applied to specific engineering activities?\n",
    "                RQ3 : Which research fields and application domains are the target of these approaches?\n",
    "                RQ4 : What are the future research directions?\n",
    "                Database: scopus\n",
    "                Pubblication year higher then 2006\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    user_text = (\n",
    "        \"\"\"\n",
    "Research Questions:\n",
    "\n",
    "RQ0: What are the bibliometric key facts of peer-reviewed literature documenting applications of MDE to DTs?\n",
    "RQ0.1: In which years are they published?\n",
    "RQ0.2: In which types of venues are they published?\n",
    "\n",
    "RQ1: How and how often are automation techniques applied to DTs in peer-reviewed literature?\n",
    "RQ1.1: How often are the different automation techniques applied in the context of DTs?\n",
    "RQ1.2: Which modeling artifacts and software artifacts are used by these automation techniques?\n",
    "RQ1.3: Which combinations of input and output artifacts are used by these automation techniques?\n",
    "RQ1.4: What is the research type of the studies that apply these automation techniques?\n",
    "\n",
    "RQ2: To which types of DTs are automation techniques applied in peer-reviewed literature?\n",
    "RQ2.1: Which TT does the DT represent, in which SLCP of the TT are automation techniques applied, and what is the TLCP of DTs to which automation techniques are applied?\n",
    "RQ2.2: How does the application of automation techniques (identified with RQ1) vary for different DT types?\n",
    "\n",
    "RQ3: In which domains are automation techniques applied to DTs in peer-reviewed literature?\n",
    "RQ3.1: For which domains are automation techniques applied to DTs?\n",
    "RQ3.2: How does the application of automation techniques (identified with RQ1) vary for the identified domains?\n",
    "RQ3.3: How does the DT type (identified with RQ2) vary for the identified domains?\n",
    "\n",
    "Database: Scopus\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    #user_text = input(\"Enter the search text: \")\n",
    "    \"\"\"\n",
    "    user_text = \"automated software engineering using LLM, pubblished after 2020, database scopus\"\n",
    "    \"\"\"\n",
    "    \n",
    "    initial_state: GraphState = {\n",
    "        \"input_text\": user_text\n",
    "    }\n",
    "    final_state = list(workflow.compile().stream(initial_state, config={\"recursion_limit\": 100}))[-1]\n",
    "    \n",
    "    print(\"\\n--- FINAL WORKFLOW STATE ---\\n\")\n",
    "    \n",
    "    # If the useful data is under \"final_output\", use that; otherwise, use final_state directly.\n",
    "    data = final_state.get(\"aggregate_results\", final_state)\n",
    "    \n",
    "    # Print the various elements\n",
    "    print(\"\\n--- FINAL WORKFLOW STATE ---\\n\")\n",
    "    \n",
    "    print(\"Input Text:\")\n",
    "    print(json.dumps(data.get(\"input_text\", {}), indent=4))\n",
    "    \n",
    "    print(\"\\nDatabases:\")\n",
    "    print(json.dumps(data.get(\"databases\", {}), indent=4))\n",
    "    \n",
    "    print(\"\\nFormatted Queries:\")\n",
    "    print(json.dumps(data.get(\"formatted_queries\", {}), indent=4))\n",
    "    \n",
    "    print(\"\\nSearch String:\")\n",
    "    print(json.dumps(data.get(\"search_string\", {}), indent=4))\n",
    "    \n",
    "    print(\"\\nDB Results:\")\n",
    "    #print(json.dumps(data.get(\"db_results\", {}), indent=4))\n",
    "    db_results = data.get(\"db_results\", {})\n",
    "\n",
    "    # For each database (here, \"scopus\")\n",
    "    for db, json_str in db_results.items():\n",
    "        print(f\"\\nResults for {db}:\")\n",
    "    \n",
    "        # Convert the JSON string into a Python object\n",
    "        try:\n",
    "            # In our case the value is something like '{\"scopus\": \"[{...}, {...}]\" }'\n",
    "            db_obj = json.loads(json_str)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing results for {db}: {e}\")\n",
    "            continue\n",
    "    \n",
    "        # If the structure has a key (like \"scopus\") containing a JSON string,\n",
    "        # parse that as well.\n",
    "        if isinstance(db_obj, dict) and \"scopus\" in db_obj:\n",
    "            try:\n",
    "                articles = json.loads(db_obj[\"scopus\"])\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing articles for {db}: {e}\")\n",
    "                continue\n",
    "        else:\n",
    "            articles = db_obj\n",
    "    \n",
    "        # Now, iterate through the list of articles and print each article in a pretty format\n",
    "        for article in articles:\n",
    "            print(json.dumps(article, indent=4))\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    # Print aggregate results (if available)\n",
    "    \"\"\"\n",
    "    if \"aggregate_results\" in final_state:\n",
    "        print(\"Aggregate Results:\")\n",
    "        print(json.dumps(final_state[\"aggregate_results\"], indent=4))\n",
    "        print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "    else:\n",
    "        print(\"No aggregate_results found.\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # Optionally, print the entire final_state\n",
    "    # print(\"Full Final State:\")\n",
    "    # print(json.dumps(final_state, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4712151-6b30-4014-98b8-bb6e9603fb88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529a02b-5328-44db-9af2-917b8f847f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6fe5ab8-556d-4ff4-ae57-c3df7f715502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"<API_KEY>\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",  # Usa un modello valido come 'gpt-4o'\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Decide whether a Tweet's sentiment is positive, neutral, or negative.\"},\n",
    "        {\"role\": \"user\", \"content\": \"I did not like the new Batman movie!\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65422ef5-0441-4c37-bdea-db9d1d9207cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
