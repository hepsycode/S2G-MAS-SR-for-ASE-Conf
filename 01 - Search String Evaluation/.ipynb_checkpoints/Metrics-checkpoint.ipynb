{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4cbb8f9-3b7a-49e9-8fd4-d1ee188ee41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "\n",
    "# === SIMILARITY FUNCTIONS ===\n",
    "def levenshtein_similarity(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def cosine_text_similarity(a, b):\n",
    "    vectorizer = TfidfVectorizer().fit([a, b])\n",
    "    tfidf = vectorizer.transform([a, b])\n",
    "    return cosine_similarity(tfidf[0], tfidf[1])[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5efd82f-458d-47d0-8609-82b3e259dc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              LLM                                               Test  \\\n",
      "0       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-0.0-Test01   \n",
      "1       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-0.0-Test02   \n",
      "2       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-0.0-Test03   \n",
      "3       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-1.0-Test01   \n",
      "4       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-1.0-Test02   \n",
      "5       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-1.0-Test03   \n",
      "6       Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-0.0-Test01   \n",
      "7       Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-0.0-Test02   \n",
      "8       Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-0.0-Test03   \n",
      "9       Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-1.0-Test01   \n",
      "10      Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-1.0-Test02   \n",
      "11      Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-1.0-Test03   \n",
      "12    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-0.0-Test01   \n",
      "13    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-0.0-Test02   \n",
      "14    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-0.0-Test03   \n",
      "15    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-1.0-Test01   \n",
      "16    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-1.0-Test02   \n",
      "17    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-1.0-Test03   \n",
      "18    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-0.0-Test01   \n",
      "19    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-0.0-Test02   \n",
      "20    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-0.0-Test03   \n",
      "21    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-1.0-Test01   \n",
      "22    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-1.0-Test02   \n",
      "23    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-1.0-Test03   \n",
      "24  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-0.0-T...   \n",
      "25  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-0.0-T...   \n",
      "26  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-0.0-T...   \n",
      "27  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-1.0-T...   \n",
      "28  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-1.0-T...   \n",
      "29  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-1.0-T...   \n",
      "30  Mistral-Large  00-Query-Agent-RAG-WEB-MDTE-mistral-large-late...   \n",
      "31  Mistral-Large  00-Query-Agent-RAG-WEB-MDTE-mistral-large-late...   \n",
      "32  Mistral-Large  00-Query-Agent-RAG-WEB-MDTE-mistral-large-late...   \n",
      "33  Mistral-Large  00-Query-Agent-RAG-WEB-MDTE-mistral-large-late...   \n",
      "\n",
      "    Levenshtein    Cosine  Precision    Recall        F1   TP    FP    FN  \n",
      "0      0.015649  0.617313   0.688889  0.003698  0.007357   31    14  8351  \n",
      "1      0.287313  0.494116   0.519695  0.048795  0.089214  409   378  7973  \n",
      "2      0.287313  0.494116   0.519695  0.048795  0.089214  409   378  7973  \n",
      "3      0.271062  0.367708   1.000000  0.003102  0.006185   26     0  8356  \n",
      "4      0.044164  0.652862   1.000000  0.000119  0.000239    1     0  8381  \n",
      "5      0.129915  0.596623   0.153846  0.000239  0.000476    2    11  8380  \n",
      "6      0.021739  0.785148   0.583333  0.005011  0.009936   42    30  8340  \n",
      "7      0.086687  0.799663   0.318182  0.019208  0.036229  161   345  8221  \n",
      "8      0.021739  0.785148   0.583333  0.005011  0.009936   42    30  8340  \n",
      "9      0.086687  0.799663   0.318182  0.019208  0.036229  161   345  8221  \n",
      "10     0.086687  0.799663   0.318182  0.019208  0.036229  161   345  8221  \n",
      "11     0.021739  0.785148   0.583333  0.005011  0.009936   42    30  8340  \n",
      "12     0.164773  0.760989   0.577982  0.007516  0.014839   63    46  8319  \n",
      "13     0.143448  0.734289   0.840237  0.016941  0.033212  142    27  8240  \n",
      "14     0.143448  0.734289   0.840237  0.016941  0.033212  142    27  8240  \n",
      "15     0.143906  0.722784   0.800000  0.001909  0.003809   16     4  8366  \n",
      "16     0.191933  0.771432   0.611111  0.002625  0.005227   22    14  8360  \n",
      "17     0.076583  0.753729   0.743590  0.003460  0.006888   29    10  8353  \n",
      "18     0.028346  0.789821   0.237152  0.045693  0.076623  383  1232  7999  \n",
      "19     0.021739  0.785148   0.583333  0.005011  0.009936   42    30  8340  \n",
      "20     0.021739  0.785148   0.583333  0.005011  0.009936   42    30  8340  \n",
      "21     0.021739  0.785148   0.583333  0.005011  0.009936   42    30  8340  \n",
      "22     0.086687  0.799663   0.318182  0.019208  0.036229  161   345  8221  \n",
      "23     0.086687  0.799663   0.318182  0.019208  0.036229  161   345  8221  \n",
      "24     0.071186  0.673626   0.500000  0.003579  0.007107   30    30  8352  \n",
      "25     0.130742  0.647532   0.484848  0.001909  0.003803   16    17  8366  \n",
      "26     0.071186  0.673626   0.500000  0.003579  0.007107   30    30  8352  \n",
      "27     0.221402  0.630655   0.468750  0.001790  0.003565   15    17  8367  \n",
      "28     0.130742  0.647532   0.484848  0.001909  0.003803   16    17  8366  \n",
      "29     0.071186  0.673626   0.500000  0.003579  0.007107   30    30  8352  \n",
      "30     0.086687  0.799663   0.318182  0.019208  0.036229  161   345  8221  \n",
      "31     0.021739  0.785148   0.583333  0.005011  0.009936   42    30  8340  \n",
      "32     0.086687  0.799663   0.318182  0.019208  0.036229  161   345  8221  \n",
      "33     0.086687  0.799663   0.318182  0.019208  0.036229  161   345  8221  \n"
     ]
    }
   ],
   "source": [
    "########## OK - Compare search on Scopus #################\n",
    "# Locate the reference string and GT file\n",
    "# base_dir = os.path.join(extract_path, \"Model-driven engineering for digital twins\")\n",
    "base_dir = os.path.join(os.getcwd(), \"Model-based Trustworthiness Evaluation\")\n",
    "\n",
    "ref_path = [f for f in os.listdir(base_dir) if f.endswith(\"String.txt\")][0]\n",
    "gt_path = [f for f in os.listdir(base_dir) if f.endswith(\"scopus-Full.csv\")][0]\n",
    "\n",
    "with open(os.path.join(base_dir, ref_path), encoding='utf-8') as f:\n",
    "    ref_string = f.read().strip()\n",
    "\n",
    "gt_df = pd.read_csv(os.path.join(base_dir, gt_path))\n",
    "gt_titles = set(gt_df['title'].dropna().str.strip().str.lower())\n",
    "\n",
    "results = []\n",
    "def evaluate_llm():\n",
    "    for llm_dir in os.listdir(base_dir):\n",
    "        llm_path = os.path.join(base_dir, llm_dir)\n",
    "        if not os.path.isdir(llm_path):\n",
    "            continue\n",
    "        for test_dir in os.listdir(llm_path):\n",
    "            test_path = os.path.join(llm_path, test_dir)\n",
    "            str_file = os.path.join(test_path, \"String.txt\")\n",
    "            csv_file = os.path.join(test_path, \"Scopus_Search_results.csv\")\n",
    "            if os.path.isfile(str_file) and os.path.isfile(csv_file):\n",
    "                with open(str_file, encoding='utf-8') as f:\n",
    "                    gen_string = f.read().strip()\n",
    "                try:\n",
    "                    results_df = pd.read_csv(csv_file)\n",
    "                except:\n",
    "                    continue\n",
    "                titles = set(results_df['title'].dropna().str.strip().str.lower())\n",
    "    \n",
    "                tp = len(titles & gt_titles)\n",
    "                fp = len(titles - gt_titles)\n",
    "                fn = len(gt_titles - titles)\n",
    "    \n",
    "                precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "                recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "                results.append({\n",
    "                    \"LLM\": llm_dir,\n",
    "                    \"Test\": test_dir,\n",
    "                    \"Levenshtein\": levenshtein_similarity(ref_string, gen_string),\n",
    "                    \"Cosine\": cosine_text_similarity(ref_string, gen_string),\n",
    "                    \"Precision\": precision,\n",
    "                    \"Recall\": recall,\n",
    "                    \"F1\": f1,\n",
    "                    \"TP\": tp,\n",
    "                    \"FP\": fp,\n",
    "                    \"FN\": fn\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(results)            \n",
    "\n",
    "df_results = evaluate_llm()\n",
    "print(df_results)\n",
    "df_results.to_csv(\"llm_metrics-scopus-full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4bf1322-f4f6-481e-ada8-23403bcefc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              LLM                                               Test  \\\n",
      "0       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-0.0-Test01   \n",
      "1       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-0.0-Test02   \n",
      "2       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-0.0-Test03   \n",
      "3       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-1.0-Test01   \n",
      "4       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-1.0-Test02   \n",
      "5       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-1.0-Test03   \n",
      "6       Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-0.0-Test01   \n",
      "7       Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-0.0-Test02   \n",
      "8       Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-0.0-Test03   \n",
      "9       Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-1.0-Test01   \n",
      "10      Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-1.0-Test02   \n",
      "11      Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-1.0-Test03   \n",
      "12    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-0.0-Test01   \n",
      "13    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-0.0-Test02   \n",
      "14    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-0.0-Test03   \n",
      "15    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-1.0-Test01   \n",
      "16    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-1.0-Test02   \n",
      "17    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-1.0-Test03   \n",
      "18    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-0.0-Test01   \n",
      "19    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-0.0-Test02   \n",
      "20    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-0.0-Test03   \n",
      "21    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-1.0-Test01   \n",
      "22    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-1.0-Test02   \n",
      "23    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-1.0-Test03   \n",
      "24  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-0.0-T...   \n",
      "25  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-0.0-T...   \n",
      "26  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-0.0-T...   \n",
      "27  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-1.0-T...   \n",
      "28  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-1.0-T...   \n",
      "29  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-1.0-T...   \n",
      "30  Mistral-Large  00-Query-Agent-RAG-WEB-MDTE-mistral-large-late...   \n",
      "31  Mistral-Large  00-Query-Agent-RAG-WEB-MDTE-mistral-large-late...   \n",
      "32  Mistral-Large  00-Query-Agent-RAG-WEB-MDTE-mistral-large-late...   \n",
      "33  Mistral-Large  00-Query-Agent-RAG-WEB-MDTE-mistral-large-late...   \n",
      "\n",
      "    Levenshtein    Cosine  Precision  Recall        F1  TP    FP  FN  \n",
      "0      0.015649  0.617313   0.000000     0.0  0.000000   0    90   5  \n",
      "1      0.287313  0.494116   0.000637     0.2  0.001271   1  1568   4  \n",
      "2      0.287313  0.494116   0.000637     0.2  0.001271   1  1568   4  \n",
      "3      0.271062  0.367708   0.000000     0.0  0.000000   0    52   5  \n",
      "4      0.044164  0.652862   0.000000     0.0  0.000000   0     2   5  \n",
      "5      0.129915  0.596623   0.041667     0.2  0.068966   1    23   4  \n",
      "6      0.021739  0.785148   0.000000     0.0  0.000000   0   144   5  \n",
      "7      0.086687  0.799663   0.001005     0.2  0.002000   1   994   4  \n",
      "8      0.021739  0.785148   0.000000     0.0  0.000000   0   144   5  \n",
      "9      0.086687  0.799663   0.001005     0.2  0.002000   1   994   4  \n",
      "10     0.086687  0.799663   0.001005     0.2  0.002000   1   994   4  \n",
      "11     0.021739  0.785148   0.000000     0.0  0.000000   0   144   5  \n",
      "12     0.164773  0.760989   0.000000     0.0  0.000000   0   218   5  \n",
      "13     0.143448  0.734289   0.002976     0.2  0.005865   1   335   4  \n",
      "14     0.143448  0.734289   0.002976     0.2  0.005865   1   335   4  \n",
      "15     0.143906  0.722784   0.000000     0.0  0.000000   0    40   5  \n",
      "16     0.191933  0.771432   0.000000     0.0  0.000000   0    72   5  \n",
      "17     0.076583  0.753729   0.000000     0.0  0.000000   0    78   5  \n",
      "18     0.028346  0.789821   0.000316     0.2  0.000631   1  3165   4  \n",
      "19     0.021739  0.785148   0.000000     0.0  0.000000   0   144   5  \n",
      "20     0.021739  0.785148   0.000000     0.0  0.000000   0   144   5  \n",
      "21     0.021739  0.785148   0.000000     0.0  0.000000   0   144   5  \n",
      "22     0.086687  0.799663   0.001005     0.2  0.002000   1   994   4  \n",
      "23     0.086687  0.799663   0.001005     0.2  0.002000   1   994   4  \n",
      "24     0.071186  0.673626   0.008403     0.2  0.016129   1   118   4  \n",
      "25     0.130742  0.647532   0.015152     0.2  0.028169   1    65   4  \n",
      "26     0.071186  0.673626   0.008403     0.2  0.016129   1   118   4  \n",
      "27     0.221402  0.630655   0.015625     0.2  0.028986   1    63   4  \n",
      "28     0.130742  0.647532   0.015152     0.2  0.028169   1    65   4  \n",
      "29     0.071186  0.673626   0.008403     0.2  0.016129   1   118   4  \n",
      "30     0.086687  0.799663   0.001005     0.2  0.002000   1   994   4  \n",
      "31     0.021739  0.785148   0.000000     0.0  0.000000   0   144   5  \n",
      "32     0.086687  0.799663   0.001005     0.2  0.002000   1   994   4  \n",
      "33     0.086687  0.799663   0.001005     0.2  0.002000   1   994   4  \n"
     ]
    }
   ],
   "source": [
    "################## OK - Compare String-Results with selected papers ################\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define base folder where the structure is located\n",
    "base_dir = os.path.join(os.getcwd(), \"Model-based Trustworthiness Evaluation\")\n",
    "\n",
    "# Locate reference string and GT dataset\n",
    "ref_path = [f for f in os.listdir(base_dir) if f.endswith(\"String.txt\")][0]\n",
    "gt_path = [f for f in os.listdir(base_dir) if f.endswith(\"Full - Scopus.csv\")][0]\n",
    "\n",
    "with open(os.path.join(base_dir, ref_path), encoding='utf-8') as f:\n",
    "    ref_string = f.read().strip()\n",
    "\n",
    "gt_df = pd.read_csv(os.path.join(base_dir, gt_path))\n",
    "\n",
    "# Normalize GT titles and dois\n",
    "gt_df['title_norm'] = gt_df['title'].astype(str).str.strip().str.lower()\n",
    "gt_df['doi_norm'] = gt_df['doi'].astype(str).str.strip().str.lower()\n",
    "gt_titles = set(gt_df['title_norm'])\n",
    "gt_dois = set(gt_df['doi_norm'])\n",
    "\n",
    "results = []\n",
    "\n",
    "def evaluate_llm():\n",
    "    for llm_dir in os.listdir(base_dir):\n",
    "        llm_path = os.path.join(base_dir, llm_dir)\n",
    "        if not os.path.isdir(llm_path):\n",
    "            continue\n",
    "        for test_dir in os.listdir(llm_path):\n",
    "            test_path = os.path.join(llm_path, test_dir)\n",
    "            str_file = os.path.join(test_path, \"String.txt\")\n",
    "            csv_file = os.path.join(test_path, \"Scopus_Search_results.csv\")\n",
    "\n",
    "            if os.path.isfile(str_file) and os.path.isfile(csv_file):\n",
    "                with open(str_file, encoding='utf-8') as f:\n",
    "                    gen_string = f.read().strip()\n",
    "\n",
    "                try:\n",
    "                    results_df = pd.read_csv(csv_file)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                # Normalize titles and DOIs in the retrieved set\n",
    "                results_df['title_norm'] = results_df['title'].astype(str).str.strip().str.lower()\n",
    "                results_df['doi_norm'] = results_df['doi'].astype(str).str.strip().str.lower()\n",
    "\n",
    "                titles = set(results_df['title_norm'])\n",
    "                dois = set(results_df['doi_norm'])\n",
    "\n",
    "                # Combined match: title OR doi\n",
    "                tp = len((titles & gt_titles) | (dois & gt_dois))\n",
    "                fp = len((titles | dois) - (gt_titles | gt_dois))\n",
    "                fn = len((gt_titles | gt_dois) - (titles | dois))\n",
    "\n",
    "                precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "                recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "                results.append({\n",
    "                    \"LLM\": llm_dir,\n",
    "                    \"Test\": test_dir,\n",
    "                    \"Levenshtein\": levenshtein_similarity(ref_string, gen_string),\n",
    "                    \"Cosine\": cosine_text_similarity(ref_string, gen_string),\n",
    "                    \"Precision\": precision,\n",
    "                    \"Recall\": recall,\n",
    "                    \"F1\": f1,\n",
    "                    \"TP\": tp,\n",
    "                    \"FP\": fp,\n",
    "                    \"FN\": fn\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# === Execute\n",
    "df_results = evaluate_llm()\n",
    "print(df_results)\n",
    "df_results.to_csv(\"llm_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c10dc77-2b96-40d4-9b4d-362a00ade80a",
   "metadata": {},
   "source": [
    "# String Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "178dfb9b-d1bf-4d44-b922-fb795930d43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             LLM  String_Consistency  Result_Consistency  Num_Tests\n",
      "0      Gemma2-2b              0.3003              0.1373         12\n",
      "1    Llama3.1-8B              0.3427              0.1532         12\n",
      "2  Mistral-Large              0.5647              0.3167         12\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from pytextdist.vector_similarity import jaccard_similarity\n",
    "\n",
    "base_dir = os.path.join(os.getcwd(), \"Model-based Trustworthiness Evaluation\")\n",
    "\n",
    "def jaccard_set(a, b):\n",
    "    return len(a & b) / len(a | b) if (a | b) else 1.0\n",
    "\n",
    "def compute_consistency(sets):\n",
    "    pairs = list(combinations(sets, 2))\n",
    "    if not pairs:\n",
    "        return 1.0\n",
    "    scores = [jaccard_set(a, b) for a, b in pairs]\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "def compute_string_consistency(strings):\n",
    "    pairs = list(combinations(strings, 2))\n",
    "    scores = [jaccard_similarity(s1, s2, n=2) for s1, s2 in pairs]\n",
    "    return sum(scores) / len(scores) if pairs else 1.0\n",
    "\n",
    "def compute_nested_consistency_metrics():\n",
    "    results = []\n",
    "\n",
    "    # Loop over all LLM root folders (e.g., Llama3.1-8B)\n",
    "    for llm_name in os.listdir(base_dir):\n",
    "        llm_path = os.path.join(base_dir, llm_name)\n",
    "        if not os.path.isdir(llm_path):\n",
    "            continue\n",
    "\n",
    "        # Gather all test runs (subfolders like ...-Test01, -Test02, -Test03)\n",
    "        test_runs = [\n",
    "            os.path.join(llm_path, subdir)\n",
    "            for subdir in os.listdir(llm_path)\n",
    "            if os.path.isdir(os.path.join(llm_path, subdir)) and \"Test\" in subdir\n",
    "        ]\n",
    "\n",
    "        string_variants = []\n",
    "        title_sets = []\n",
    "\n",
    "        for path in test_runs:\n",
    "            str_file = os.path.join(path, \"String.txt\")\n",
    "            csv_file = os.path.join(path, \"Scopus_Search_results.csv\")\n",
    "\n",
    "            if os.path.isfile(str_file) and os.path.isfile(csv_file):\n",
    "                try:\n",
    "                    with open(str_file, encoding='utf-8') as f:\n",
    "                        string_variants.append(f.read().strip())\n",
    "\n",
    "                    df = pd.read_csv(csv_file)\n",
    "                    titles = set(df['title'].dropna().astype(str).str.strip().str.lower())\n",
    "                    title_sets.append(titles)\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipped {path} due to error: {e}\")\n",
    "\n",
    "        if string_variants and title_sets:\n",
    "            string_cons = compute_string_consistency(string_variants)\n",
    "            result_cons = compute_consistency(title_sets)\n",
    "            results.append({\n",
    "                \"LLM\": llm_name,\n",
    "                \"String_Consistency\": round(string_cons, 4),\n",
    "                \"Result_Consistency\": round(result_cons, 4),\n",
    "                \"Num_Tests\": len(test_runs)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# === Run and Save\n",
    "df_consistency = compute_nested_consistency_metrics()\n",
    "print(df_consistency)\n",
    "df_consistency.to_csv(\"llm_consistency_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1c2be51f-efb1-4470-874e-3db0613668aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning base directory: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\n",
      "Exploring LLM folder: Gemma2-2b\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-MDTE-gemma2-2b-0.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-MDTE-gemma2-2b-0.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-MDTE-gemma2-2b-0.0-Test03\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-MDTE-gemma2-2b-1.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-MDTE-gemma2-2b-1.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-MDTE-gemma2-2b-1.0-Test03\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-0.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-0.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-0.0-Test03\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-1.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-1.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-1.0-Test03\n",
      "Exploring LLM folder: Llama3.1-8B\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-MDTE-llama3.1-0.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-MDTE-llama3.1-0.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-MDTE-llama3.1-0.0-Test03\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-MDTE-llama3.1-1.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-MDTE-llama3.1-1.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-MDTE-llama3.1-1.0-Test03\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-RAG-WEB-MDTE-llama3.1-0.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-RAG-WEB-MDTE-llama3.1-0.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-RAG-WEB-MDTE-llama3.1-0.0-Test03\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-RAG-WEB-MDTE-llama3.1-1.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-RAG-WEB-MDTE-llama3.1-1.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-RAG-WEB-MDTE-llama3.1-1.0-Test03\n",
      "Exploring LLM folder: Mistral-Large\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-MDTE-mistral-large-latest-0.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-MDTE-mistral-large-latest-0.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-MDTE-mistral-large-latest-0.0-Test03\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-MDTE-mistral-large-latest-1.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-MDTE-mistral-large-latest-1.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-MDTE-mistral-large-latest-1.0-Test03\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-RAG-WEB-MDTE-mistral-large-latest-0.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-RAG-WEB-MDTE-mistral-large-latest-0.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-RAG-WEB-MDTE-mistral-large-latest-0.0-Test03\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-RAG-WEB-MDTE-mistral-large-latest-1.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-RAG-WEB-MDTE-mistral-large-latest-1.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\ALESSIO-AUTOMATIC-LLM\\Results\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-RAG-WEB-MDTE-mistral-large-latest-1.0-Test03\n",
      "              LLM                                              Group  \\\n",
      "0       Gemma2-2b                  00-Query-Agent-MDTE-gemma2-2b-0.0   \n",
      "1       Gemma2-2b                  00-Query-Agent-MDTE-gemma2-2b-1.0   \n",
      "2       Gemma2-2b          00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-0.0   \n",
      "3       Gemma2-2b          00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-1.0   \n",
      "4     Llama3.1-8B                   00-Query-Agent-MDTE-llama3.1-0.0   \n",
      "5     Llama3.1-8B                   00-Query-Agent-MDTE-llama3.1-1.0   \n",
      "6     Llama3.1-8B           00-Query-Agent-RAG-WEB-MDTE-llama3.1-0.0   \n",
      "7     Llama3.1-8B           00-Query-Agent-RAG-WEB-MDTE-llama3.1-1.0   \n",
      "8   Mistral-Large       00-Query-Agent-MDTE-mistral-large-latest-0.0   \n",
      "9   Mistral-Large       00-Query-Agent-MDTE-mistral-large-latest-1.0   \n",
      "10  Mistral-Large  00-Query-Agent-RAG-WEB-MDTE-mistral-large-late...   \n",
      "\n",
      "      RAG_Type  String_Consistency  Result_Consistency  Num_Tests  \n",
      "0   No-RAG-WEB              0.5088              0.3644          3  \n",
      "1   No-RAG-WEB              0.1603              0.0000          3  \n",
      "2      RAG-WEB              0.7500              0.3622          3  \n",
      "3      RAG-WEB              0.7500              0.3622          3  \n",
      "4   No-RAG-WEB              0.4921              0.4867          3  \n",
      "5   No-RAG-WEB              0.2259              0.3379          3  \n",
      "6      RAG-WEB              0.9487              0.3631          3  \n",
      "7      RAG-WEB              0.7500              0.3622          3  \n",
      "8   No-RAG-WEB              0.9259              0.7000          3  \n",
      "9   No-RAG-WEB              0.8735              0.6843          3  \n",
      "10     RAG-WEB              0.7500              0.3622          3  \n"
     ]
    }
   ],
   "source": [
    "################ OK ###################\n",
    "import os\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from pytextdist.vector_similarity import jaccard_similarity\n",
    "\n",
    "base_dir = os.path.join(os.getcwd(), \"Model-based Trustworthiness Evaluation\")\n",
    "\n",
    "def jaccard_set(a, b):\n",
    "    return len(a & b) / len(a | b) if (a | b) else 1.0\n",
    "\n",
    "def compute_consistency(sets):\n",
    "    pairs = list(combinations(sets, 2))\n",
    "    if not pairs:\n",
    "        return 1.0\n",
    "    scores = [jaccard_set(a, b) for a, b in pairs]\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "def compute_string_consistency(strings):\n",
    "    pairs = list(combinations(strings, 2))\n",
    "    scores = [jaccard_similarity(s1, s2, n=2) for s1, s2 in pairs]\n",
    "    return sum(scores) / len(scores) if pairs else 1.0\n",
    "\n",
    "def compute_nested_consistency_metrics():\n",
    "    results = []\n",
    "\n",
    "    print(f\"Scanning base directory: {base_dir}\")\n",
    "    for llm_name in os.listdir(base_dir):\n",
    "        llm_path = os.path.join(base_dir, llm_name)\n",
    "        if not os.path.isdir(llm_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"Exploring LLM folder: {llm_name}\")\n",
    "        test_runs = []\n",
    "        for root, dirs, files in os.walk(llm_path):\n",
    "            for d in dirs:\n",
    "                if \"Test\" in d:\n",
    "                    full_path = os.path.join(root, d)\n",
    "                    print(f\"  Found test folder: {full_path}\")\n",
    "                    test_runs.append(full_path)\n",
    "\n",
    "        # Group by prefix before -TestXX\n",
    "        grouped = {}\n",
    "        for path in test_runs:\n",
    "            folder = os.path.basename(path)\n",
    "            key = folder.split(\"-Test\")[0] if \"-Test\" in folder else folder\n",
    "            grouped.setdefault(key, []).append(path)\n",
    "\n",
    "        for group_key, paths in grouped.items():\n",
    "            if len(paths) < 2:\n",
    "                continue\n",
    "\n",
    "            rag_type = \"RAG-WEB\" if \"RAG-WEB\" in group_key else \"No-RAG-WEB\"\n",
    "            string_variants = []\n",
    "            title_sets = []\n",
    "\n",
    "            valid_paths = []\n",
    "            for path in paths:\n",
    "                str_file = os.path.join(path, \"String.txt\")\n",
    "                csv_file = os.path.join(path, \"Scopus_Search_results.csv\")\n",
    "\n",
    "                if os.path.isfile(str_file) and os.path.isfile(csv_file):\n",
    "                    try:\n",
    "                        with open(str_file, encoding='utf-8') as f:\n",
    "                            string_variants.append(f.read().strip())\n",
    "\n",
    "                        df = pd.read_csv(csv_file)\n",
    "                        if 'title' in df.columns:\n",
    "                            titles = set(df['title'].dropna().astype(str).str.strip().str.lower())\n",
    "                            title_sets.append(titles)\n",
    "                            valid_paths.append(path)\n",
    "                        else:\n",
    "                            print(f\"Missing 'title' column in: {csv_file}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {path}: {e}\")\n",
    "\n",
    "            if len(valid_paths) >= 2:\n",
    "                string_cons = compute_string_consistency(string_variants)\n",
    "                result_cons = compute_consistency(title_sets)\n",
    "                results.append({\n",
    "                    \"LLM\": llm_name,\n",
    "                    \"Group\": group_key,\n",
    "                    \"RAG_Type\": rag_type,\n",
    "                    \"String_Consistency\": round(string_cons, 4),\n",
    "                    \"Result_Consistency\": round(result_cons, 4),\n",
    "                    \"Num_Tests\": len(valid_paths)\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# === Run and Save\n",
    "df_consistency = compute_nested_consistency_metrics()\n",
    "print(df_consistency)\n",
    "df_consistency.to_csv(\"llm_consistency_metrics_detailed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e415fb2f-2c44-44e5-865c-f8545f4f023d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
