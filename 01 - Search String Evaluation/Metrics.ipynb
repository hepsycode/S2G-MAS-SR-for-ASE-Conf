{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4cbb8f9-3b7a-49e9-8fd4-d1ee188ee41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "\n",
    "# === SIMILARITY FUNCTIONS ===\n",
    "def levenshtein_similarity(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def cosine_text_similarity(a, b):\n",
    "    vectorizer = TfidfVectorizer().fit([a, b])\n",
    "    tfidf = vectorizer.transform([a, b])\n",
    "    return cosine_similarity(tfidf[0], tfidf[1])[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5efd82f-458d-47d0-8609-82b3e259dc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              LLM                                               Test  \\\n",
      "0       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-0.0-Test01   \n",
      "1       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-0.0-Test02   \n",
      "2       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-0.0-Test03   \n",
      "3       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-1.0-Test01   \n",
      "4       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-1.0-Test02   \n",
      "5       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-1.0-Test03   \n",
      "6       Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-0.0-Test01   \n",
      "7       Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-0.0-Test02   \n",
      "8       Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-0.0-Test03   \n",
      "9       Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-1.0-Test01   \n",
      "10      Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-1.0-Test02   \n",
      "11      Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-1.0-Test03   \n",
      "12    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-0.0-Test01   \n",
      "13    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-0.0-Test02   \n",
      "14    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-0.0-Test03   \n",
      "15    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-1.0-Test01   \n",
      "16    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-1.0-Test02   \n",
      "17    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-1.0-Test03   \n",
      "18    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-0.0-Test01   \n",
      "19    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-0.0-Test02   \n",
      "20    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-0.0-Test03   \n",
      "21    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-1.0-Test01   \n",
      "22    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-1.0-Test02   \n",
      "23    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-1.0-Test03   \n",
      "24  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-0.0-T...   \n",
      "25  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-0.0-T...   \n",
      "26  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-0.0-T...   \n",
      "27  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-1.0-T...   \n",
      "28  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-1.0-T...   \n",
      "29  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-1.0-T...   \n",
      "30  Mistral-Large  00-Query-Agent-RAG-WEB-MDTE-mistral-large-late...   \n",
      "31  Mistral-Large  00-Query-Agent-RAG-WEB-MDTE-mistral-large-late...   \n",
      "32  Mistral-Large  00-Query-Agent-RAG-WEB-MDTE-mistral-large-late...   \n",
      "33  Mistral-Large  00-Query-Agent-RAG-WEB-MDTE-mistral-large-late...   \n",
      "34  Mistral-Large  00-Query-Agent-RAG-WEB-MDTE-mistral-large-late...   \n",
      "35  Mistral-Large  00-Query-Agent-RAG-WEB-MDTE-mistral-large-late...   \n",
      "36   mixtral-8x7b   00-Query-Agent-MDTE-open-mixtral-8x7b-0.0-Test01   \n",
      "37   mixtral-8x7b   00-Query-Agent-MDTE-open-mixtral-8x7b-0.0-Test02   \n",
      "38   mixtral-8x7b   00-Query-Agent-MDTE-open-mixtral-8x7b-0.0-Test03   \n",
      "39   mixtral-8x7b   00-Query-Agent-MDTE-open-mixtral-8x7b-1.0-Test01   \n",
      "40   mixtral-8x7b   00-Query-Agent-MDTE-open-mixtral-8x7b-1.0-Test02   \n",
      "41   mixtral-8x7b   00-Query-Agent-MDTE-open-mixtral-8x7b-1.0-Test03   \n",
      "42   mixtral-8x7b  00-Query-Agent-RAG-WEB-MDTE-open-mixtral-8x7b-0.0   \n",
      "43   mixtral-8x7b  00-Query-Agent-RAG-WEB-MDTE-open-mixtral-8x7b-...   \n",
      "44   mixtral-8x7b  00-Query-Agent-RAG-WEB-MDTE-open-mixtral-8x7b-...   \n",
      "45   mixtral-8x7b  00-Query-Agent-RAG-WEB-MDTE-open-mixtral-8x7b-...   \n",
      "46   mixtral-8x7b  00-Query-Agent-RAG-WEB-MDTE-open-mixtral-8x7b-...   \n",
      "\n",
      "    Levenshtein    Cosine  Precision    Recall        F1  TP   FP    FN  \n",
      "0      0.101045  0.619441   0.017857  0.000695  0.001338   3  165  4313  \n",
      "1      0.100346  0.619441   0.021739  0.000695  0.001347   3  135  4313  \n",
      "2      0.100346  0.619441   0.021739  0.000695  0.001347   3  135  4313  \n",
      "3      0.029851  0.532167   0.028037  0.002780  0.005059  12  416  4304  \n",
      "4      0.083682  0.564312   0.008439  0.000463  0.000879   2  235  4314  \n",
      "5      0.042308  0.659326   0.006935  0.001158  0.001985   5  716  4311  \n",
      "6      0.051984  0.707264   0.000000  0.000000  0.000000   0   36  4316  \n",
      "7      0.051075  0.705808   0.000000  0.000000  0.000000   0    6  4316  \n",
      "8      0.054545  0.722321   0.000000  0.000000  0.000000   0   83  4316  \n",
      "9      0.065099  0.743129   0.024691  0.000927  0.001787   4  158  4312  \n",
      "10     0.053973  0.728536   0.000000  0.000000  0.000000   0   87  4316  \n",
      "11     0.052342  0.725313   0.021053  0.000463  0.000907   2   93  4314  \n",
      "12     0.050383  0.704575   0.000000  0.000000  0.000000   0   95  4316  \n",
      "13     0.165657  0.730177   0.013986  0.001854  0.003273   8  564  4308  \n",
      "14     0.165657  0.730177   0.013986  0.001854  0.003273   8  564  4308  \n",
      "15     0.169697  0.738175   0.008547  0.000927  0.001672   4  464  4312  \n",
      "16     0.169697  0.738175   0.008547  0.000927  0.001672   4  464  4312  \n",
      "17     0.165657  0.730177   0.013986  0.001854  0.003273   8  564  4308  \n",
      "18     0.073394  0.702505   0.014925  0.000463  0.000899   2  132  4314  \n",
      "19     0.165657  0.730177   0.014925  0.000463  0.000899   2  132  4314  \n",
      "20     0.073394  0.702505   0.014925  0.000463  0.000899   2  132  4314  \n",
      "21     0.042035  0.693822   0.000000  0.000000  0.000000   0    3  4316  \n",
      "22     0.073394  0.702505   0.014925  0.000463  0.000899   2  132  4314  \n",
      "23     0.073394  0.702505   0.014925  0.000463  0.000899   2  132  4314  \n",
      "24     0.169697  0.738175   0.008547  0.000927  0.001672   4  464  4312  \n",
      "25     0.169697  0.738175   0.008547  0.000927  0.001672   4  464  4312  \n",
      "26     0.169697  0.738175   0.008547  0.000927  0.001672   4  464  4312  \n",
      "27     0.169697  0.738175   0.020690  0.001390  0.002605   6  284  4310  \n",
      "28     0.169697  0.738175   0.008547  0.000927  0.001672   4  464  4312  \n",
      "29     0.169697  0.738175   0.008547  0.000927  0.001672   4  464  4312  \n",
      "30     0.042035  0.693822   0.000000  0.000000  0.000000   0    3  4316  \n",
      "31     0.053419  0.686084   0.000000  0.000000  0.000000   0    3  4316  \n",
      "32     0.042035  0.693822   0.000000  0.000000  0.000000   0    3  4316  \n",
      "33     0.054054  0.688634   0.000000  0.000000  0.000000   0    3  4316  \n",
      "34     0.054054  0.688634   0.000000  0.000000  0.000000   0    3  4316  \n",
      "35     0.053879  0.688634   0.000000  0.000000  0.000000   0    1  4316  \n",
      "36     0.169697  0.738175   0.008547  0.000927  0.001672   4  464  4312  \n",
      "37     0.165657  0.730177   0.013986  0.001854  0.003273   8  564  4308  \n",
      "38     0.165657  0.730177   0.013986  0.001854  0.003273   8  564  4308  \n",
      "39     0.169697  0.738175   0.020690  0.001390  0.002605   6  284  4310  \n",
      "40     0.169697  0.738175   0.020690  0.001390  0.002605   6  284  4310  \n",
      "41     0.165657  0.730177   0.013986  0.001854  0.003273   8  564  4308  \n",
      "42     0.044776  0.686119   0.007843  0.000463  0.000875   2  253  4314  \n",
      "43     0.055046  0.702505   0.014925  0.000463  0.000899   2  132  4314  \n",
      "44     0.055046  0.702505   0.014925  0.000463  0.000899   2  132  4314  \n",
      "45     0.055046  0.702505   0.014925  0.000463  0.000899   2  132  4314  \n",
      "46     0.055046  0.702505   0.014925  0.000463  0.000899   2  132  4314  \n"
     ]
    }
   ],
   "source": [
    "########## OK - Compare search on Scopus #################\n",
    "# Locate the reference string and GT file\n",
    "# base_dir = os.path.join(extract_path, \"Model-driven engineering for digital twins\")\n",
    "base_dir = os.path.join(os.getcwd(), \"Mobile app software engineering\")\n",
    "\n",
    "ref_path = [f for f in os.listdir(base_dir) if f.endswith(\"String.txt\")][0]\n",
    "gt_path = [f for f in os.listdir(base_dir) if f.endswith(\"scopus-Full.csv\")][0]\n",
    "\n",
    "with open(os.path.join(base_dir, ref_path), encoding='utf-8') as f:\n",
    "    ref_string = f.read().strip()\n",
    "\n",
    "gt_df = pd.read_csv(os.path.join(base_dir, gt_path))\n",
    "gt_titles = set(gt_df['title'].dropna().str.strip().str.lower())\n",
    "\n",
    "results = []\n",
    "def evaluate_llm():\n",
    "    for llm_dir in os.listdir(base_dir):\n",
    "        llm_path = os.path.join(base_dir, llm_dir)\n",
    "        if not os.path.isdir(llm_path):\n",
    "            continue\n",
    "        for test_dir in os.listdir(llm_path):\n",
    "            test_path = os.path.join(llm_path, test_dir)\n",
    "            str_file = os.path.join(test_path, \"String.txt\")\n",
    "            csv_file = os.path.join(test_path, \"Scopus_Search_results.csv\")\n",
    "            if os.path.isfile(str_file) and os.path.isfile(csv_file):\n",
    "                with open(str_file, encoding='utf-8') as f:\n",
    "                    gen_string = f.read().strip()\n",
    "                try:\n",
    "                    results_df = pd.read_csv(csv_file)\n",
    "                except:\n",
    "                    continue\n",
    "                titles = set(results_df['title'].dropna().str.strip().str.lower())\n",
    "    \n",
    "                tp = len(titles & gt_titles)\n",
    "                fp = len(titles - gt_titles)\n",
    "                fn = len(gt_titles - titles)\n",
    "    \n",
    "                precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "                recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "                results.append({\n",
    "                    \"LLM\": llm_dir,\n",
    "                    \"Test\": test_dir,\n",
    "                    \"Levenshtein\": levenshtein_similarity(ref_string, gen_string),\n",
    "                    \"Cosine\": cosine_text_similarity(ref_string, gen_string),\n",
    "                    \"Precision\": precision,\n",
    "                    \"Recall\": recall,\n",
    "                    \"F1\": f1,\n",
    "                    \"TP\": tp,\n",
    "                    \"FP\": fp,\n",
    "                    \"FN\": fn\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(results)            \n",
    "\n",
    "df_results = evaluate_llm()\n",
    "print(df_results)\n",
    "df_results.to_csv(\"llm_metrics-scopus-full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4bf1322-f4f6-481e-ada8-23403bcefc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              LLM                                               Test  \\\n",
      "0       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-0.0-Test01   \n",
      "1       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-0.0-Test02   \n",
      "2       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-0.0-Test03   \n",
      "3       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-1.0-Test01   \n",
      "4       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-1.0-Test02   \n",
      "5       Gemma2-2b           00-Query-Agent-MDTE-gemma2-2b-1.0-Test03   \n",
      "6       Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-0.0-Test01   \n",
      "7       Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-0.0-Test02   \n",
      "8       Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-0.0-Test03   \n",
      "9       Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-1.0-Test01   \n",
      "10      Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-1.0-Test02   \n",
      "11      Gemma2-2b   00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-1.0-Test03   \n",
      "12    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-0.0-Test01   \n",
      "13    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-0.0-Test02   \n",
      "14    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-0.0-Test03   \n",
      "15    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-1.0-Test01   \n",
      "16    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-1.0-Test02   \n",
      "17    Llama3.1-8B            00-Query-Agent-MDTE-llama3.1-1.0-Test03   \n",
      "18    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-0.0-Test01   \n",
      "19    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-0.0-Test02   \n",
      "20    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-0.0-Test03   \n",
      "21    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-1.0-Test01   \n",
      "22    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-1.0-Test02   \n",
      "23    Llama3.1-8B    00-Query-Agent-RAG-WEB-MDTE-llama3.1-1.0-Test03   \n",
      "24  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-0.0-T...   \n",
      "25  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-0.0-T...   \n",
      "26  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-0.0-T...   \n",
      "27  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-1.0-T...   \n",
      "28  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-1.0-T...   \n",
      "29  Mistral-Large  00-Query-Agent-MDTE-mistral-large-latest-1.0-T...   \n",
      "30  Mistral-Large  00-Query-Agent-RAG-WEB-MDTE-mistral-large-late...   \n",
      "31  Mistral-Large  00-Query-Agent-RAG-WEB-MDTE-mistral-large-late...   \n",
      "32  Mistral-Large  00-Query-Agent-RAG-WEB-MDTE-mistral-large-late...   \n",
      "33  Mistral-Large  00-Query-Agent-RAG-WEB-MDTE-mistral-large-late...   \n",
      "34  Mistral-Large  00-Query-Agent-RAG-WEB-MDTE-mistral-large-late...   \n",
      "35  Mistral-Large  00-Query-Agent-RAG-WEB-MDTE-mistral-large-late...   \n",
      "36   mixtral-8x7b   00-Query-Agent-MDTE-open-mixtral-8x7b-0.0-Test01   \n",
      "37   mixtral-8x7b   00-Query-Agent-MDTE-open-mixtral-8x7b-0.0-Test02   \n",
      "38   mixtral-8x7b   00-Query-Agent-MDTE-open-mixtral-8x7b-0.0-Test03   \n",
      "39   mixtral-8x7b   00-Query-Agent-MDTE-open-mixtral-8x7b-1.0-Test01   \n",
      "40   mixtral-8x7b   00-Query-Agent-MDTE-open-mixtral-8x7b-1.0-Test02   \n",
      "41   mixtral-8x7b   00-Query-Agent-MDTE-open-mixtral-8x7b-1.0-Test03   \n",
      "42   mixtral-8x7b  00-Query-Agent-RAG-WEB-MDTE-open-mixtral-8x7b-0.0   \n",
      "43   mixtral-8x7b  00-Query-Agent-RAG-WEB-MDTE-open-mixtral-8x7b-...   \n",
      "44   mixtral-8x7b  00-Query-Agent-RAG-WEB-MDTE-open-mixtral-8x7b-...   \n",
      "45   mixtral-8x7b  00-Query-Agent-RAG-WEB-MDTE-open-mixtral-8x7b-...   \n",
      "46   mixtral-8x7b  00-Query-Agent-RAG-WEB-MDTE-open-mixtral-8x7b-...   \n",
      "\n",
      "    Levenshtein    Cosine  Precision    Recall        F1  TP    FP  FN  \n",
      "0      0.101045  0.619441   0.003058  0.023256  0.005405   1   326  42  \n",
      "1      0.100346  0.619441   0.003731  0.023256  0.006431   1   267  42  \n",
      "2      0.100346  0.619441   0.003731  0.023256  0.006431   1   267  42  \n",
      "3      0.029851  0.532167   0.001195  0.023256  0.002273   1   836  42  \n",
      "4      0.083682  0.564312   0.002183  0.023256  0.003992   1   457  42  \n",
      "5      0.042308  0.659326   0.000719  0.023256  0.001395   1  1390  42  \n",
      "6      0.051984  0.707264   0.013889  0.023256  0.017391   1    71  42  \n",
      "7      0.051075  0.705808   0.000000  0.000000  0.000000   0    12  43  \n",
      "8      0.054545  0.722321   0.006061  0.023256  0.009615   1   164  42  \n",
      "9      0.065099  0.743129   0.006231  0.046512  0.010989   2   319  41  \n",
      "10     0.053973  0.728536   0.005848  0.023256  0.009346   1   170  42  \n",
      "11     0.052342  0.725313   0.010695  0.046512  0.017391   2   185  41  \n",
      "12     0.050383  0.704575   0.005435  0.023256  0.008811   1   183  42  \n",
      "13     0.165657  0.730177   0.000890  0.023256  0.001715   1  1122  42  \n",
      "14     0.165657  0.730177   0.000890  0.023256  0.001715   1  1122  42  \n",
      "15     0.169697  0.738175   0.001091  0.023256  0.002083   1   916  42  \n",
      "16     0.169697  0.738175   0.001091  0.023256  0.002083   1   916  42  \n",
      "17     0.165657  0.730177   0.000890  0.023256  0.001715   1  1122  42  \n",
      "18     0.073394  0.702505   0.003774  0.023256  0.006494   1   264  42  \n",
      "19     0.165657  0.730177   0.003774  0.023256  0.006494   1   264  42  \n",
      "20     0.073394  0.702505   0.003774  0.023256  0.006494   1   264  42  \n",
      "21     0.042035  0.693822   0.000000  0.000000  0.000000   0     6  43  \n",
      "22     0.073394  0.702505   0.003774  0.023256  0.006494   1   264  42  \n",
      "23     0.073394  0.702505   0.003774  0.023256  0.006494   1   264  42  \n",
      "24     0.169697  0.738175   0.001091  0.023256  0.002083   1   916  42  \n",
      "25     0.169697  0.738175   0.001091  0.023256  0.002083   1   916  42  \n",
      "26     0.169697  0.738175   0.001091  0.023256  0.002083   1   916  42  \n",
      "27     0.169697  0.738175   0.001767  0.023256  0.003284   1   565  42  \n",
      "28     0.169697  0.738175   0.001091  0.023256  0.002083   1   916  42  \n",
      "29     0.169697  0.738175   0.001091  0.023256  0.002083   1   916  42  \n",
      "30     0.042035  0.693822   0.000000  0.000000  0.000000   0     6  43  \n",
      "31     0.053419  0.686084   0.000000  0.000000  0.000000   0     6  43  \n",
      "32     0.042035  0.693822   0.000000  0.000000  0.000000   0     6  43  \n",
      "33     0.054054  0.688634   0.000000  0.000000  0.000000   0     6  43  \n",
      "34     0.054054  0.688634   0.000000  0.000000  0.000000   0     6  43  \n",
      "35     0.053879  0.688634   0.000000  0.000000  0.000000   0     2  43  \n",
      "36     0.169697  0.738175   0.001091  0.023256  0.002083   1   916  42  \n",
      "37     0.165657  0.730177   0.000890  0.023256  0.001715   1  1122  42  \n",
      "38     0.165657  0.730177   0.000890  0.023256  0.001715   1  1122  42  \n",
      "39     0.169697  0.738175   0.001767  0.023256  0.003284   1   565  42  \n",
      "40     0.169697  0.738175   0.001767  0.023256  0.003284   1   565  42  \n",
      "41     0.165657  0.730177   0.000890  0.023256  0.001715   1  1122  42  \n",
      "42     0.044776  0.686119   0.002033  0.023256  0.003738   1   491  42  \n",
      "43     0.055046  0.702505   0.003774  0.023256  0.006494   1   264  42  \n",
      "44     0.055046  0.702505   0.003774  0.023256  0.006494   1   264  42  \n",
      "45     0.055046  0.702505   0.003774  0.023256  0.006494   1   264  42  \n",
      "46     0.055046  0.702505   0.003774  0.023256  0.006494   1   264  42  \n"
     ]
    }
   ],
   "source": [
    "################## OK - Compare String-Results with selected papers ################\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define base folder where the structure is located\n",
    "base_dir = os.path.join(os.getcwd(), \"Mobile app software engineering\")\n",
    "\n",
    "# Locate reference string and GT dataset\n",
    "ref_path = [f for f in os.listdir(base_dir) if f.endswith(\"String.txt\")][0]\n",
    "gt_path = [f for f in os.listdir(base_dir) if f.endswith(\"Mobile app software engineering - GT.csv\")][0]\n",
    "\n",
    "with open(os.path.join(base_dir, ref_path), encoding='utf-8') as f:\n",
    "    ref_string = f.read().strip()\n",
    "\n",
    "gt_df = pd.read_csv(os.path.join(base_dir, gt_path))\n",
    "\n",
    "# Normalize GT titles and dois\n",
    "gt_df['title_norm'] = gt_df['title'].astype(str).str.strip().str.lower()\n",
    "gt_df['doi_norm'] = gt_df['doi'].astype(str).str.strip().str.lower()\n",
    "gt_titles = set(gt_df['title_norm'])\n",
    "gt_dois = set(gt_df['doi_norm'])\n",
    "\n",
    "results = []\n",
    "\n",
    "def evaluate_llm():\n",
    "    for llm_dir in os.listdir(base_dir):\n",
    "        llm_path = os.path.join(base_dir, llm_dir)\n",
    "        if not os.path.isdir(llm_path):\n",
    "            continue\n",
    "        for test_dir in os.listdir(llm_path):\n",
    "            test_path = os.path.join(llm_path, test_dir)\n",
    "            str_file = os.path.join(test_path, \"String.txt\")\n",
    "            csv_file = os.path.join(test_path, \"Scopus_Search_results.csv\")\n",
    "\n",
    "            if os.path.isfile(str_file) and os.path.isfile(csv_file):\n",
    "                with open(str_file, encoding='utf-8') as f:\n",
    "                    gen_string = f.read().strip()\n",
    "\n",
    "                try:\n",
    "                    results_df = pd.read_csv(csv_file)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                # Normalize titles and DOIs in the retrieved set\n",
    "                results_df['title_norm'] = results_df['title'].astype(str).str.strip().str.lower()\n",
    "                results_df['doi_norm'] = results_df['doi'].astype(str).str.strip().str.lower()\n",
    "\n",
    "                titles = set(results_df['title_norm'])\n",
    "                dois = set(results_df['doi_norm'])\n",
    "\n",
    "                # Combined match: title OR doi\n",
    "                tp = len((titles & gt_titles) | (dois & gt_dois))\n",
    "                fp = len((titles | dois) - (gt_titles | gt_dois))\n",
    "                fn = len((gt_titles | gt_dois) - (titles | dois))\n",
    "\n",
    "                precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "                recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "                results.append({\n",
    "                    \"LLM\": llm_dir,\n",
    "                    \"Test\": test_dir,\n",
    "                    \"Levenshtein\": levenshtein_similarity(ref_string, gen_string),\n",
    "                    \"Cosine\": cosine_text_similarity(ref_string, gen_string),\n",
    "                    \"Precision\": precision,\n",
    "                    \"Recall\": recall,\n",
    "                    \"F1\": f1,\n",
    "                    \"TP\": tp,\n",
    "                    \"FP\": fp,\n",
    "                    \"FN\": fn\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# === Execute\n",
    "df_results = evaluate_llm()\n",
    "print(df_results)\n",
    "df_results.to_csv(\"llm_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c10dc77-2b96-40d4-9b4d-362a00ade80a",
   "metadata": {},
   "source": [
    "# String Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "178dfb9b-d1bf-4d44-b922-fb795930d43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             LLM  String_Consistency  Result_Consistency  Num_Tests\n",
      "0      Gemma2-2b              0.2425              0.1223         13\n",
      "1    Llama3.1-8B              0.2422              0.0558         12\n",
      "2    Llama3.2-3B              0.2723              0.1396         12\n",
      "3  Mistral-Large              0.2409              0.1328         12\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from pytextdist.vector_similarity import jaccard_similarity\n",
    "\n",
    "base_dir = os.path.join(os.getcwd(), \"Model-driven engineering for digital twins\")\n",
    "\n",
    "def jaccard_set(a, b):\n",
    "    return len(a & b) / len(a | b) if (a | b) else 1.0\n",
    "\n",
    "def compute_consistency(sets):\n",
    "    pairs = list(combinations(sets, 2))\n",
    "    if not pairs:\n",
    "        return 1.0\n",
    "    scores = [jaccard_set(a, b) for a, b in pairs]\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "def compute_string_consistency(strings):\n",
    "    pairs = list(combinations(strings, 2))\n",
    "    scores = [jaccard_similarity(s1, s2, n=2) for s1, s2 in pairs]\n",
    "    return sum(scores) / len(scores) if pairs else 1.0\n",
    "\n",
    "def compute_nested_consistency_metrics():\n",
    "    results = []\n",
    "\n",
    "    # Loop over all LLM root folders (e.g., Llama3.1-8B)\n",
    "    for llm_name in os.listdir(base_dir):\n",
    "        llm_path = os.path.join(base_dir, llm_name)\n",
    "        if not os.path.isdir(llm_path):\n",
    "            continue\n",
    "\n",
    "        # Gather all test runs (subfolders like ...-Test01, -Test02, -Test03)\n",
    "        test_runs = [\n",
    "            os.path.join(llm_path, subdir)\n",
    "            for subdir in os.listdir(llm_path)\n",
    "            if os.path.isdir(os.path.join(llm_path, subdir)) and \"Test\" in subdir\n",
    "        ]\n",
    "\n",
    "        string_variants = []\n",
    "        title_sets = []\n",
    "\n",
    "        for path in test_runs:\n",
    "            str_file = os.path.join(path, \"String.txt\")\n",
    "            csv_file = os.path.join(path, \"Scopus_Search_results.csv\")\n",
    "\n",
    "            if os.path.isfile(str_file) and os.path.isfile(csv_file):\n",
    "                try:\n",
    "                    with open(str_file, encoding='utf-8') as f:\n",
    "                        string_variants.append(f.read().strip())\n",
    "\n",
    "                    df = pd.read_csv(csv_file)\n",
    "                    titles = set(df['title'].dropna().astype(str).str.strip().str.lower())\n",
    "                    title_sets.append(titles)\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipped {path} due to error: {e}\")\n",
    "\n",
    "        if string_variants and title_sets:\n",
    "            string_cons = compute_string_consistency(string_variants)\n",
    "            result_cons = compute_consistency(title_sets)\n",
    "            results.append({\n",
    "                \"LLM\": llm_name,\n",
    "                \"String_Consistency\": round(string_cons, 4),\n",
    "                \"Result_Consistency\": round(result_cons, 4),\n",
    "                \"Num_Tests\": len(test_runs)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# === Run and Save\n",
    "df_consistency = compute_nested_consistency_metrics()\n",
    "print(df_consistency)\n",
    "df_consistency.to_csv(\"llm_consistency_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c2be51f-efb1-4470-874e-3db0613668aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning base directory: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\n",
      "Exploring LLM folder: Gemma2-2b\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-MDTE-gemma2-2b-0.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-MDTE-gemma2-2b-0.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-MDTE-gemma2-2b-0.0-Test03\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-MDTE-gemma2-2b-1.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-MDTE-gemma2-2b-1.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-MDTE-gemma2-2b-1.0-Test03\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-0.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-0.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-0.0-Test03\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-1.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-1.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Gemma2-2b\\00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-1.0-Test03\n",
      "Exploring LLM folder: Llama3.1-8B\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-MDTE-llama3.1-0.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-MDTE-llama3.1-0.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-MDTE-llama3.1-0.0-Test03\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-MDTE-llama3.1-1.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-MDTE-llama3.1-1.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-MDTE-llama3.1-1.0-Test03\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-RAG-WEB-MDTE-llama3.1-0.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-RAG-WEB-MDTE-llama3.1-0.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-RAG-WEB-MDTE-llama3.1-0.0-Test03\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-RAG-WEB-MDTE-llama3.1-1.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-RAG-WEB-MDTE-llama3.1-1.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Llama3.1-8B\\00-Query-Agent-RAG-WEB-MDTE-llama3.1-1.0-Test03\n",
      "Exploring LLM folder: Mistral-Large\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-MDTE-mistral-large-latest-0.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-MDTE-mistral-large-latest-0.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-MDTE-mistral-large-latest-0.0-Test03\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-MDTE-mistral-large-latest-1.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-MDTE-mistral-large-latest-1.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-MDTE-mistral-large-latest-1.0-Test03\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-RAG-WEB-MDTE-mistral-large-latest-0.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-RAG-WEB-MDTE-mistral-large-latest-0.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-RAG-WEB-MDTE-mistral-large-latest-0.0-Test03\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-RAG-WEB-MDTE-mistral-large-latest-1.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-RAG-WEB-MDTE-mistral-large-latest-1.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\Mistral-Large\\00-Query-Agent-RAG-WEB-MDTE-mistral-large-latest-1.0-Test03\n",
      "Exploring LLM folder: mixtral-8x7b\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\mixtral-8x7b\\00-Query-Agent-MDTE-open-mixtral-8x7b-0.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\mixtral-8x7b\\00-Query-Agent-MDTE-open-mixtral-8x7b-1.0-Test01\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\mixtral-8x7b\\00-Query-Agent-MDTE-open-mixtral-8x7b-1.0-Test02\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\mixtral-8x7b\\00-Query-Agent-MDTE-open-mixtral-8x7b-1.0-Test03\n",
      "  Found test folder: C:\\Users\\vitto\\Desktop\\repo\\01 - Search String Evaluation\\Model-based Trustworthiness Evaluation\\mixtral-8x7b\\00-Query-Agent-RAG-WEB-MDTE-open-mixtral-8x7b-0.0-Test01\n",
      "              LLM                                              Group  \\\n",
      "0       Gemma2-2b                  00-Query-Agent-MDTE-gemma2-2b-0.0   \n",
      "1       Gemma2-2b                  00-Query-Agent-MDTE-gemma2-2b-1.0   \n",
      "2       Gemma2-2b          00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-0.0   \n",
      "3       Gemma2-2b          00-Query-Agent-RAG-WEB-MDTE-gemma2-2b-1.0   \n",
      "4     Llama3.1-8B                   00-Query-Agent-MDTE-llama3.1-0.0   \n",
      "5     Llama3.1-8B                   00-Query-Agent-MDTE-llama3.1-1.0   \n",
      "6     Llama3.1-8B           00-Query-Agent-RAG-WEB-MDTE-llama3.1-0.0   \n",
      "7     Llama3.1-8B           00-Query-Agent-RAG-WEB-MDTE-llama3.1-1.0   \n",
      "8   Mistral-Large       00-Query-Agent-MDTE-mistral-large-latest-0.0   \n",
      "9   Mistral-Large       00-Query-Agent-MDTE-mistral-large-latest-1.0   \n",
      "10  Mistral-Large  00-Query-Agent-RAG-WEB-MDTE-mistral-large-late...   \n",
      "11   mixtral-8x7b          00-Query-Agent-MDTE-open-mixtral-8x7b-1.0   \n",
      "\n",
      "      RAG_Type  String_Consistency  Result_Consistency  Num_Tests  \n",
      "0   No-RAG-WEB              0.5088              0.3644          3  \n",
      "1   No-RAG-WEB              0.1603              0.0000          3  \n",
      "2      RAG-WEB              0.7500              0.3622          3  \n",
      "3      RAG-WEB              0.7500              0.3622          3  \n",
      "4   No-RAG-WEB              0.4921              0.4867          3  \n",
      "5   No-RAG-WEB              0.2259              0.3379          3  \n",
      "6      RAG-WEB              0.9487              0.3631          3  \n",
      "7      RAG-WEB              0.7500              0.3622          3  \n",
      "8   No-RAG-WEB              0.9259              0.7000          3  \n",
      "9   No-RAG-WEB              0.8735              0.6843          3  \n",
      "10     RAG-WEB              0.7500              0.3622          3  \n",
      "11  No-RAG-WEB              1.0000              1.0000          3  \n"
     ]
    }
   ],
   "source": [
    "################ OK ###################\n",
    "import os\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from pytextdist.vector_similarity import jaccard_similarity\n",
    "\n",
    "base_dir = os.path.join(os.getcwd(), \"Model-based Trustworthiness Evaluation\")\n",
    "\n",
    "def jaccard_set(a, b):\n",
    "    return len(a & b) / len(a | b) if (a | b) else 1.0\n",
    "\n",
    "def compute_consistency(sets):\n",
    "    pairs = list(combinations(sets, 2))\n",
    "    if not pairs:\n",
    "        return 1.0\n",
    "    scores = [jaccard_set(a, b) for a, b in pairs]\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "def compute_string_consistency(strings):\n",
    "    pairs = list(combinations(strings, 2))\n",
    "    scores = [jaccard_similarity(s1, s2, n=2) for s1, s2 in pairs]\n",
    "    return sum(scores) / len(scores) if pairs else 1.0\n",
    "\n",
    "def compute_nested_consistency_metrics():\n",
    "    results = []\n",
    "\n",
    "    print(f\"Scanning base directory: {base_dir}\")\n",
    "    for llm_name in os.listdir(base_dir):\n",
    "        llm_path = os.path.join(base_dir, llm_name)\n",
    "        if not os.path.isdir(llm_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"Exploring LLM folder: {llm_name}\")\n",
    "        test_runs = []\n",
    "        for root, dirs, files in os.walk(llm_path):\n",
    "            for d in dirs:\n",
    "                if \"Test\" in d:\n",
    "                    full_path = os.path.join(root, d)\n",
    "                    print(f\"  Found test folder: {full_path}\")\n",
    "                    test_runs.append(full_path)\n",
    "\n",
    "        # Group by prefix before -TestXX\n",
    "        grouped = {}\n",
    "        for path in test_runs:\n",
    "            folder = os.path.basename(path)\n",
    "            key = folder.split(\"-Test\")[0] if \"-Test\" in folder else folder\n",
    "            grouped.setdefault(key, []).append(path)\n",
    "\n",
    "        for group_key, paths in grouped.items():\n",
    "            if len(paths) < 2:\n",
    "                continue\n",
    "\n",
    "            rag_type = \"RAG-WEB\" if \"RAG-WEB\" in group_key else \"No-RAG-WEB\"\n",
    "            string_variants = []\n",
    "            title_sets = []\n",
    "\n",
    "            valid_paths = []\n",
    "            for path in paths:\n",
    "                str_file = os.path.join(path, \"String.txt\")\n",
    "                csv_file = os.path.join(path, \"Scopus_Search_results.csv\")\n",
    "\n",
    "                if os.path.isfile(str_file) and os.path.isfile(csv_file):\n",
    "                    try:\n",
    "                        with open(str_file, encoding='utf-8') as f:\n",
    "                            string_variants.append(f.read().strip())\n",
    "\n",
    "                        df = pd.read_csv(csv_file)\n",
    "                        if 'title' in df.columns:\n",
    "                            titles = set(df['title'].dropna().astype(str).str.strip().str.lower())\n",
    "                            title_sets.append(titles)\n",
    "                            valid_paths.append(path)\n",
    "                        else:\n",
    "                            print(f\"Missing 'title' column in: {csv_file}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {path}: {e}\")\n",
    "\n",
    "            if len(valid_paths) >= 2:\n",
    "                string_cons = compute_string_consistency(string_variants)\n",
    "                result_cons = compute_consistency(title_sets)\n",
    "                results.append({\n",
    "                    \"LLM\": llm_name,\n",
    "                    \"Group\": group_key,\n",
    "                    \"RAG_Type\": rag_type,\n",
    "                    \"String_Consistency\": round(string_cons, 4),\n",
    "                    \"Result_Consistency\": round(result_cons, 4),\n",
    "                    \"Num_Tests\": len(valid_paths)\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# === Run and Save\n",
    "df_consistency = compute_nested_consistency_metrics()\n",
    "print(df_consistency)\n",
    "df_consistency.to_csv(\"llm_consistency_metrics_detailed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e415fb2f-2c44-44e5-865c-f8545f4f023d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
